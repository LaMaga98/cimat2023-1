{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{PLN. Proyecto Final: PeptideBert}$$\n",
    "$$\\textit{Y. Sarahi García Gozález}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconvert_encodings\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconvert_encodings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m m2\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertModel, BertConfig, logging\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n",
      "File \u001b[0;32m~/Desktop/proyectoNLP/.conda/lib/python3.10/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     logging,\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     46\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/proyectoNLP/.conda/lib/python3.10/site-packages/transformers/dependency_versions_check.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m~/Desktop/proyectoNLP/.conda/lib/python3.10/site-packages/transformers/utils/__init__.py:30\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m     24\u001b[0m     add_end_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m     ContextManagers,\n\u001b[1;32m     32\u001b[0m     ExplicitEnum,\n\u001b[1;32m     33\u001b[0m     ModelOutput,\n\u001b[1;32m     34\u001b[0m     PaddingStrategy,\n\u001b[1;32m     35\u001b[0m     TensorType,\n\u001b[1;32m     36\u001b[0m     add_model_info_to_auto_map,\n\u001b[1;32m     37\u001b[0m     cached_property,\n\u001b[1;32m     38\u001b[0m     can_return_loss,\n\u001b[1;32m     39\u001b[0m     expand_dims,\n\u001b[1;32m     40\u001b[0m     find_labels,\n\u001b[1;32m     41\u001b[0m     flatten_dict,\n\u001b[1;32m     42\u001b[0m     infer_framework,\n\u001b[1;32m     43\u001b[0m     is_jax_tensor,\n\u001b[1;32m     44\u001b[0m     is_numpy_array,\n\u001b[1;32m     45\u001b[0m     is_tensor,\n\u001b[1;32m     46\u001b[0m     is_tf_symbolic_tensor,\n\u001b[1;32m     47\u001b[0m     is_tf_tensor,\n\u001b[1;32m     48\u001b[0m     is_torch_device,\n\u001b[1;32m     49\u001b[0m     is_torch_dtype,\n\u001b[1;32m     50\u001b[0m     is_torch_tensor,\n\u001b[1;32m     51\u001b[0m     reshape,\n\u001b[1;32m     52\u001b[0m     squeeze,\n\u001b[1;32m     53\u001b[0m     strtobool,\n\u001b[1;32m     54\u001b[0m     tensor_size,\n\u001b[1;32m     55\u001b[0m     to_numpy,\n\u001b[1;32m     56\u001b[0m     to_py_obj,\n\u001b[1;32m     57\u001b[0m     transpose,\n\u001b[1;32m     58\u001b[0m     working_or_temp_dir,\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     61\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[1;32m     62\u001b[0m     DISABLE_TELEMETRY,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m     try_to_load_from_cache,\n\u001b[1;32m     90\u001b[0m )\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     92\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[1;32m     93\u001b[0m     ENV_VARS_TRUE_VALUES,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     torch_only_method,\n\u001b[1;32m    178\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/proyectoNLP/.conda/lib/python3.10/site-packages/transformers/utils/generic.py:29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, ContextManager, List, Tuple\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_flax_available, is_tf_available, is_torch_available, is_torch_fx_proxy\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/proyectoNLP/.conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:34\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Tuple, Union\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m     37\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# TODO: This doesn't work for all packages (`bs4`, `faiss`, etc.) Talk to Sylvain to see how to do with it better.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/proyectoNLP/.conda/lib/python3.10/site-packages/transformers/utils/logging.py:35\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     CRITICAL,  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     DEBUG,  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     WARNING,  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhf_hub_utils\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto \u001b[38;5;28;01mas\u001b[39;00m tqdm_lib\n\u001b[1;32m     39\u001b[0m _lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n",
      "File \u001b[0;32m~/Desktop/proyectoNLP/.conda/lib/python3.10/site-packages/huggingface_hub/utils/__init__.py:19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# ruff: noqa: F401\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     HFValidationError,\n\u001b[1;32m     21\u001b[0m     LocalTokenNotFoundError,\n\u001b[1;32m     22\u001b[0m     NotASafetensorsRepoError,\n\u001b[1;32m     23\u001b[0m     OfflineModeIsEnabled,\n\u001b[1;32m     24\u001b[0m     SafetensorsParsingError,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;28;01mas\u001b[39;00m _tqdm  \u001b[38;5;66;03m# _tqdm is the module\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cache_assets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cached_assets_path\n",
      "File \u001b[0;32m~/Desktop/proyectoNLP/.conda/lib/python3.10/site-packages/huggingface_hub/errors.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Contains all custom errors.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# HEADERS ERRORS\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLocalTokenNotFoundError\u001b[39;00m(\u001b[38;5;167;01mEnvironmentError\u001b[39;00m):\n",
      "File \u001b[0;32m~/Desktop/proyectoNLP/.conda/lib/python3.10/site-packages/requests/__init__.py:48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RequestsDependencyWarning\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m charset_normalizer_version\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     charset_normalizer_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/proyectoNLP/.conda/lib/python3.10/site-packages/charset_normalizer/__init__.py:23\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mCharset-Normalizer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_fp, from_path, from_bytes, normalize\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, VERSION\n",
      "File \u001b[0;32m~/Desktop/proyectoNLP/.conda/lib/python3.10/site-packages/charset_normalizer/api.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m     PathLike \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.PathLike[str]\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mess_ratio\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharsetMatches, CharsetMatch\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import urllib.request\n",
    "import convert_encodings\n",
    "from convert_encodings import m2\n",
    "from transformers import BertModel, BertConfig, logging\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - pytorch-nightly\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install pytorch torchvision torchaudio -c pytorch-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PeptideBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "        self.length = len(self.input_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_id = self.input_ids[idx]\n",
    "        attention_mask = self.attention_masks[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_id, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "def load_data(config):\n",
    "    print(f'{\"=\"*30}{\"DATA\":^20}{\"=\"*30}')\n",
    "\n",
    "    with np.load(f'./data/{config[\"task\"]}/train.npz') as train,\\\n",
    "         np.load(f'./data/{config[\"task\"]}/val.npz') as val,\\\n",
    "         np.load(f'./data/{config[\"task\"]}/test.npz') as test:\n",
    "        train_inputs = train['inputs']\n",
    "        train_labels = train['labels']\n",
    "        val_inputs = val['inputs']\n",
    "        val_labels = val['labels']\n",
    "        test_inputs = test['inputs']\n",
    "        test_labels = test['labels']\n",
    "\n",
    "    attention_mask = np.asarray(train_inputs > 0, dtype=np.float64)\n",
    "    attention_mask_val = np.asarray(val_inputs > 0, dtype=np.float64)\n",
    "    attention_mask_test = np.asarray(test_inputs > 0, dtype=np.float64)\n",
    "\n",
    "    train_dataset = PeptideBERTDataset(input_ids=train_inputs, attention_masks=attention_mask, labels=train_labels)\n",
    "    val_dataset = PeptideBERTDataset(input_ids=val_inputs, attention_masks=attention_mask_val, labels=val_labels)\n",
    "    test_dataset = PeptideBERTDataset(input_ids=test_inputs, attention_masks=attention_mask_test, labels=test_labels)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_data_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_data_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    print('Batch size: ', config['batch_size'])\n",
    "\n",
    "    print('Train dataset samples: ', len(train_dataset))\n",
    "    print('Validation dataset samples: ', len(val_dataset))\n",
    "    print('Test dataset samples: ', len(test_dataset))\n",
    "\n",
    "    print('Train dataset batches: ', len(train_data_loader))\n",
    "    print('Validation dataset batches: ', len(val_data_loader))\n",
    "    print('Test dataset batches: ', len(test_data_loader))\n",
    "\n",
    "    print()\n",
    "\n",
    "    return train_data_loader, val_data_loader, test_data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_hemolysis():\n",
    "    urllib.request.urlretrieve(\n",
    "        'https://github.com/ur-whitelab/peptide-dashboard/raw/master/ml/data/hemo-positive.npz',\n",
    "        './data/hemo-positive.npz',\n",
    "    )\n",
    "    urllib.request.urlretrieve(\n",
    "        'https://github.com/ur-whitelab/peptide-dashboard/raw/master/ml/data/hemo-negative.npz',\n",
    "        './data/hemo-negative.npz',\n",
    "    )\n",
    "\n",
    "\n",
    "def download_solubility():\n",
    "    urllib.request.urlretrieve(\n",
    "        'https://github.com/ur-whitelab/peptide-dashboard/raw/master/ml/data/soluble.npz',\n",
    "        './data/sol-positive.npz',\n",
    "    )\n",
    "    urllib.request.urlretrieve(\n",
    "        'https://github.com/ur-whitelab/peptide-dashboard/raw/master/ml/data/insoluble.npz',\n",
    "        './data/sol-negative.npz',\n",
    "    )\n",
    "\n",
    "\n",
    "def download_nonfouling():\n",
    "    urllib.request.urlretrieve(\n",
    "        'https://github.com/ur-whitelab/peptide-dashboard/raw/master/ml/data/human-positive.npz',\n",
    "        './data/nf-positive.npz',\n",
    "    )\n",
    "    urllib.request.urlretrieve(\n",
    "        'https://github.com/ur-whitelab/peptide-dashboard/raw/master/ml/data/human-negative.npz',\n",
    "        './data/nf-negative.npz',\n",
    "    )\n",
    "\n",
    "    neg = np.load('./data/nf-negative.npz')\n",
    "    np.savez(\n",
    "        './data/nf-negative.npz',\n",
    "        arr_0=neg['seqs'],\n",
    "        weights=neg['weights']\n",
    "    )\n",
    "\n",
    "\n",
    "download_hemolysis()\n",
    "download_solubility()\n",
    "download_nonfouling()\n",
    "\n",
    "convert_encodings.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres de los arrays en el archivo .npz: ['arr_0']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1826 rows × 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  180  181  182  \\\n",
       "0      13    8   12   13    8   24   16    5    8   11  ...    0    0    0   \n",
       "1      13    8   12   13    8   24   16    5    8   11  ...    0    0    0   \n",
       "2      13    8   12   13    8   24   16    5    8   11  ...    0    0    0   \n",
       "3      13    8   12   13    8   24   16    5    8   11  ...    0    0    0   \n",
       "4       7   21    6   10   12    6    7    6   11    6  ...    0    0    0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1821   23   13   24    7   13   24    5   13   12   11  ...    0    0    0   \n",
       "1822   13   24    7   13   24    5   13   12   11   13  ...    0    0    0   \n",
       "1823   13    5   21   13   11   19   13   11    5   12  ...    0    0    0   \n",
       "1824   13   21   21   13   11   19   24    8   11   12  ...    0    0    0   \n",
       "1825    7   11   11   17   15    5   18   12   20   20  ...    0    0    0   \n",
       "\n",
       "      183  184  185  186  187  188  189  \n",
       "0       0    0    0    0    0    0    0  \n",
       "1       0    0    0    0    0    0    0  \n",
       "2       0    0    0    0    0    0    0  \n",
       "3       0    0    0    0    0    0    0  \n",
       "4       0    0    0    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "1821    0    0    0    0    0    0    0  \n",
       "1822    0    0    0    0    0    0    0  \n",
       "1823    0    0    0    0    0    0    0  \n",
       "1824    0    0    0    0    0    0    0  \n",
       "1825    0    0    0    0    0    0    0  \n",
       "\n",
       "[1826 rows x 190 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el archivo .npz\n",
    "data = np.load('/Users/ely/Desktop/proyectoNLP/PeptideBERT/data/hemo-positive.npz')\n",
    "\n",
    "# Listar los nombres de los arrays en el archivo .npz\n",
    "print(\"Nombres de los arrays en el archivo .npz:\", data.files)\n",
    "\n",
    "files=data.files\n",
    "df = pd.DataFrame(data[files[0]])\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres de los arrays en el archivo .npz: ['arr_0']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7485</th>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7486</th>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7487</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7488</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7490 rows × 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  180  181  182  \\\n",
       "0      13    8   12   13    8   24   16    5    8   11  ...    0    0    0   \n",
       "1      13    8   12   13    8   24   16    5    8   11  ...    0    0    0   \n",
       "2      13    8   12   13    8   24   16    5    8   11  ...    0    0    0   \n",
       "3      13    8   12   13    8   24   16    5    8   11  ...    0    0    0   \n",
       "4       7   11   24   14   15   11   12   10   21    7  ...    0    0    0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "7485   19    5   11   12   11    5   12    7    7   12  ...    0    0    0   \n",
       "7486   19   11    7    6   11    6   10   20    5   12  ...    0    0    0   \n",
       "7487    7    8    8   14   11   11   12    7    6    7  ...    0    0    0   \n",
       "7488    7    5   21   14   15    8   12   17    6    6  ...    0    0    0   \n",
       "7489    7   11   11   17   15    5   18   12   20   20  ...    0    0    0   \n",
       "\n",
       "      183  184  185  186  187  188  189  \n",
       "0       0    0    0    0    0    0    0  \n",
       "1       0    0    0    0    0    0    0  \n",
       "2       0    0    0    0    0    0    0  \n",
       "3       0    0    0    0    0    0    0  \n",
       "4       0    0    0    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "7485    0    0    0    0    0    0    0  \n",
       "7486    0    0    0    0    0    0    0  \n",
       "7487    0    0    0    0    0    0    0  \n",
       "7488    0    0    0    0    0    0    0  \n",
       "7489    0    0    0    0    0    0    0  \n",
       "\n",
       "[7490 rows x 190 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el archivo .npz\n",
    "data = np.load('/Users/ely/Desktop/proyectoNLP/PeptideBERT/data/hemo-negative.npz')\n",
    "\n",
    "# Listar los nombres de los arrays en el archivo .npz\n",
    "print(\"Nombres de los arrays en el archivo .npz:\", data.files)\n",
    "\n",
    "files=data.files\n",
    "df = pd.DataFrame(data[files[0]])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separmos val,test y train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(task):\n",
    "    with np.load(f'/Users/ely/Desktop/proyectoNLP/PeptideBERT/data/{task}-positive.npz') as pos,\\\n",
    "         np.load(f'/Users/ely/Desktop/proyectoNLP/PeptideBERT/data/{task}-negative.npz') as neg:\n",
    "        pos_data = pos['arr_0']\n",
    "        neg_data = neg['arr_0']\n",
    "\n",
    "    input_ids = np.vstack((\n",
    "        pos_data,\n",
    "        neg_data\n",
    "    ))\n",
    "\n",
    "    labels = np.hstack((\n",
    "        np.ones(len(pos_data)),\n",
    "        np.zeros(len(neg_data))\n",
    "    ))\n",
    "\n",
    "    train_val_inputs, test_inputs, train_val_labels, test_labels = train_test_split(\n",
    "        input_ids, labels, test_size=0.1\n",
    "    )\n",
    "\n",
    "    train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "        train_val_inputs, train_val_labels, test_size=0.1\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(f'/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/Proyecto_lenguaje/data/{task}'):\n",
    "        os.mkdir(f'/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/Proyecto_lenguaje/data/{task}')\n",
    "\n",
    "    np.savez(\n",
    "        f'/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/Proyecto_lenguaje/data/{task}/train.npz',\n",
    "        inputs=train_inputs,\n",
    "        labels=train_labels\n",
    "    )\n",
    "\n",
    "    np.savez(\n",
    "        f'/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/Proyecto_lenguaje/data/{task}/val.npz',\n",
    "        inputs=val_inputs,\n",
    "        labels=val_labels\n",
    "    )\n",
    "\n",
    "    np.savez(\n",
    "        f'/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/Proyecto_lenguaje/data/{task}/test.npz',\n",
    "        inputs=test_inputs,\n",
    "        labels=test_labels\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data('hemo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(inputs, labels, new_inputs, new_labels):\n",
    "    new_inputs = np.vstack(new_inputs)\n",
    "    new_labels = np.hstack(new_labels)\n",
    "\n",
    "    inputs = np.vstack((inputs, new_inputs))\n",
    "    labels = np.hstack((labels, new_labels))\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def random_replace(inputs, labels, factor):\n",
    "    new_inputs = []\n",
    "    new_labels = []\n",
    "    for idx in range(inputs.shape[0]):\n",
    "        ip = inputs[idx]\n",
    "        label = labels[idx]\n",
    "\n",
    "        try:\n",
    "            unpadded_len = np.where(ip == 0)[0][0]\n",
    "        except IndexError:\n",
    "            unpadded_len = len(ip)\n",
    "        num_to_replace = round(unpadded_len * factor)\n",
    "        indices = np.random.choice(unpadded_len, num_to_replace, replace=False)\n",
    "        ip[indices] = np.random.choice(np.arange(5, 25), num_to_replace, replace=True)\n",
    "\n",
    "        new_inputs.append(ip)\n",
    "        new_labels.append(label)\n",
    "\n",
    "    return new_inputs, new_labels\n",
    "\n",
    "\n",
    "def random_delete(inputs, labels, factor):\n",
    "    new_inputs = []\n",
    "    new_labels = []\n",
    "    for idx in range(inputs.shape[0]):\n",
    "        ip = inputs[idx]\n",
    "        label = labels[idx]\n",
    "\n",
    "        try:\n",
    "            unpadded_len = np.where(ip == 0)[0][0]\n",
    "        except IndexError:\n",
    "            unpadded_len = len(ip)\n",
    "        ip = list(ip[:unpadded_len])\n",
    "        num_to_delete = round(unpadded_len * factor)\n",
    "        indices = np.random.choice(unpadded_len, num_to_delete, replace=False)\n",
    "        for i in reversed(sorted(indices)):\n",
    "            ip.pop(i)\n",
    "        ip.extend([0] * (200 - len(ip)))\n",
    "\n",
    "        new_inputs.append(np.asarray(ip))\n",
    "        new_labels.append(label)\n",
    "\n",
    "    return new_inputs, new_labels\n",
    "\n",
    "\n",
    "def random_replace_with_A(inputs, labels, factor):\n",
    "    new_inputs = []\n",
    "    new_labels = []\n",
    "    for idx in range(inputs.shape[0]):\n",
    "        ip = inputs[idx]\n",
    "        label = labels[idx]\n",
    "\n",
    "        try:\n",
    "            unpadded_len = np.where(ip == 0)[0][0]\n",
    "        except IndexError:\n",
    "            unpadded_len = len(ip)\n",
    "        num_to_replace = round(unpadded_len * factor)\n",
    "        indices = np.random.choice(unpadded_len, num_to_replace, replace=False)\n",
    "        ip[indices] = m2['A']\n",
    "\n",
    "        new_inputs.append(ip)\n",
    "        new_labels.append(label)\n",
    "\n",
    "    return new_inputs, new_labels\n",
    "\n",
    "\n",
    "def random_swap(inputs, labels, factor):\n",
    "    new_inputs = []\n",
    "    new_labels = []\n",
    "    for idx in range(inputs.shape[0]):\n",
    "        ip = inputs[idx]\n",
    "        label = labels[idx]\n",
    "\n",
    "        try:\n",
    "            unpadded_len = np.where(ip == 0)[0][0]\n",
    "        except IndexError:\n",
    "            unpadded_len = len(ip)\n",
    "        ip = list(ip[:unpadded_len])\n",
    "        num_to_swap = round(unpadded_len * factor)\n",
    "        indices = np.random.choice(range(1, unpadded_len, 2), num_to_swap, replace=False)\n",
    "        for i in indices:\n",
    "            ip[i-1], ip[i] = ip[i], ip[i-1]\n",
    "        ip.extend([0] * (200 - len(ip)))\n",
    "\n",
    "        new_inputs.append(np.asarray(ip))\n",
    "        new_labels.append(label)\n",
    "\n",
    "    return new_inputs, new_labels\n",
    "\n",
    "\n",
    "def random_insertion_with_A(inputs, labels, factor):\n",
    "    new_inputs = []\n",
    "    new_labels = []\n",
    "    for idx in range(inputs.shape[0]):\n",
    "        ip = inputs[idx]\n",
    "        label = labels[idx]\n",
    "\n",
    "        try:\n",
    "            unpadded_len = np.where(ip == 0)[0][0]\n",
    "        except IndexError:\n",
    "            unpadded_len = len(ip)\n",
    "        ip = list(ip[:unpadded_len])\n",
    "        num_to_insert = round(unpadded_len * factor)\n",
    "        indices = np.random.choice(unpadded_len, num_to_insert, replace=False)\n",
    "        for i in indices:\n",
    "            ip.insert(i, m2['A'])\n",
    "        if len(ip) < 200:\n",
    "            ip.extend([0] * (200 - len(ip)))\n",
    "        elif len(ip) > 200:\n",
    "            ip = ip[:200]\n",
    "\n",
    "        new_inputs.append(np.asarray(ip))\n",
    "        new_labels.append(label)\n",
    "\n",
    "    return new_inputs, new_labels\n",
    "\n",
    "\n",
    "def random_masking(sequences, mask_prob=0.15, mask_token_id=0):\n",
    "    masked_sequences = np.copy(sequences)\n",
    "    mask = np.random.rand(*sequences.shape) < mask_prob\n",
    "    masked_sequences[mask] = mask_token_id\n",
    "    return masked_sequences\n",
    "\n",
    "\n",
    "def augment_data(task):\n",
    "    with np.load(f'./data/{task}/train.npz') as train:\n",
    "        inputs = train['inputs']\n",
    "        labels = train['labels']\n",
    "\n",
    "    # new_inputs1, new_labels1 = random_replace(inputs, labels, 0.02)\n",
    "    # new_inputs2, new_labels2 = random_delete(inputs, labels, 0.02)\n",
    "    # new_inputs3, new_labels3 = random_replace_with_A(inputs, labels, 0.02)\n",
    "    new_inputs4, new_labels4 = random_swap(inputs, labels, 0.02)\n",
    "    # new_inputs5, new_labels5 = random_insertion_with_A(inputs, labels, 0.02)\n",
    "    #new_inputs6, new_labels6 = random_masking(inputs, mask_prob=0.15, mask_token_id=0)\n",
    "\n",
    "    # inputs, labels = combine(inputs, labels, new_inputs1, new_labels1)\n",
    "    # inputs, labels = combine(inputs, labels, new_inputs2, new_labels2)\n",
    "    # inputs, labels = combine(inputs, labels, new_inputs3, new_labels3)\n",
    "    inputs, labels = combine(inputs, labels, new_inputs4, new_labels4)\n",
    "    # inputs, labels = combine(inputs, labels, new_inputs5, new_labels5)\n",
    "    #inputs, labels = combine(inputs, labels, new_inputs6, new_labels6)\n",
    "\n",
    "    np.savez(\n",
    "        f'./data/{task}/train.npz',\n",
    "        inputs=inputs,\n",
    "        labels=labels\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "\n",
    "# Definimos la clase PeptideBERT, que hereda de torch.nn.Module (la clase base para todas las redes neuronales en PyTorch)\n",
    "class PeptideBERT(torch.nn.Module):\n",
    "    def __init__(self, bert_config): #constructor de la clase con parametros para bert\n",
    "        #llamamos al constructor de la clase base torch.nn.Module.\n",
    "        super(PeptideBERT, self).__init__()\n",
    "\n",
    "        #cargamos el modelo preentrenado\n",
    "        self.protbert = BertModel.from_pretrained(\n",
    "            'Rostlab/prot_bert_bfd', \n",
    "            config=bert_config, #pasamos la onfiguracion del modelo\n",
    "            ignore_mismatched_sizes=True \n",
    "        ) \n",
    "        #clasificacion\n",
    "        self.head = torch.nn.Sequential( \n",
    "            torch.nn.Linear(bert_config.hidden_size, 1), #toma la salida de protVert y la convierte en un valor\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    #definimos el metodo Fordward que especifica como procesar los datos\n",
    "    def forward(self, inputs, attention_mask):\n",
    "        #pasamos las entradas a través de ProtBert\n",
    "        output = self.protbert(inputs, attention_mask=attention_mask)\n",
    "        #usamos la salida de ProtBert como entrada a la capa de clasificacion\n",
    "        return self.head(output.pooler_output)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos y configuramos el modelo basado en los parámetros especificados en el diccionario config. (viene en el git)\n",
    "def create_model(config):\n",
    "    bert_config = BertConfig(\n",
    "        vocab_size=config['vocab_size'],\n",
    "        hidden_size=config['network']['hidden_size'],\n",
    "        num_hidden_layers=config['network']['hidden_layers'],\n",
    "        num_attention_heads=config['network']['attn_heads'],\n",
    "        hidden_dropout_prob=config['network']['dropout']\n",
    "    )\n",
    "    #creamos una istancia de PeptideBERT utilizando la configuración de BERT definida\n",
    "    model = PeptideBERT(bert_config).to(config['device'])\n",
    "    #regresamos el modelo\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterio de pérdida,optimizador y el planificador de learning rate  para el entrenamiento del modelo\n",
    "\n",
    "def cri_opt_sch(config, model):\n",
    "    #criterio de loss (BinaryCrossEntropy)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    #optimizador AmadW\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['optim']['lr'])\n",
    "    #Scheduler\n",
    "    if config['sch']['name'] == 'onecycle':\n",
    "        ## Durante el entrenamiento, el learning-rate empieza en un valor inicial, aumenta hasta el valor máximo especificado (max_lr), y luego disminuye nuevamente hacia el final del entrenamiento.\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=config['optim']['lr'],\n",
    "            epochs=config['epochs'],\n",
    "            steps_per_epoch=config['sch']['steps']\n",
    "        ) #Ajusta el learning-rate utilizando un ciclo de una sola pasada\n",
    "    elif config['sch']['name'] == 'lronplateau':\n",
    "        ## ajusta el learning-rate basándose en el rendimiento del modelo. Específicamente, reduce la tasa de aprendizaje cuando una métrica de rendimiento ha dejado de mejorar.\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='max',\n",
    "            factor=config['sch']['factor'],\n",
    "            patience=config['sch']['patience']\n",
    "        )# Reduce lr cuando la métrica especificada ha dejado de mejorar.\n",
    "\n",
    "    return criterion, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Función que se encarga del proceso de entrenamiento\n",
    "def train(model, dataloader, optimizer, criterion, scheduler, device):\n",
    "    model.train()  # Pone el modelo en modo de entrenamiento\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(dataloader):  # Itera sobre los lotes de datos en el dataloader\n",
    "        inputs = batch['input_ids'].to(device)  # Mueve las entradas al dispositivo (CPU o GPU)\n",
    "        attention_mask = batch['attention_mask'].to(device)  # Mueve la máscara de atención al dispositivo\n",
    "        labels = batch['labels'].to(device)  # Mueve las etiquetas al dispositivo\n",
    "\n",
    "        optimizer.zero_grad()  # Resetea los gradientes del optimizador\n",
    "\n",
    "        logits = model(inputs, attention_mask).squeeze(1)  # Pasa las entradas a través del modelo y ajusta las dimensiones\n",
    "        loss = criterion(logits, labels)  # Calcula la pérdida\n",
    "\n",
    "        loss.backward()  # Calcula los gradientes\n",
    "        optimizer.step()  # Actualiza los parámetros del modelo\n",
    "        # scheduler.step()  # Si el scheduler es OneCycleLR, ajusta la tasa de aprendizaje en cada paso\n",
    "\n",
    "        total_loss += loss.item()  # Acumula la pérdida total\n",
    "\n",
    "    return total_loss / len(dataloader)  # Retorna la pérdida promedio por lote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()  # Pone el modelo en modo de evaluación\n",
    "    total_loss = 0.0\n",
    "\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "\n",
    "    for batch in tqdm(dataloader):  # Itera sobre los lotes de datos en el dataloader\n",
    "        inputs = batch['input_ids'].to(device)  # Mueve las entradas al dispositivo\n",
    "        attention_mask = batch['attention_mask'].to(device)  # Mueve la máscara de atención al dispositivo\n",
    "        labels = batch['labels'].to(device)  # Mueve las etiquetas al dispositivo\n",
    "\n",
    "        with torch.inference_mode():  # Desactiva el cálculo de gradientes\n",
    "            logits = model(inputs, attention_mask).squeeze(1)  # Pasa las entradas a través del modelo\n",
    "            loss = criterion(logits, labels)  # Calcula la pérdida\n",
    "\n",
    "        total_loss += loss.item()  # Acumula la pérdida total\n",
    "        # Genera predicciones binarias\n",
    "        preds = torch.where(logits > 0.5, 1, 0)  \n",
    "        predictions.extend(preds.cpu().tolist())  # Añade las predicciones a la lista\n",
    "        ground_truth.extend(labels.cpu().tolist())  # Añade las etiquetas reales a la lista\n",
    "\n",
    "    total_loss = total_loss / len(dataloader)  # Calcula la pérdida promedio\n",
    "    accuracy = 100 * accuracy_score(ground_truth, predictions)  # Calcula la precisión\n",
    "\n",
    "    return total_loss, accuracy  # Retorna la pérdida promedio y la precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, device):\n",
    "    model.eval()  # Pone el modelo en modo de evaluación\n",
    "\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "\n",
    "    for batch in tqdm(dataloader):  # Itera sobre los lotes de datos en el dataloader\n",
    "        inputs = batch['input_ids'].to(device)  # Mueve las entradas al dispositivo\n",
    "        attention_mask = batch['attention_mask'].to(device)  # Mueve la máscara de atención al dispositivo\n",
    "        labels = batch['labels']  # Las etiquetas permanecen en la CPU\n",
    "\n",
    "        with torch.inference_mode():  # Desactiva el cálculo de gradientes\n",
    "            logits = model(inputs, attention_mask).squeeze(1)  # Pasa las entradas a través del modelo\n",
    "\n",
    "        preds = torch.where(logits > 0.5, 1, 0)  # Genera predicciones binarias\n",
    "        predictions.extend(preds.cpu().tolist())  # Añade las predicciones a la lista\n",
    "        ground_truth.extend(labels.tolist())  # Añade las etiquetas reales a la lista\n",
    "\n",
    "    accuracy = 100 * accuracy_score(ground_truth, predictions)  # Calcula la precisión\n",
    "\n",
    "    return accuracy  # Retorna la precisión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    print(f'{\"=\"*30}{\"TRAINING\":^20}{\"=\"*30}')\n",
    "\n",
    "    best_acc = 0 #inicializa la mejor precisión en cero\n",
    "\n",
    "    #iteramos cada época\n",
    "    for epoch in range(config['epochs']):\n",
    "\n",
    "        #llamamos a la funcion de entrenamiento\n",
    "        train_loss = train(model, train_data_loader, optimizer, criterion, scheduler, device)\n",
    "        #obtenemos learning rate\n",
    "        curr_lr = optimizer.param_groups[0]['lr']\n",
    "        #imprimimos loss de entrenamiento y learning rate\n",
    "        print(f'Epoch {epoch+1}/{config[\"epochs\"]} - Train Loss: {train_loss}\\tLR: {curr_lr}')\n",
    "        #imprimimos loss y accuracy de validacion\n",
    "        val_loss, val_acc = validate(model, val_data_loader, criterion, device)\n",
    "        print(f'Epoch {epoch+1}/{config[\"epochs\"]} - Validation Loss: {val_loss}\\tValidation Accuracy: {val_acc}\\n')\n",
    "        #Actualizar el Scheduler:\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        #Registrar Métricas con wandb\n",
    "        if not config['debug']:\n",
    "            wandb.log({\n",
    "                'train_loss': train_loss, \n",
    "                'val_loss': val_loss, \n",
    "                'val_accuracy': val_acc, \n",
    "                'lr': curr_lr\n",
    "            })\n",
    "        #Guardamos mejor modelo\n",
    "        if val_acc >= best_acc and not config['debug']:\n",
    "            best_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'acc': val_acc, \n",
    "                'lr': curr_lr\n",
    "            }, f'{save_dir}/model.pt')\n",
    "            print('Model Saved\\n')\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "##Configuración y Preparación###\n",
    "#llamamos al archivo donde se guarda la config del modelo peptidebert\n",
    "config = yaml.load(open('./config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "config['device'] = device\n",
    "\n",
    "#xargamos los datos\n",
    "train_data_loader, val_data_loader, test_data_loader = load_data(config)\n",
    "config['sch']['steps'] = len(train_data_loader)\n",
    "#creamos el modelo\n",
    "model = create_model(config)\n",
    "\n",
    "#configuramos criterio de pérdida, optimizador y scheduler\n",
    "criterion, optimizer, scheduler = cri_opt_sch(config, model)\n",
    "\n",
    "\n",
    "#Configuración de Weights & Biases (WandB)\n",
    "if not config['debug']:\n",
    "    run_name = f'{config[\"task\"]}-{datetime.now().strftime(\"%m%d_%H%M\")}'\n",
    "    wandb.init(project='PeptideBERT', name=run_name)\n",
    "\n",
    "    save_dir = f'./checkpoints/{run_name}'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    shutil.copy('./config.yaml', f'{save_dir}/config.yaml')\n",
    "    shutil.copy('./model/network.py', f'{save_dir}/network.py')\n",
    "\n",
    "#Entrenamiento del Modelo\n",
    "train_model()\n",
    "if not config['debug']:\n",
    "    model.load_state_dict(torch.load(f'{save_dir}/model.pt')['model_state_dict'], strict=False)\n",
    "\n",
    "#test\n",
    "test_acc = test(model, test_data_loader, device)\n",
    "print(f'Test Accuracy: {test_acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
