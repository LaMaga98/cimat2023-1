{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{PLN. Tarea 2: Minería de texto básica}$$\n",
    "$$\\textit{Y. Sarahi García Gozález}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import Counter\n",
    "import matplotlib.pylab as plt\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "#clasificacion\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,precision_recall_fscore_support,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarea realizada en MacOs. \n",
      "Las versiones de las librerías y de python utilizadas fueron:\n",
      "\n",
      "Python version 3.11.5\n",
      "Numpy version 1.23.5\n",
      "NLTK version 3.8.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Tarea realizada en MacOs. \\nLas versiones de las librerías y de python utilizadas fueron:\\n\")\n",
    "from platform import python_version\n",
    "print(\"Python version\", python_version())\n",
    "print(\"Numpy version\", np.__version__)\n",
    "print(\"NLTK version\", nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4.5 color='lightblue'>\n",
    "\n",
    "$\\textit{Ejercicio 2. Bolsas de Palabras, Bigramas y Emociones}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Preparación de texto, corpus y diccionarios}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_text_from_file(path_corpus,path_truth):\n",
    "\n",
    "    tr_txt=[]\n",
    "    tr_labels=[]\n",
    "\n",
    "    with open(path_corpus, \"r\") as f_corpus,open(path_truth, \"r\") as f_truth:\n",
    "        for tweet in f_corpus:\n",
    "            tr_txt += [tweet]\n",
    "        for label in f_truth:\n",
    "            tr_labels += [label]   \n",
    "             \n",
    "    return tr_txt, tr_labels\n",
    "\n",
    "\n",
    "def create_corpus_from_text(text,tokenizer):\n",
    "    corpus_palabras=[]\n",
    "    for documento in text:\n",
    "        corpus_palabras+=tokenizer.tokenize(documento)\n",
    "\n",
    "    return corpus_palabras\n",
    "\n",
    "\n",
    "def create_dic_freq(corpus,n):\n",
    "    fdist = nltk.FreqDist(corpus)\n",
    "    aux=[(fdist[key],key) for key in fdist]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    aux=aux[:n]\n",
    "\n",
    "    return aux\n",
    "\n",
    "def create_dic_ranking(dic_freq):\n",
    "    dict_indices=dict()\n",
    "    cont = 0\n",
    "    for weight, word in dic_freq:\n",
    "        dict_indices[word]= cont\n",
    "        cont+= 1\n",
    "\n",
    "    return dict_indices\n",
    "\n",
    "#Clasificador\n",
    "\n",
    "parameters={'C':[0.05,0.12,0.25,0.5,1,2,4]} #parámetro de complejidad\n",
    "\n",
    "#llamamos el clasificador SVM\n",
    "#nota: cambié el max_iter y dual porque me aparecían una serie de warnings pero no cambiaron los resultados\n",
    "svr=svm.LinearSVC(class_weight='balanced',dual='auto',max_iter=3000) \n",
    "\n",
    "#grid search recide el objeto de clasificacion\n",
    "grid=GridSearchCV(estimator=svr,param_grid=parameters,n_jobs=8,scoring=\"f1_macro\",cv=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardamos los textos de entrenamiento y validación\n",
    "tr_txt,tr_labels=get_text_from_file(\"/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/practicas/03_practica/mex20_train.txt\",\"/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/practicas/03_practica/mex20_train_labels.txt\")\n",
    "val_txt,val_labels=get_text_from_file(\"/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/practicas/03_practica/mex20_val.txt\",\"/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/practicas/03_practica/mex20_val_labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connvertimos a lista las etiquetas\n",
    "tr_labels=list(map(int,tr_labels))\n",
    "val_labels=list(map(int,val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizamos\n",
    "tokenizer=TweetTokenizer()\n",
    "#Generamos el corpus\n",
    "corpus_palabras=create_corpus_from_text(tr_txt,tokenizer)\n",
    "#Generamos diccionario de frecuencias con las primeras 5 mil palabras\n",
    "dict_freq=create_dic_freq(corpus_palabras,5000)\n",
    "#Generamos diccionario de indices\n",
    "dict_indices=create_dic_ranking(dict_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me - 10\n",
      "el - 11\n",
      "en - 12\n",
      "se - 13\n",
      "es - 14\n",
      "con - 15\n",
      "? - 16\n",
      "verga - 17\n",
      "los - 18\n",
      "madre - 19\n",
      "por - 20\n",
      "las - 21\n",
      "\" - 22\n",
      "un - 23\n",
      "te - 24\n",
      "mi - 25\n",
      "lo - 26\n",
      "putas - 27\n",
      "una - 28\n",
      "... - 29\n"
     ]
    }
   ],
   "source": [
    "#imprimimos algunos elementos del diccionario de indices\n",
    "lista_indices = list(dict_indices.items())\n",
    "for clave, valor in lista_indices[10:30]:\n",
    "    print(clave, \"-\", valor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3.5 color='lightblue'>\n",
    "\n",
    "1,2,3. Evalue BoW con pesado binario, de frecuencia y tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Bolsas de palabras con distintos equemas de pesado y sin normalizar}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#________________________________PESADO_BINARIO________________________________#\n",
    "\n",
    "\n",
    "#matriz_nxm de bolsa de palabras n=numero de textos, m=numero de palabras (5000)\n",
    "def build_binary_bow(tr_txt,dic_freq,dic_indices):\n",
    "    '''\n",
    "    Función que genera la bolsa de palabras con pesado binario\n",
    "\n",
    "    '''\n",
    "    BoW=np.zeros((len(tr_txt),len(dic_freq)),dtype=int) #construimos bolsta de palabras en ceros\n",
    "    cont_documento=0 #indice que recorre las FILAS\n",
    "\n",
    "    for tr in tr_txt:#recorremos cada documento\n",
    "        fdist_doc=nltk.FreqDist(tokenizer.tokenize(tr))#hacemos freqDist (tokenizado con el tokenizador ya definido) de cada documento\n",
    "        for word in fdist_doc: #para cada palabra en el diccionario del documento\n",
    "            if word in dic_indices: #si la palabra está en el corte final\n",
    "                #AGREGAMOS un uno en el lugar correspondiente\n",
    "                BoW[cont_documento,dic_indices[word]] = 1 \n",
    "\n",
    "        cont_documento+=1\n",
    "\n",
    "    return BoW\n",
    "\n",
    "#________________________________PESADO_FRECUENCIA________________________________#\n",
    "\n",
    "#matriz_nxm de bolsa de palabras n=numero de textos, m=numero de palabras (5000)\n",
    "def build_freq_bow(tr_txt,dic_feq,dic_inices):\n",
    "    BoW=np.zeros((len(tr_txt),len(dic_feq)),dtype=int) #construimos bolsta de palabras en ceros\n",
    "    cont_documento=0 #indice que recorre las FILAS\n",
    " \n",
    "    for tr in tr_txt:#recorremos cada documento\n",
    "        fdist_doc=nltk.FreqDist(tokenizer.tokenize(tr))#hacemos freqDist (tokenizado con el tokenizador ya definido) de cada documento\n",
    "        for word in fdist_doc: #para cada palabra en el diccionario del documento\n",
    "            if word in dic_inices: #si la palabra está en el corte final\n",
    "                #le asignamos su frecuencia\n",
    "                BoW[cont_documento,dic_inices[word]] = fdist_doc[word]\n",
    "\n",
    "        cont_documento+=1\n",
    "\n",
    "    return BoW\n",
    "\n",
    "#________________________________PESADO_TFIDF________________________________#\n",
    "\n",
    "#primero calculamos el num de documentos en que aparece cada palabra\n",
    "def df(tr_txt, dic_indices):\n",
    "    df = np.zeros(5000, dtype=int)\n",
    "\n",
    "    # comp list para las frecuencias de cada documento\n",
    "    fdist_docs = [nltk.FreqDist(tokenizer.tokenize(tr)) for tr in tr_txt]\n",
    "\n",
    "    #iteramos sobre cada palbra\n",
    "    for word in dic_indices:\n",
    "        #obtenemos el indice de la palabra\n",
    "        index = dic_indices[word] \n",
    "        for j,fdist_doc in enumerate(fdist_docs): #iteramos osbre cada documento\n",
    "            if word in fdist_doc: #si la palabra está en el documento actual\n",
    "                df[index] += 1 #agredamos uno al conador de esa palabra\n",
    "\n",
    "    return df\n",
    "\n",
    "#\n",
    "def build_tfidf_bow(tr_txt, dic_freq, dic_indices):\n",
    "    BoW = build_freq_bow(tr_txt, dic_freq, dic_indices) #la bolsa de palabras de frecuencias es el tf\n",
    "    df_list = df(tr_txt, dic_indices) #llamamos a la duncion df\n",
    "    N = len(tr_txt)\n",
    "   \n",
    "    for i in range(N):\n",
    "        for j,df_val in enumerate(df_list): \n",
    "            if df_val!=0: #si df es distinto de ceo (puede ser cero para el conjunto de validacion, por ejemplo que contiene menos lementos )\n",
    "                BoW[i][j] *= np.log(N / df_val) #tfxdf\n",
    "\n",
    "    return BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 0,\n",
       " 'de': 1,\n",
       " 'que': 2,\n",
       " '.': 3,\n",
       " 'la': 4,\n",
       " 'a': 5,\n",
       " 'y': 6,\n",
       " '!': 7,\n",
       " 'no': 8,\n",
       " '@USUARIO': 9,\n",
       " 'me': 10,\n",
       " 'el': 11,\n",
       " 'en': 12,\n",
       " 'se': 13,\n",
       " 'es': 14,\n",
       " 'con': 15,\n",
       " '?': 16,\n",
       " 'verga': 17,\n",
       " 'los': 18,\n",
       " 'madre': 19,\n",
       " 'por': 20,\n",
       " 'las': 21,\n",
       " '\"': 22,\n",
       " 'un': 23,\n",
       " 'te': 24,\n",
       " 'mi': 25,\n",
       " 'lo': 26,\n",
       " 'putas': 27,\n",
       " 'una': 28,\n",
       " '...': 29,\n",
       " 'putos': 30,\n",
       " 'para': 31,\n",
       " '😂': 32,\n",
       " 'si': 33,\n",
       " 'ya': 34,\n",
       " 'como': 35,\n",
       " 'su': 36,\n",
       " 'pero': 37,\n",
       " 'tu': 38,\n",
       " 'loca': 39,\n",
       " 'le': 40,\n",
       " 'más': 41,\n",
       " 'No': 42,\n",
       " 'del': 43,\n",
       " 'gorda': 44,\n",
       " 'al': 45,\n",
       " 'bien': 46,\n",
       " 'A': 47,\n",
       " '¿': 48,\n",
       " 'Y': 49,\n",
       " 'son': 50,\n",
       " 'Me': 51,\n",
       " 'o': 52,\n",
       " 'feas': 53,\n",
       " 'cuando': 54,\n",
       " 'Que': 55,\n",
       " ':': 56,\n",
       " 'yo': 57,\n",
       " 'les': 58,\n",
       " 'porque': 59,\n",
       " 'ni': 60,\n",
       " 'está': 61,\n",
       " 'ser': 62,\n",
       " 'estoy': 63,\n",
       " 'sus': 64,\n",
       " 'todos': 65,\n",
       " 'esta': 66,\n",
       " 'puta': 67,\n",
       " 'Ya': 68,\n",
       " 'todo': 69,\n",
       " 'pinche': 70,\n",
       " 'puto': 71,\n",
       " 'tan': 72,\n",
       " 'Si': 73,\n",
       " 'La': 74,\n",
       " 'qué': 75,\n",
       " '…': 76,\n",
       " 'eso': 77,\n",
       " 'muy': 78,\n",
       " 'soy': 79,\n",
       " 'hasta': 80,\n",
       " 'así': 81,\n",
       " '¡': 82,\n",
       " '<URL>': 83,\n",
       " 'mamar': 84,\n",
       " 'hay': 85,\n",
       " 'q': 86,\n",
       " 'DE': 87,\n",
       " 'mis': 88,\n",
       " 'joto': 89,\n",
       " 'hace': 90,\n",
       " 'este': 91,\n",
       " 'cosas': 92,\n",
       " '️': 93,\n",
       " 'vida': 94,\n",
       " 'nos': 95,\n",
       " 'ver': 96,\n",
       " 'mejor': 97,\n",
       " 'solo': 98,\n",
       " 'nada': 99,\n",
       " 'vale': 100,\n",
       " 'va': 101,\n",
       " 'quiero': 102,\n",
       " 'marica': 103,\n",
       " 'eres': 104,\n",
       " 'día': 105,\n",
       " 'siempre': 106,\n",
       " 'esa': 107,\n",
       " 'voy': 108,\n",
       " 'gente': 109,\n",
       " 'Yo': 110,\n",
       " '😭': 111,\n",
       " 'vez': 112,\n",
       " 'El': 113,\n",
       " 'mierda': 114,\n",
       " '-': 115,\n",
       " 'tengo': 116,\n",
       " '(': 117,\n",
       " 'sin': 118,\n",
       " 'ese': 119,\n",
       " ')': 120,\n",
       " 'Es': 121,\n",
       " 'luchona': 122,\n",
       " '😍': 123,\n",
       " 'hdp': 124,\n",
       " 'ahora': 125,\n",
       " 'Por': 126,\n",
       " '😡': 127,\n",
       " '“': 128,\n",
       " 'tienen': 129,\n",
       " 'tiene': 130,\n",
       " 'pinches': 131,\n",
       " 'hacer': 132,\n",
       " 'tus': 133,\n",
       " 'tontas': 134,\n",
       " 'LA': 135,\n",
       " '”': 136,\n",
       " 'gusta': 137,\n",
       " 'Como': 138,\n",
       " 'sea': 139,\n",
       " 'HDP': 140,\n",
       " 'toda': 141,\n",
       " 'Se': 142,\n",
       " 'hoy': 143,\n",
       " 'Qué': 144,\n",
       " 'mamando': 145,\n",
       " 'están': 146,\n",
       " 'cagado': 147,\n",
       " 'tonta': 148,\n",
       " 'Pero': 149,\n",
       " 'puedo': 150,\n",
       " 'mas': 151,\n",
       " '🙄': 152,\n",
       " 'pendejo': 153,\n",
       " 'hijo': 154,\n",
       " 'NO': 155,\n",
       " 'En': 156,\n",
       " 'Mi': 157,\n",
       " 'mal': 158,\n",
       " 'estar': 159,\n",
       " 'QUE': 160,\n",
       " '..': 161,\n",
       " 'Lo': 162,\n",
       " 'algo': 163,\n",
       " 'PUTOS': 164,\n",
       " 'tener': 165,\n",
       " 'alguien': 166,\n",
       " 'Putos': 167,\n",
       " 'verdad': 168,\n",
       " 'mujer': 169,\n",
       " 'cabrona': 170,\n",
       " 'también': 171,\n",
       " 'da': 172,\n",
       " 'puede': 173,\n",
       " 'decir': 174,\n",
       " 'madres': 175,\n",
       " 'mujeres': 176,\n",
       " 'maricon': 177,\n",
       " 'vas': 178,\n",
       " 'mucho': 179,\n",
       " 'dos': 180,\n",
       " 'MADRE': 181,\n",
       " '❤': 182,\n",
       " 'van': 183,\n",
       " 'sé': 184,\n",
       " 'Estoy': 185,\n",
       " 'Cuando': 186,\n",
       " ';': 187,\n",
       " 'sí': 188,\n",
       " 'otra': 189,\n",
       " 'estás': 190,\n",
       " 'años': 191,\n",
       " 'Verga': 192,\n",
       " 'PUTAS': 193,\n",
       " 'Las': 194,\n",
       " 'ir': 195,\n",
       " 'chingada': 196,\n",
       " \"'\": 197,\n",
       " 'veces': 198,\n",
       " 'tú': 199,\n",
       " 'hijos': 200,\n",
       " 'De': 201,\n",
       " 'quiere': 202,\n",
       " 'quien': 203,\n",
       " 'pues': 204,\n",
       " 'jajaja': 205,\n",
       " 'VERGA': 206,\n",
       " 'Te': 207,\n",
       " '3': 208,\n",
       " 'mundo': 209,\n",
       " 'menos': 210,\n",
       " '2': 211,\n",
       " '🤔': 212,\n",
       " 'uno': 213,\n",
       " 'nunca': 214,\n",
       " 'era': 215,\n",
       " 'cada': 216,\n",
       " 'días': 217,\n",
       " 'dice': 218,\n",
       " 'México': 219,\n",
       " 'todas': 220,\n",
       " 'esos': 221,\n",
       " 'amor': 222,\n",
       " 'Una': 223,\n",
       " 'tanto': 224,\n",
       " 'mamá': 225,\n",
       " 'ganas': 226,\n",
       " 'esas': 227,\n",
       " 'O': 228,\n",
       " '😒': 229,\n",
       " 'tienes': 230,\n",
       " 'tiempo': 231,\n",
       " 'fue': 232,\n",
       " 'cuenta': 233,\n",
       " 'ME': 234,\n",
       " 'Jajajaja': 235,\n",
       " 'pendeja': 236,\n",
       " 'estaba': 237,\n",
       " 'dicen': 238,\n",
       " 'Madre': 239,\n",
       " 'wey': 240,\n",
       " 'pedo': 241,\n",
       " 'neta': 242,\n",
       " 'esto': 243,\n",
       " 'ti': 244,\n",
       " 'siento': 245,\n",
       " 'mañana': 246,\n",
       " 'igual': 247,\n",
       " 'he': 248,\n",
       " 'd': 249,\n",
       " 'camote': 250,\n",
       " 'Los': 251,\n",
       " 'veo': 252,\n",
       " 'personas': 253,\n",
       " 'pasa': 254,\n",
       " 'hacen': 255,\n",
       " 'ha': 256,\n",
       " 'donde': 257,\n",
       " 'digo': 258,\n",
       " 'Hoy': 259,\n",
       " '/': 260,\n",
       " 'v': 261,\n",
       " 'unas': 262,\n",
       " 'bueno': 263,\n",
       " ':(': 264,\n",
       " 'fotos': 265,\n",
       " 'cabrón': 266,\n",
       " 'Ahora': 267,\n",
       " 'otro': 268,\n",
       " 'mismo': 269,\n",
       " 'cabron': 270,\n",
       " 'buena': 271,\n",
       " 'ahí': 272,\n",
       " 'trabajo': 273,\n",
       " 'sabe': 274,\n",
       " 'nadie': 275,\n",
       " 'estas': 276,\n",
       " 'desde': 277,\n",
       " 'amigos': 278,\n",
       " 'alv': 279,\n",
       " 'Pinche': 280,\n",
       " 'mí': 281,\n",
       " 'hombres': 282,\n",
       " 'foto': 283,\n",
       " 'culo': 284,\n",
       " 'casa': 285,\n",
       " 'ardida': 286,\n",
       " 'Jajaja': 287,\n",
       " '*': 288,\n",
       " '😈': 289,\n",
       " 've': 290,\n",
       " 'valer': 291,\n",
       " 'han': 292,\n",
       " 'fuera': 293,\n",
       " 'e': 294,\n",
       " 'chingar': 295,\n",
       " 'Pues': 296,\n",
       " 'Esta': 297,\n",
       " 'pendejos': 298,\n",
       " 'mames': 299,\n",
       " 'ella': 300,\n",
       " 'cara': 301,\n",
       " '🤣': 302,\n",
       " 'sólo': 303,\n",
       " 'mil': 304,\n",
       " 'súper': 305,\n",
       " 'rico': 306,\n",
       " 'noche': 307,\n",
       " 'mundial': 308,\n",
       " 'luego': 309,\n",
       " 'jajajaja': 310,\n",
       " 'fin': 311,\n",
       " 'estos': 312,\n",
       " 'encanta': 313,\n",
       " 'amigo': 314,\n",
       " 'Tu': 315,\n",
       " 'EL': 316,\n",
       " '😩': 317,\n",
       " '🎶': 318,\n",
       " 'ustedes': 319,\n",
       " 'semana': 320,\n",
       " 'quieren': 321,\n",
       " 'pueden': 322,\n",
       " 'mientras': 323,\n",
       " 'aquí': 324,\n",
       " 'antes': 325,\n",
       " 'dan': 326,\n",
       " 'Para': 327,\n",
       " 'Porque': 328,\n",
       " 'Gracias': 329,\n",
       " 'persona': 330,\n",
       " 'jaja': 331,\n",
       " 'gay': 332,\n",
       " 'dinero': 333,\n",
       " 'dijo': 334,\n",
       " 'creo': 335,\n",
       " 'buen': 336,\n",
       " 'amo': 337,\n",
       " 'YA': 338,\n",
       " 'PUTA': 339,\n",
       " 'ven': 340,\n",
       " 'sabes': 341,\n",
       " 'novio': 342,\n",
       " 'dar': 343,\n",
       " 'creen': 344,\n",
       " 'chingo': 345,\n",
       " 'caga': 346,\n",
       " 'Un': 347,\n",
       " 'Está': 348,\n",
       " 'Con': 349,\n",
       " 'Así': 350,\n",
       " 'unos': 351,\n",
       " 'putita': 352,\n",
       " 'perra': 353,\n",
       " 'peor': 354,\n",
       " 'gata': 355,\n",
       " 'falta': 356,\n",
       " 'digan': 357,\n",
       " 'deja': 358,\n",
       " 'aún': 359,\n",
       " 'amiga': 360,\n",
       " 'Soy': 361,\n",
       " 'Putas': 362,\n",
       " 'EN': 363,\n",
       " '4': 364,\n",
       " '1': 365,\n",
       " '😠': 366,\n",
       " 'triste': 367,\n",
       " 'puedes': 368,\n",
       " 'mamen': 369,\n",
       " 'horas': 370,\n",
       " 'hablar': 371,\n",
       " 'entiendo': 372,\n",
       " 'dejar': 373,\n",
       " 'sean': 374,\n",
       " 'saben': 375,\n",
       " 'quién': 376,\n",
       " 'pelan': 377,\n",
       " 'país': 378,\n",
       " 'novia': 379,\n",
       " 'hora': 380,\n",
       " 'haciendo': 381,\n",
       " 'dormir': 382,\n",
       " 'después': 383,\n",
       " 'andar': 384,\n",
       " 'Ni': 385,\n",
       " '5': 386,\n",
       " '🇲🇽': 387,\n",
       " 'viendo': 388,\n",
       " 'vamos': 389,\n",
       " 'tarea': 390,\n",
       " 'tal': 391,\n",
       " 'prieta': 392,\n",
       " 'mamadas': 393,\n",
       " 'gusto': 394,\n",
       " 'cosa': 395,\n",
       " 'asi': 396,\n",
       " 'asco': 397,\n",
       " 'Ojalá': 398,\n",
       " 'Hay': 399,\n",
       " '😤': 400,\n",
       " '😘': 401,\n",
       " 'valió': 402,\n",
       " 'señora': 403,\n",
       " 'saber': 404,\n",
       " 'rica': 405,\n",
       " 'palabra': 406,\n",
       " 'nalgas': 407,\n",
       " 'maricón': 408,\n",
       " 'hago': 409,\n",
       " 'había': 410,\n",
       " 'ellos': 411,\n",
       " 'cómo': 412,\n",
       " 'boca': 413,\n",
       " 'SU': 414,\n",
       " 'LOS': 415,\n",
       " '#MasterChefMx': 416,\n",
       " '😏': 417,\n",
       " 'poca': 418,\n",
       " 'perro': 419,\n",
       " 'mandar': 420,\n",
       " 'llorar': 421,\n",
       " 'hombre': 422,\n",
       " 'chingas': 423,\n",
       " 'año': 424,\n",
       " 'Todos': 425,\n",
       " 'Tengo': 426,\n",
       " 'Pinches': 427,\n",
       " '😌': 428,\n",
       " '💔': 429,\n",
       " '🍆': 430,\n",
       " 'él': 431,\n",
       " 'vergas': 432,\n",
       " 'sigue': 433,\n",
       " 'risa': 434,\n",
       " 're': 435,\n",
       " 'quieres': 436,\n",
       " 'queda': 437,\n",
       " 'puro': 438,\n",
       " 'ponen': 439,\n",
       " 'pone': 440,\n",
       " 'otras': 441,\n",
       " 'odio': 442,\n",
       " 'misma': 443,\n",
       " 'miedo': 444,\n",
       " 'iba': 445,\n",
       " 'hubiera': 446,\n",
       " 'golfa': 447,\n",
       " 'ex': 448,\n",
       " 'dejen': 449,\n",
       " 'debe': 450,\n",
       " 'bonito': 451,\n",
       " 'PARA': 452,\n",
       " 'siguen': 453,\n",
       " 'pobre': 454,\n",
       " 'parte': 455,\n",
       " 'importa': 456,\n",
       " 'hizo': 457,\n",
       " 'hija': 458,\n",
       " 'feo': 459,\n",
       " 'feliz': 460,\n",
       " 'fea': 461,\n",
       " 'favor': 462,\n",
       " 'culpa': 463,\n",
       " 'Quiero': 464,\n",
       " 'Este': 465,\n",
       " 'Alguien': 466,\n",
       " '10': 467,\n",
       " '🔥': 468,\n",
       " 'vieja': 469,\n",
       " 'valiendo': 470,\n",
       " 'tarde': 471,\n",
       " 'seguro': 472,\n",
       " 'salir': 473,\n",
       " 'puñal': 474,\n",
       " 'poner': 475,\n",
       " 'pensar': 476,\n",
       " 'partido': 477,\n",
       " 'minutos': 478,\n",
       " 'lugar': 479,\n",
       " 'llega': 480,\n",
       " 'diga': 481,\n",
       " 'chinga': 482,\n",
       " 'canción': 483,\n",
       " 'ando': 484,\n",
       " 'anda': 485,\n",
       " 'RT': 486,\n",
       " 'PUTO': 487,\n",
       " 'Eso': 488,\n",
       " '$': 489,\n",
       " '🙃': 490,\n",
       " '😞': 491,\n",
       " '😔': 492,\n",
       " 'somos': 493,\n",
       " 'sido': 494,\n",
       " 'pena': 495,\n",
       " 'parece': 496,\n",
       " 'momento': 497,\n",
       " 'mando': 498,\n",
       " 'lameculos': 499,\n",
       " 'huevos': 500,\n",
       " 'hermano': 501,\n",
       " 'familia': 502,\n",
       " 'entre': 503,\n",
       " 'contigo': 504,\n",
       " 'bonita': 505,\n",
       " 'agua': 506,\n",
       " 'acabo': 507,\n",
       " 'Siempre': 508,\n",
       " 'Neta': 509,\n",
       " 'Les': 510,\n",
       " 'Le': 511,\n",
       " 'Hasta': 512,\n",
       " 'ES': 513,\n",
       " '🤦🏻\\u200d♀': 514,\n",
       " '😎': 515,\n",
       " '💦': 516,\n",
       " '|': 517,\n",
       " 'volver': 518,\n",
       " 'visto': 519,\n",
       " 'viejas': 520,\n",
       " 'ves': 521,\n",
       " 'valen': 522,\n",
       " 'sobre': 523,\n",
       " 'servicio': 524,\n",
       " 'seas': 525,\n",
       " 'sale': 526,\n",
       " 'primera': 527,\n",
       " 'pongo': 528,\n",
       " 'poder': 529,\n",
       " 'perros': 530,\n",
       " 'pasan': 531,\n",
       " 'pasado': 532,\n",
       " 'nuevo': 533,\n",
       " 'mama': 534,\n",
       " 'lado': 535,\n",
       " 'fui': 536,\n",
       " 'forma': 537,\n",
       " 'escuela': 538,\n",
       " 'escuchar': 539,\n",
       " 'dije': 540,\n",
       " 'dicho': 541,\n",
       " 'dices': 542,\n",
       " 'conmigo': 543,\n",
       " 'Twitter': 544,\n",
       " 'TU': 545,\n",
       " 'Solo': 546,\n",
       " 'SE': 547,\n",
       " 'Jajajajaja': 548,\n",
       " 'Gorda': 549,\n",
       " 'Dios': 550,\n",
       " 'Cómo': 551,\n",
       " 'Creo': 552,\n",
       " 'Ay': 553,\n",
       " 'Aquí': 554,\n",
       " 'Ah': 555,\n",
       " '7': 556,\n",
       " '20': 557,\n",
       " '😱': 558,\n",
       " 'vista': 559,\n",
       " 'video': 560,\n",
       " 'valgo': 561,\n",
       " 'tanta': 562,\n",
       " 'sigo': 563,\n",
       " 'será': 564,\n",
       " 'rato': 565,\n",
       " 'problema': 566,\n",
       " 'pa': 567,\n",
       " 'otros': 568,\n",
       " 'música': 569,\n",
       " 'llevo': 570,\n",
       " 'lleva': 571,\n",
       " 'jajajajaja': 572,\n",
       " 'hecho': 573,\n",
       " 'haga': 574,\n",
       " 'haber': 575,\n",
       " 'grande': 576,\n",
       " 'gracias': 577,\n",
       " 'entonces': 578,\n",
       " 'doy': 579,\n",
       " 'diciendo': 580,\n",
       " 'chiflar': 581,\n",
       " 'cagan': 582,\n",
       " 'buenas': 583,\n",
       " 'andan': 584,\n",
       " 'amigas': 585,\n",
       " 'V': 586,\n",
       " 'Su': 587,\n",
       " 'POR': 588,\n",
       " 'PINCHE': 589,\n",
       " 'LOCA': 590,\n",
       " 'JAJAJA': 591,\n",
       " '🙊': 592,\n",
       " '😣': 593,\n",
       " '☹': 594,\n",
       " 'único': 595,\n",
       " 'xD': 596,\n",
       " 'vi': 597,\n",
       " 'tres': 598,\n",
       " 'trabajar': 599,\n",
       " 'siendo': 600,\n",
       " 'seguir': 601,\n",
       " 'quiera': 602,\n",
       " 'quería': 603,\n",
       " 'primero': 604,\n",
       " 'papá': 605,\n",
       " 'padre': 606,\n",
       " 'ojalá': 607,\n",
       " 'niño': 608,\n",
       " 'niñas': 609,\n",
       " 'niña': 610,\n",
       " 'meter': 611,\n",
       " 'hablando': 612,\n",
       " 'final': 613,\n",
       " 'estamos': 614,\n",
       " 'demás': 615,\n",
       " 'das': 616,\n",
       " 'comer': 617,\n",
       " 'clase': 618,\n",
       " 'caso': 619,\n",
       " 'casi': 620,\n",
       " 'UN': 621,\n",
       " 'TE': 622,\n",
       " 'Mis': 623,\n",
       " 'Marica': 624,\n",
       " 'LO': 625,\n",
       " 'LAS': 626,\n",
       " 'Eres': 627,\n",
       " 'COMO': 628,\n",
       " '😬': 629,\n",
       " '😢': 630,\n",
       " '😊': 631,\n",
       " '😁': 632,\n",
       " 'viejo': 633,\n",
       " 'tipo': 634,\n",
       " 'sola': 635,\n",
       " 'respeto': 636,\n",
       " 'razón': 637,\n",
       " 'pasar': 638,\n",
       " 'palabras': 639,\n",
       " 'pagar': 640,\n",
       " 'nombre': 641,\n",
       " 'noches': 642,\n",
       " 'medio': 643,\n",
       " 'mano': 644,\n",
       " 'llama': 645,\n",
       " 'huevo': 646,\n",
       " 'hambre': 647,\n",
       " 'haces': 648,\n",
       " 'esperando': 649,\n",
       " 'equipo': 650,\n",
       " 'cualquier': 651,\n",
       " 'creer': 652,\n",
       " 'chinguen': 653,\n",
       " 'chica': 654,\n",
       " 'celular': 655,\n",
       " 'calle': 656,\n",
       " 'aunque': 657,\n",
       " 'ardidas': 658,\n",
       " 'Sí': 659,\n",
       " 'Quien': 660,\n",
       " 'Puto': 661,\n",
       " 'Mira': 662,\n",
       " 'Mañana': 663,\n",
       " 'Esa': 664,\n",
       " 'Al': 665,\n",
       " '6': 666,\n",
       " '&': 667,\n",
       " '🤤': 668,\n",
       " '😜': 669,\n",
       " 'vos': 670,\n",
       " 'videos': 671,\n",
       " 'vaya': 672,\n",
       " 'tenía': 673,\n",
       " 'siente': 674,\n",
       " 'punto': 675,\n",
       " 'porqué': 676,\n",
       " 'poco': 677,\n",
       " 'paso': 678,\n",
       " 'partir': 679,\n",
       " 'parecen': 680,\n",
       " 'ojos': 681,\n",
       " 'nosotros': 682,\n",
       " 'maldito': 683,\n",
       " 'lluvia': 684,\n",
       " 'hice': 685,\n",
       " 'gordas': 686,\n",
       " 'estan': 687,\n",
       " 'estado': 688,\n",
       " 'eran': 689,\n",
       " 'ellas': 690,\n",
       " 'dónde': 691,\n",
       " 'den': 692,\n",
       " 'deberían': 693,\n",
       " 'culero': 694,\n",
       " 'corazón': 695,\n",
       " 'clases': 696,\n",
       " 'cargo': 697,\n",
       " 'cae': 698,\n",
       " 'ayer': 699,\n",
       " 'además': 700,\n",
       " 'Todo': 701,\n",
       " 'Teresa': 702,\n",
       " 'TODOS': 703,\n",
       " 'SI': 704,\n",
       " 'Puta': 705,\n",
       " 'Jajajajajaja': 706,\n",
       " 'HIJO': 707,\n",
       " 'CON': 708,\n",
       " '@': 709,\n",
       " '😪': 710,\n",
       " '😕': 711,\n",
       " '😋': 712,\n",
       " 'x': 713,\n",
       " 'vuelve': 714,\n",
       " 'viene': 715,\n",
       " 'usar': 716,\n",
       " 'tengan': 717,\n",
       " 'tenemos': 718,\n",
       " 'tambien': 719,\n",
       " 'tacos': 720,\n",
       " 'sueño': 721,\n",
       " 'serio': 722,\n",
       " 'salen': 723,\n",
       " 'sabemos': 724,\n",
       " 'pura': 725,\n",
       " 'pueblo': 726,\n",
       " 'poniendo': 727,\n",
       " 'políticos': 728,\n",
       " 'pienso': 729,\n",
       " 'periodistas': 730,\n",
       " 'pendejas': 731,\n",
       " 'nivel': 732,\n",
       " 'meses': 733,\n",
       " 'mes': 734,\n",
       " 'mariquita': 735,\n",
       " 'lleno': 736,\n",
       " 'llegar': 737,\n",
       " 'juego': 738,\n",
       " 'historia': 739,\n",
       " 'hermoso': 740,\n",
       " 'hagan': 741,\n",
       " 'grupo': 742,\n",
       " 'fan': 743,\n",
       " 'espero': 744,\n",
       " 'escribir': 745,\n",
       " 'dio': 746,\n",
       " 'demasiado': 747,\n",
       " 'deje': 748,\n",
       " 'dado': 749,\n",
       " 'contra': 750,\n",
       " 'arruga': 751,\n",
       " 'Wey': 752,\n",
       " 'Son': 753,\n",
       " 'Quién': 754,\n",
       " 'HIJOS': 755,\n",
       " 'Chinga': 756,\n",
       " 'ALV': 757,\n",
       " '🙈': 758,\n",
       " '😻': 759,\n",
       " '😅': 760,\n",
       " '💕': 761,\n",
       " 'volviendo': 762,\n",
       " 'vato': 763,\n",
       " 'tuits': 764,\n",
       " 'tenga': 765,\n",
       " 'salió': 766,\n",
       " 'putito': 767,\n",
       " 'primer': 768,\n",
       " 'pones': 769,\n",
       " 'pesos': 770,\n",
       " 'pela': 771,\n",
       " 'partidos': 772,\n",
       " 'nuestros': 773,\n",
       " 'nuestro': 774,\n",
       " 'necesito': 775,\n",
       " 'muchas': 776,\n",
       " 'morra': 777,\n",
       " 'moral': 778,\n",
       " 'mayor': 779,\n",
       " 'matar': 780,\n",
       " 'manos': 781,\n",
       " 'mamo': 782,\n",
       " 'mamaste': 783,\n",
       " 'maldita': 784,\n",
       " 'mala': 785,\n",
       " 'leche': 786,\n",
       " 'lados': 787,\n",
       " 'hermosa': 788,\n",
       " 'has': 789,\n",
       " 'gran': 790,\n",
       " 'golfas': 791,\n",
       " 'fútbol': 792,\n",
       " 'fueron': 793,\n",
       " 'frío': 794,\n",
       " 'diario': 795,\n",
       " 'dejan': 796,\n",
       " 'darle': 797,\n",
       " 'dando': 798,\n",
       " 'costumbre': 799,\n",
       " 'cierto': 800,\n",
       " 'chingue': 801,\n",
       " 'cabeza': 802,\n",
       " 'buenos': 803,\n",
       " 'bola': 804,\n",
       " 'baño': 805,\n",
       " 'acabar': 806,\n",
       " 'Vale': 807,\n",
       " 'Tú': 808,\n",
       " 'SUS': 809,\n",
       " 'Mamá': 810,\n",
       " 'MI': 811,\n",
       " 'Jaja': 812,\n",
       " 'Esos': 813,\n",
       " 'Esas': 814,\n",
       " 'Desde': 815,\n",
       " 'Bueno': 816,\n",
       " '>': 817,\n",
       " '🤷🏻\\u200d♀': 818,\n",
       " '🤗': 819,\n",
       " '😑': 820,\n",
       " '😉': 821,\n",
       " '👏': 822,\n",
       " 'xq': 823,\n",
       " 'we': 824,\n",
       " 'vivo': 825,\n",
       " 'vayan': 826,\n",
       " 'tuve': 827,\n",
       " 'tristes': 828,\n",
       " 'tantos': 829,\n",
       " 'tantas': 830,\n",
       " 'suerte': 831,\n",
       " 'siquiera': 832,\n",
       " 'sienten': 833,\n",
       " 'saca': 834,\n",
       " 'ropa': 835,\n",
       " 'puras': 836,\n",
       " 'perder': 837,\n",
       " 'pensé': 838,\n",
       " 'paz': 839,\n",
       " 'pase': 840,\n",
       " 'nueva': 841,\n",
       " 'niños': 842,\n",
       " 'ningún': 843,\n",
       " 'mandan': 844,\n",
       " 'mamó': 845,\n",
       " 'mamada': 846,\n",
       " 'llorando': 847,\n",
       " 'jugar': 848,\n",
       " 'jamás': 849,\n",
       " 'hondureños': 850,\n",
       " 'hermana': 851,\n",
       " 'habla': 852,\n",
       " 'gustan': 853,\n",
       " 'gobierno': 854,\n",
       " 'frase': 855,\n",
       " 'examen': 856,\n",
       " 'empieza': 857,\n",
       " 'dijeron': 858,\n",
       " 'darme': 859,\n",
       " 'cual': 860,\n",
       " 'comiendo': 861,\n",
       " 'chile': 862,\n",
       " 'caracteres': 863,\n",
       " 'cagada': 864,\n",
       " 'bonitas': 865,\n",
       " 'Sabes': 866,\n",
       " 'Q': 867,\n",
       " 'Nunca': 868,\n",
       " 'Nada': 869,\n",
       " 'Más': 870,\n",
       " 'Messi': 871,\n",
       " 'Esto': 872,\n",
       " 'Ese': 873,\n",
       " 'DEL': 874,\n",
       " 'Chingas': 875,\n",
       " 'Chile': 876,\n",
       " 'Cada': 877,\n",
       " 'Amo': 878,\n",
       " '8': 879,\n",
       " '😖': 880,\n",
       " '😐': 881,\n",
       " '💖': 882,\n",
       " '👌': 883,\n",
       " '☺': 884,\n",
       " 'váyanse': 885,\n",
       " 'vuelven': 886,\n",
       " 'vivir': 887,\n",
       " 'venir': 888,\n",
       " 'vemos': 889,\n",
       " 'valga': 890,\n",
       " 'usan': 891,\n",
       " 'tweets': 892,\n",
       " 'trae': 893,\n",
       " 'todavía': 894,\n",
       " 'temprano': 895,\n",
       " 'sigues': 896,\n",
       " 'sería': 897,\n",
       " 'seria': 898,\n",
       " 'salgo': 899,\n",
       " 'querer': 900,\n",
       " 'prietas': 901,\n",
       " 'peda': 902,\n",
       " 'pasó': 903,\n",
       " 'normal': 904,\n",
       " 'naturaleza': 905,\n",
       " 'mía': 906,\n",
       " 'mucha': 907,\n",
       " 'mira': 908,\n",
       " 'mente': 909,\n",
       " 'mentada': 910,\n",
       " 'juntos': 911,\n",
       " 'ja': 912,\n",
       " 'imagen': 913,\n",
       " 'horrible': 914,\n",
       " 'haya': 915,\n",
       " 'hacerlo': 916,\n",
       " 'hablas': 917,\n",
       " 'hablan': 918,\n",
       " 'gringos': 919,\n",
       " 'gatos': 920,\n",
       " 'fuerte': 921,\n",
       " 'extraño': 922,\n",
       " 'etc': 923,\n",
       " 'esté': 924,\n",
       " 'estaría': 925,\n",
       " 'esperar': 926,\n",
       " 'empiezan': 927,\n",
       " 'edad': 928,\n",
       " 'duele': 929,\n",
       " 'diferente': 930,\n",
       " 'dieron': 931,\n",
       " 'derechos': 932,\n",
       " 'dentro': 933,\n",
       " 'cuerpo': 934,\n",
       " 'critican': 935,\n",
       " 'cree': 936,\n",
       " 'clima': 937,\n",
       " 'claro': 938,\n",
       " 'carro': 939,\n",
       " 'cagas': 940,\n",
       " 'ayuda': 941,\n",
       " 'am': 942,\n",
       " 'algún': 943,\n",
       " 'alguna': 944,\n",
       " 'alcohol': 945,\n",
       " 'acaba': 946,\n",
       " 'UNA': 947,\n",
       " 'Todas': 948,\n",
       " 'También': 949,\n",
       " 'Sólo': 950,\n",
       " 'Nos': 951,\n",
       " 'Mejor': 952,\n",
       " 'Loca': 953,\n",
       " 'Gente': 954,\n",
       " 'Facebook': 955,\n",
       " 'Están': 956,\n",
       " 'Cosas': 957,\n",
       " 'Calcuta': 958,\n",
       " 'Buenos': 959,\n",
       " 'Ando': 960,\n",
       " '280': 961,\n",
       " '🤦🏻\\u200d♂': 962,\n",
       " '🙂': 963,\n",
       " '😓': 964,\n",
       " '🐷': 965,\n",
       " '🎵': 966,\n",
       " '✊🏼': 967,\n",
       " '—': 968,\n",
       " 'única': 969,\n",
       " 'vergüenza': 970,\n",
       " 'vergazos': 971,\n",
       " 'tráfico': 972,\n",
       " 'tobogán': 973,\n",
       " 'teléfono': 974,\n",
       " 't': 975,\n",
       " 'super': 976,\n",
       " 'subir': 977,\n",
       " 'sigan': 978,\n",
       " 'señor': 979,\n",
       " 'sexo': 980,\n",
       " 'sentir': 981,\n",
       " 'sentido': 982,\n",
       " 'selección': 983,\n",
       " 'sabía': 984,\n",
       " 'regreso': 985,\n",
       " 'realidad': 986,\n",
       " 'rateros': 987,\n",
       " 'queriendo': 988,\n",
       " 'puso': 989,\n",
       " 'puse': 990,\n",
       " 'puesto': 991,\n",
       " 'presidente': 992,\n",
       " 'porno': 993,\n",
       " 'ponerme': 994,\n",
       " 'placer': 995,\n",
       " 'piensan': 996,\n",
       " 'piel': 997,\n",
       " 'pesar': 998,\n",
       " 'pensando': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generamos las bolsas de palabrascon pesado binario  de training y de validación \n",
    "binary_bow_tr=build_binary_bow(tr_txt,dict_freq,dict_indices)\n",
    "binary_boW_validacion=build_binary_bow(val_txt,dict_freq,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generamos las bolsas de palabras con pesado frecuencia de training y de validación \n",
    "frequency_bow_tr=build_freq_bow(tr_txt,dict_freq,dict_indices)\n",
    "frequency_boW_validacion=build_freq_bow(val_txt,dict_freq,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generamos las bolsas de palabras con pesado tfidf de training y de validación \n",
    "tfidf_bow_tr=build_tfidf_bow(tr_txt,dict_freq,dict_indices)\n",
    "tfidf_boW_validacion=build_tfidf_bow(val_txt,dict_freq,dict_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Diferencias en bolsa de palabras\n",
    "\n",
    "veamos la diferencia entre los pesos con cada esquema para un caso particular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el token @USUARIO corresponde al lugar 9 en el diccionario de frecuencias \n",
      "el primer tweet de los datos (es decir el documento 0) contiene tres veces este token\n",
      "El peso w_(0,9) es:\n",
      "con pesado binario: 1\n",
      "con pesado de frecnuencia: 3\n",
      "con pesado de tfidf: 5\n"
     ]
    }
   ],
   "source": [
    "#el token \"\"@USUARIO\"\" corresponde al lugar 9 en el diccionario de frecuencias \n",
    "x=dict_indices.get(\"@USUARIO\")\n",
    "print(\"el token \"\"@USUARIO\"\" corresponde al lugar\",x, \"en el diccionario de frecuencias \")\n",
    "print(\"el primer tweet de los datos (es decir el documento 0) contiene tres veces este token\")\n",
    "print(\"El peso w_(0,9) es:\")\n",
    "print(\"con pesado binario:\",binary_bow_tr[0][9])\n",
    "print(\"con pesado de frecnuencia:\", frequency_bow_tr[0][9])\n",
    "print(\"con pesado de tfidf:\", tfidf_bow_tr[0][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Clasificador}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras binaria y las etiqutas\n",
    "grid.fit(binary_bow_tr,tr_labels)\n",
    "#predicción\n",
    "y_pred_binary=grid.predict(binary_boW_validacion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras de frecuencia y las etiqutas\n",
    "grid.fit(frequency_bow_tr,tr_labels)\n",
    "#predicción\n",
    "y_pred_frequency=grid.predict(frequency_boW_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras TFIDF y las etiqutas\n",
    "grid.fit(tfidf_bow_tr,tr_labels)\n",
    "#predicción\n",
    "y_pred_tfidf=grid.predict(tfidf_boW_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Binary BoW_____________\n",
      "[[356  62]\n",
      " [ 49 120]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.87       418\n",
      "           1       0.66      0.71      0.68       169\n",
      "\n",
      "    accuracy                           0.81       587\n",
      "   macro avg       0.77      0.78      0.77       587\n",
      "weighted avg       0.82      0.81      0.81       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificación,matriz de confusión\n",
    "print(\"_____________Binary BoW_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_binary))\n",
    "print(metrics.classification_report(val_labels,y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Frequency BoW_____________\n",
      "[[362  56]\n",
      " [ 45 124]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88       418\n",
      "           1       0.69      0.73      0.71       169\n",
      "\n",
      "    accuracy                           0.83       587\n",
      "   macro avg       0.79      0.80      0.79       587\n",
      "weighted avg       0.83      0.83      0.83       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificación,matriz de confusión\n",
    "print(\"_____________Frequency BoW_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_frequency))\n",
    "print(metrics.classification_report(val_labels,y_pred_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________TfiDf BoW_____________\n",
      "[[356  62]\n",
      " [ 71  98]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       418\n",
      "           1       0.61      0.58      0.60       169\n",
      "\n",
      "    accuracy                           0.77       587\n",
      "   macro avg       0.72      0.72      0.72       587\n",
      "weighted avg       0.77      0.77      0.77       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificación,matriz de confusión\n",
    "print(\"_____________TfiDf BoW_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_tfidf))\n",
    "print(metrics.classification_report(val_labels,y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "4,5,6. Evalue BoW con pesado binario, de frecuencia y tfidf con normalizado l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Bolsas de palabras con distintos equemas de pesado con normalizado L2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizacion_dos(X):\n",
    "    norm = np.linalg.norm(X, ord=2, axis=1)  # Calcula la norma L2 de cada fila\n",
    "    norm = np.where(norm == 0, 1, norm) \n",
    "    norm = norm.reshape(-1, 1)  # Reshape para que la división sea compatible\n",
    "    return X / norm  # Divide cada elemento de X por la norma correspondiente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizamos las bolsas de palabrascon pesado binario  de training y de validación \n",
    "binary_bow_tr_norm=normalizacion_dos(binary_bow_tr)\n",
    "binary_boW_validacion_norm=normalizacion_dos(binary_boW_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizamos las bolsas de palabrascon pesado frecuencia  de training y de validación \n",
    "frequency_bow_tr_norm=normalizacion_dos(frequency_bow_tr)\n",
    "frequency_boW_validacion_norm=normalizacion_dos(frequency_boW_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizamos las bolsas de palabrascon pesado tfidf  de training y de validación \n",
    "tfidf_bow_tr_norm=normalizacion_dos(tfidf_bow_tr)\n",
    "tfidf_boW_validacion_norm=normalizacion_dos(tfidf_boW_validacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Diferencias en bolsa de palabras\n",
    "\n",
    "veamos la diferencia entre los pesos con cada esquema para un caso particular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el token @USUARIO corresponde al lugar 9 en el diccionario de frecuencias \n",
      "el primer tweet de los datos (es decir el documento 0) contiene tres veces este token\n",
      "El peso w_(0,9) es:\n",
      "con pesado binario: 0.20412414523193154\n",
      "con pesado de frecnuencia: 0.48666426339228763\n",
      "con pesado de tfidf: 0.21128856368212914\n"
     ]
    }
   ],
   "source": [
    "#el token \"\"@USUARIO\"\" corresponde al lugar 9 en el diccionario de frecuencias \n",
    "x=dict_indices.get(\"@USUARIO\")\n",
    "print(\"el token \"\"@USUARIO\"\" corresponde al lugar\",x, \"en el diccionario de frecuencias \")\n",
    "print(\"el primer tweet de los datos (es decir el documento 0) contiene tres veces este token\")\n",
    "print(\"El peso w_(0,9) es:\")\n",
    "print(\"con pesado binario:\",binary_bow_tr_norm[0][9])\n",
    "print(\"con pesado de frecnuencia:\", frequency_bow_tr_norm[0][9])\n",
    "print(\"con pesado de tfidf:\", tfidf_bow_tr_norm[0][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Clasificador}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras binaria y las etiqutas\n",
    "grid.fit(binary_bow_tr_norm,tr_labels)\n",
    "#predicción\n",
    "y_pred_binary_norm=grid.predict(binary_boW_validacion_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras de frecuencia y las etiqutas\n",
    "grid.fit(frequency_bow_tr_norm,tr_labels)\n",
    "#predicción\n",
    "y_pred_frequency_norm=grid.predict(frequency_boW_validacion_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras TFIDF y las etiqutas\n",
    "grid.fit(tfidf_bow_tr_norm,tr_labels)\n",
    "#predicción\n",
    "y_pred_tfidf_norm=grid.predict(tfidf_boW_validacion_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Binary BoW L2 normalization_____________\n",
      "[[356  62]\n",
      " [ 47 122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.87       418\n",
      "           1       0.66      0.72      0.69       169\n",
      "\n",
      "    accuracy                           0.81       587\n",
      "   macro avg       0.77      0.79      0.78       587\n",
      "weighted avg       0.82      0.81      0.82       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificación,matriz de confusión\n",
    "print(\"_____________Binary BoW L2 normalization_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_binary_norm))\n",
    "print(metrics.classification_report(val_labels,y_pred_binary_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Frequency BoW L2 normalization_____________\n",
      "[[358  60]\n",
      " [ 38 131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88       418\n",
      "           1       0.69      0.78      0.73       169\n",
      "\n",
      "    accuracy                           0.83       587\n",
      "   macro avg       0.79      0.82      0.80       587\n",
      "weighted avg       0.84      0.83      0.84       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificación,matriz de confusión\n",
    "print(\"_____________Frequency BoW L2 normalization_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_frequency_norm))\n",
    "print(metrics.classification_report(val_labels,y_pred_frequency_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________TfiDf BoW L2 normalization_____________\n",
      "[[356  62]\n",
      " [ 40 129]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.87       418\n",
      "           1       0.68      0.76      0.72       169\n",
      "\n",
      "    accuracy                           0.83       587\n",
      "   macro avg       0.79      0.81      0.80       587\n",
      "weighted avg       0.83      0.83      0.83       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificación,matriz de confusión\n",
    "print(\"_____________TfiDf BoW L2 normalization_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_tfidf_norm))\n",
    "print(metrics.classification_report(val_labels,y_pred_tfidf_norm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "7. Ponga una tabla comparativa a modo de resumen con las seis entradas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesado binario \n",
      " Accuracy 0.82 \n",
      " Precision (0) 0.88 \n",
      " Precision (1) 0.66\n",
      "Pesado binario normlizado \n",
      " Accuracy 0.82 \n",
      " Precision (0) 0.88 \n",
      " Precision (1) 0.66\n",
      "Pesado frecuencia \n",
      " Accuracy 0.83 \n",
      " Precision (0) 0.89 \n",
      " Precision (1) 0.69\n",
      "Pesado frecuencia normalizado \n",
      " Accuracy 0.84 \n",
      " Precision (0) 0.9 \n",
      " Precision (1) 0.69\n",
      "Pesado tfidf \n",
      " Accuracy 0.77 \n",
      " Precision (0) 0.83 \n",
      " Precision (1) 0.61\n",
      "Pesado tfidf normalizado \n",
      " Accuracy 0.83 \n",
      " Precision (0) 0.9 \n",
      " Precision (1) 0.68\n"
     ]
    }
   ],
   "source": [
    "lista_predicciones=[y_pred_binary,y_pred_binary_norm,y_pred_frequency,y_pred_frequency_norm,y_pred_tfidf,y_pred_tfidf_norm]\n",
    "lista_pesados=[\"Pesado binario\",\"Pesado binario normlizado\",\"Pesado frecuencia\",\"Pesado frecuencia normalizado\",\"Pesado tfidf\",\"Pesado tfidf normalizado\"]\n",
    "for predict, pesado in zip(lista_predicciones, lista_pesados):\n",
    "    m = metrics.classification_report(val_labels, predict)\n",
    "    accuracy = float(m.split('\\n')[-2].split()[2])\n",
    "    precision_0 = float(m.split('\\n')[2].split()[1])  \n",
    "    precision_1 = float(m.split('\\n')[3].split()[1])  \n",
    "    \n",
    "    print(pesado, \"\\n Accuracy\", accuracy, \"\\n Precision (0)\",precision_0,\"\\n Precision (1)\",precision_1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El accuracy más alto se obtene para el pesado *frecuencia normalizado*, con 0.84. Como tenemos un conjunto desbalanceado buscaremos maximizar el recall para la clase minoritaria (etiqueta 1) para capturar la mayor cantidad posible de casos positivos verdaderos, así que comparremos con la el siguiente pesado con mayor accuracy que es *Pesado tfidf normalizado*. Notemos a precisión para la etiqueta 0 es la misma: 0.9, miestras que para la etiqueta uno varia un poco (0.1), siendo mayor para Frecuecia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesado frecuencia normalizado \n",
      " Accuracy 0.84 \n",
      " Recall (0) 0.86 \n",
      " Recall (1) 0.78\n",
      "Pesado tfidf normalizado \n",
      " Accuracy 0.83 \n",
      " Recall (0) 0.85 \n",
      " Recall (1) 0.76\n"
     ]
    }
   ],
   "source": [
    "lista_predicciones=[y_pred_frequency_norm,y_pred_tfidf_norm]\n",
    "lista_pesados=[\"Pesado frecuencia normalizado\",\"Pesado tfidf normalizado\"]\n",
    "for predict, pesado in zip(lista_predicciones, lista_pesados):\n",
    "    m = metrics.classification_report(val_labels, predict)\n",
    "    accuracy = float(m.split('\\n')[-2].split()[2])\n",
    "    precision_0 = float(m.split('\\n')[2].split()[2])  \n",
    "    precision_1 = float(m.split('\\n')[3].split()[2])  \n",
    "    \n",
    "    print(pesado, \"\\n Accuracy\", accuracy, \"\\n Recall (0)\",precision_0,\"\\n Recall (1)\",precision_1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que el mejor resultado se obtuvo con pesado defrecuencia y normalización L2, mayor accuracy, mayor precision e incluso mayor recall para la etiqueta minoritaria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "8. De las configuraciones anteriores elija la mejor y evalúela con más y menos términos (e.g., 1000 y 7000). Ponga una tabla dónde compare las tres configuraciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos diccionario de indices y de frecuencias haciendo corte en las mil primeras palabras\n",
    "dict_freq_1000=create_dic_freq(corpus_palabras,1000)\n",
    "dict_indices_1000=create_dic_ranking(dict_freq_1000)\n",
    "\n",
    "#Bolsas de palabras con pesado frecuencia de training y de validación usando los diccionarios anteriores\n",
    "bow_tr_1000=build_freq_bow(tr_txt,dict_freq_1000,dict_indices_1000)\n",
    "boW_validacion_1000=build_freq_bow(val_txt,dict_freq_1000,dict_indices_1000)\n",
    "\n",
    "#normalizamos \n",
    "bow_tr_norm_1000=normalizacion_dos(bow_tr_1000)\n",
    "boW_validacion_norm_1000=normalizacion_dos(boW_validacion_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras \n",
    "grid.fit(bow_tr_norm_1000,tr_labels)\n",
    "#predicción\n",
    "y_pred_1000=grid.predict(boW_validacion_norm_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Frequency BoW (1000) L2 normalization_____________\n",
      "[[346  72]\n",
      " [ 39 130]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86       418\n",
      "           1       0.64      0.77      0.70       169\n",
      "\n",
      "    accuracy                           0.81       587\n",
      "   macro avg       0.77      0.80      0.78       587\n",
      "weighted avg       0.83      0.81      0.82       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificación,matriz de confusión\n",
    "print(\"_____________Frequency BoW (1000) L2 normalization_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_1000))\n",
    "print(metrics.classification_report(val_labels,y_pred_1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos diccionario de indices y de frecuencias haciendo corte en las mil primeras palabras\n",
    "dict_freq_7000=create_dic_freq(corpus_palabras,7000)\n",
    "dict_indices_7000=create_dic_ranking(dict_freq_7000)\n",
    "\n",
    "#Bolsas de palabras con pesado frecuencia de training y de validación usando los diccionarios anteriores\n",
    "bow_tr_7000=build_freq_bow(tr_txt,dict_freq_7000,dict_indices_7000)\n",
    "boW_validacion_7000=build_freq_bow(val_txt,dict_freq_7000,dict_indices_7000)\n",
    "\n",
    "#normalizamos \n",
    "bow_tr_norm_7000=normalizacion_dos(bow_tr_7000)\n",
    "boW_validacion_norm_7000=normalizacion_dos(boW_validacion_7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras \n",
    "grid.fit(bow_tr_norm_7000,tr_labels)\n",
    "#predicción\n",
    "y_pred_7000=grid.predict(boW_validacion_norm_7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Frequency BoW (7000) L2 normalization_____________\n",
      "[[359  59]\n",
      " [ 39 130]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88       418\n",
      "           1       0.69      0.77      0.73       169\n",
      "\n",
      "    accuracy                           0.83       587\n",
      "   macro avg       0.79      0.81      0.80       587\n",
      "weighted avg       0.84      0.83      0.84       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificación,matriz de confusión\n",
    "print(\"_____________Frequency BoW (7000) L2 normalization_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_7000))\n",
    "print(metrics.classification_report(val_labels,y_pred_7000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comparación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bolsa depalabras con pesado de frecuencia y normalización L2\n",
      "\n",
      "palabras tomadas para BoW: 1000 \n",
      " Accuracy 0.83 \n",
      " Precision (0) 0.9 \n",
      " Precision (1) 0.64\n",
      "palabras tomadas para BoW: 5000 \n",
      " Accuracy 0.84 \n",
      " Precision (0) 0.9 \n",
      " Precision (1) 0.69\n",
      "palabras tomadas para BoW: 7000 \n",
      " Accuracy 0.84 \n",
      " Precision (0) 0.9 \n",
      " Precision (1) 0.69\n"
     ]
    }
   ],
   "source": [
    "lista_predicciones=[y_pred_1000,y_pred_frequency_norm,y_pred_7000]\n",
    "lista_pesados=[\"1000\",\"5000\",\"7000\"]\n",
    "print(\"Bolsa depalabras con pesado de frecuencia y normalización L2\\n\")\n",
    "for predict, pesado in zip(lista_predicciones, lista_pesados):\n",
    "    m = metrics.classification_report(val_labels, predict)\n",
    "    accuracy = float(m.split('\\n')[-2].split()[2])\n",
    "    precision_0 = float(m.split('\\n')[2].split()[1])  \n",
    "    precision_1 = float(m.split('\\n')[3].split()[1])  \n",
    "    \n",
    "    print(\"palabras tomadas para BoW:\",pesado, \"\\n Accuracy\", accuracy, \"\\n Precision (0)\",precision_0,\"\\n Precision (1)\",precision_1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "9. Utilice el recurso léxico del Consejo Nacional de Investigación de Canadá llamado \"EmoLex\" (https://www.saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) para construir una \"Bolsa de Emociones\" de los Tweets de agresividad (Debe usar EmoLex en Español). Para esto, una estrategia sencilla sería enmascarar cada palabra con su emoción, y después construir la Bolsa de Emociones (BoE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "10. Evalúa tú BoE clasificando con SVM. Ponga una tabla comparativa a modo de resumen con los tres pesados, normalize cada uno si lo cree conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4.5 color='lightblue'>\n",
    "\n",
    "$\\textit{Ejercicio 3. Recurso Línguistico de Emociones Mexicano}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "11. Utiliceelrecursoléxicollamado\"SpanishEmotionLexicon(SEL)\"delDr.GrigoriSidorov, profesor del Centro de Investigación en Computación (CIC) del Instituto Politécnico Nacional (http://www.cic.ipn.mx/∼sidorov/), para enmascarar cada palabra con su emo- ción, y después construir la Bolsa de Emociones con algún pesado (e.g., binario, tf, tfidf). Proponga alguna estrategia para incorporar el \"valor\" del \"Probability Factor of Affective use\" en su representación vectorial del documento. Evalúa y escribe una tabla compara- tiva a modo de resumen con al menos tres pesados: binario, frecuencia, tfidf. Normalize cada pesado según lo crea conveniente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "12. En un comentario aparte, discuta sobre la estrategía que utilizó para incorporar el \"Probability Factor of Affective use\". No más de 5 renglones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4.5 color='lightblue'>\n",
    "\n",
    "$\\textit{Ejercicio 4. ¿Podemos mejorar con Bigramas?}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "13. Hacer un experimento dónde concatene una buena BoW según sus experimentos anteri- ores con otra BoW construida a partir de los 1000 bigramas más frecuentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "14. Hacer un experimento con las Bolsas de Emociones, Bolsa de Palabras y Bolsa de Bi- gramas; usted elige las dimensionalidades. Para construir la representación final del documento utilice la concatenación de las representaciones según sus observaciones (e.g., Bolsa de Palabras + Bolsa de Bigramas + Bolsa de Sentimientos de Canadá + Bolsa de Sentimientos de Grigori), y aliméntelas a un SVM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "15. Elabore conclusiones sobre toda esta Tarea, incluyendo observaciones, comentarios y posibles mejoras futuras. Discuta el comportamiento de la BoW de usar solo palabras a integrar bigramas, y luego a integrar todo ¿ayudó? o ¿empeoró?. Discuta también brevemente el costo computacional de los experimentos ¿Valió la Pena tener todo?. Sea breve: todo en NO más de dos párrafos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
