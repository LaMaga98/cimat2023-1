{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{PLN. Tarea 2: Miner√≠a de texto b√°sica}$$\n",
    "$$\\textit{Y. Sarahi Garc√≠a Goz√°lez}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#clasificacion\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,precision_recall_fscore_support,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarea realizada en MacOs. \n",
      "Las versiones de las librer√≠as y de python utilizadas fueron:\n",
      "\n",
      "Python version 3.11.5\n",
      "Numpy version 1.23.5\n",
      "NLTK version 3.8.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Tarea realizada en MacOs. \\nLas versiones de las librer√≠as y de python utilizadas fueron:\\n\")\n",
    "from platform import python_version\n",
    "print(\"Python version\", python_version())\n",
    "print(\"Numpy version\", np.__version__)\n",
    "print(\"NLTK version\", nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4.5 color='lightblue'>\n",
    "\n",
    "$\\textit{Ejercicio 2. Bolsas de Palabras, Bigramas y Emociones}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Preparaci√≥n de texto, corpus y diccionarios}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_text_from_file(path_corpus,path_truth):\n",
    "\n",
    "    tr_txt=[]\n",
    "    tr_labels=[]\n",
    "\n",
    "    with open(path_corpus, \"r\") as f_corpus,open(path_truth, \"r\") as f_truth:\n",
    "        for tweet in f_corpus:\n",
    "            tr_txt += [tweet]\n",
    "        for label in f_truth:\n",
    "            tr_labels += [label]   \n",
    "             \n",
    "    return tr_txt, tr_labels\n",
    "\n",
    "\n",
    "def create_corpus_from_text(text,tokenizer):\n",
    "    corpus_palabras=[]\n",
    "    for documento in text:\n",
    "        corpus_palabras+=tokenizer.tokenize(documento)\n",
    "\n",
    "    return corpus_palabras\n",
    "\n",
    "\n",
    "def create_dic_freq(corpus,n):\n",
    "    fdist = nltk.FreqDist(corpus)\n",
    "    aux=[(fdist[key],key) for key in fdist]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    aux=aux[:n]\n",
    "\n",
    "    return aux\n",
    "\n",
    "def create_dic_ranking(dic_freq):\n",
    "    dict_indices=dict()\n",
    "    cont = 0\n",
    "    for weight, word in dic_freq:\n",
    "        dict_indices[word]= cont\n",
    "        cont+= 1\n",
    "\n",
    "    return dict_indices\n",
    "\n",
    "#Clasificador\n",
    "\n",
    "parameters={'C':[0.05,0.12,0.25,0.5,1,2,4]} #par√°metro de complejidad\n",
    "\n",
    "#llamamos el clasificador SVM\n",
    "#nota: cambi√© el max_iter y dual porque me aparec√≠an una serie de warnings pero no cambiaron los resultados\n",
    "svr=svm.LinearSVC(class_weight='balanced',dual='auto',max_iter=3000) \n",
    "\n",
    "#grid search recide el objeto de clasificacion\n",
    "grid=GridSearchCV(estimator=svr,param_grid=parameters,n_jobs=8,scoring=\"f1_macro\",cv=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardamos los textos de entrenamiento y validaci√≥n\n",
    "tr_txt,tr_labels=get_text_from_file(\"/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/practicas/03_practica/mex20_train.txt\",\"/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/practicas/03_practica/mex20_train_labels.txt\")\n",
    "val_txt,val_labels=get_text_from_file(\"/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/practicas/03_practica/mex20_val.txt\",\"/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/practicas/03_practica/mex20_val_labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connvertimos a lista las etiquetas\n",
    "tr_labels=list(map(int,tr_labels))\n",
    "val_labels=list(map(int,val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizamos\n",
    "tokenizer=TweetTokenizer()\n",
    "#Generamos el corpus\n",
    "corpus_palabras=create_corpus_from_text(tr_txt,tokenizer)\n",
    "#Generamos diccionario de frecuencias con las primeras 5 mil palabras\n",
    "dict_freq=create_dic_freq(corpus_palabras,5000)\n",
    "#Generamos diccionario de indices\n",
    "dict_indices=create_dic_ranking(dict_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me - 10\n",
      "el - 11\n",
      "en - 12\n",
      "se - 13\n",
      "es - 14\n",
      "con - 15\n",
      "? - 16\n",
      "verga - 17\n",
      "los - 18\n",
      "madre - 19\n",
      "por - 20\n",
      "las - 21\n",
      "\" - 22\n",
      "un - 23\n",
      "te - 24\n",
      "mi - 25\n",
      "lo - 26\n",
      "putas - 27\n",
      "una - 28\n",
      "... - 29\n"
     ]
    }
   ],
   "source": [
    "#imprimimos algunos elementos del diccionario de indices\n",
    "lista_indices = list(dict_indices.items())\n",
    "for clave, valor in lista_indices[10:30]:\n",
    "    print(clave, \"-\", valor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3.5 color='lightblue'>\n",
    "\n",
    "1,2,3. Evalue BoW con pesado binario, de frecuencia y tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Bolsas de palabras con distintos equemas de pesado y sin normalizar}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#________________________________PESADO_BINARIO________________________________#\n",
    "\n",
    "\n",
    "#matriz_nxm de bolsa de palabras n=numero de textos, m=numero de palabras (5000)\n",
    "def build_binary_bow(tr_txt,dic_freq,dic_indices):\n",
    "    '''\n",
    "    Funci√≥n que genera la bolsa de palabras con pesado binario\n",
    "\n",
    "    '''\n",
    "    BoW=np.zeros((len(tr_txt),len(dic_freq)),dtype=int) #construimos bolsta de palabras en ceros\n",
    "    cont_documento=0 #indice que recorre las FILAS\n",
    "\n",
    "    for tr in tr_txt:#recorremos cada documento\n",
    "        fdist_doc=nltk.FreqDist(tokenizer.tokenize(tr))#hacemos freqDist (tokenizado con el tokenizador ya definido) de cada documento\n",
    "        for word in fdist_doc: #para cada palabra en el diccionario del documento\n",
    "            if word in dic_indices: #si la palabra est√° en el corte final\n",
    "                #AGREGAMOS un uno en el lugar correspondiente\n",
    "                BoW[cont_documento,dic_indices[word]] = 1 \n",
    "\n",
    "        cont_documento+=1\n",
    "\n",
    "    return BoW\n",
    "\n",
    "#________________________________PESADO_FRECUENCIA________________________________#\n",
    "\n",
    "#matriz_nxm de bolsa de palabras n=numero de textos, m=numero de palabras (5000)\n",
    "def build_freq_bow(tr_txt,dic_feq,dic_inices):\n",
    "    BoW=np.zeros((len(tr_txt),len(dic_feq)),dtype=int) #construimos bolsta de palabras en ceros\n",
    "    cont_documento=0 #indice que recorre las FILAS\n",
    " \n",
    "    for tr in tr_txt:#recorremos cada documento\n",
    "        fdist_doc=nltk.FreqDist(tokenizer.tokenize(tr))#hacemos freqDist (tokenizado con el tokenizador ya definido) de cada documento\n",
    "        for word in fdist_doc: #para cada palabra en el diccionario del documento\n",
    "            if word in dic_inices: #si la palabra est√° en el corte final\n",
    "                #le asignamos su frecuencia\n",
    "                BoW[cont_documento,dic_inices[word]] = fdist_doc[word]\n",
    "\n",
    "        cont_documento+=1\n",
    "\n",
    "    return BoW\n",
    "\n",
    "#________________________________PESADO_TFIDF________________________________#\n",
    "\n",
    "#primero calculamos el num de documentos en que aparece cada palabra\n",
    "def df(tr_txt, dic_indices):\n",
    "    df = np.zeros(5000, dtype=int)\n",
    "\n",
    "    # comp list para las frecuencias de cada documento\n",
    "    fdist_docs = [nltk.FreqDist(tokenizer.tokenize(tr)) for tr in tr_txt]\n",
    "\n",
    "    #iteramos sobre cada palbra\n",
    "    for word in dic_indices:\n",
    "        #obtenemos el indice de la palabra\n",
    "        index = dic_indices[word] \n",
    "        for j,fdist_doc in enumerate(fdist_docs): #iteramos osbre cada documento\n",
    "            if word in fdist_doc: #si la palabra est√° en el documento actual\n",
    "                df[index] += 1 #agredamos uno al conador de esa palabra\n",
    "\n",
    "    return df\n",
    "\n",
    "#\n",
    "def build_tfidf_bow(tr_txt, dic_freq, dic_indices):\n",
    "    BoW = build_freq_bow(tr_txt, dic_freq, dic_indices) #la bolsa de palabras de frecuencias es el tf\n",
    "    df_list = df(tr_txt, dic_indices) #llamamos a la duncion df\n",
    "    N = len(tr_txt)\n",
    "   \n",
    "    for i in range(N):\n",
    "        for j,df_val in enumerate(df_list): \n",
    "            if df_val!=0: #si df es distinto de ceo (puede ser cero para el conjunto de validacion, por ejemplo que contiene menos lementos )\n",
    "                BoW[i][j] *= np.log(N / df_val) #tfxdf\n",
    "\n",
    "    return BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 0,\n",
       " 'de': 1,\n",
       " 'que': 2,\n",
       " '.': 3,\n",
       " 'la': 4,\n",
       " 'a': 5,\n",
       " 'y': 6,\n",
       " '!': 7,\n",
       " 'no': 8,\n",
       " '@USUARIO': 9,\n",
       " 'me': 10,\n",
       " 'el': 11,\n",
       " 'en': 12,\n",
       " 'se': 13,\n",
       " 'es': 14,\n",
       " 'con': 15,\n",
       " '?': 16,\n",
       " 'verga': 17,\n",
       " 'los': 18,\n",
       " 'madre': 19,\n",
       " 'por': 20,\n",
       " 'las': 21,\n",
       " '\"': 22,\n",
       " 'un': 23,\n",
       " 'te': 24,\n",
       " 'mi': 25,\n",
       " 'lo': 26,\n",
       " 'putas': 27,\n",
       " 'una': 28,\n",
       " '...': 29,\n",
       " 'putos': 30,\n",
       " 'para': 31,\n",
       " 'üòÇ': 32,\n",
       " 'si': 33,\n",
       " 'ya': 34,\n",
       " 'como': 35,\n",
       " 'su': 36,\n",
       " 'pero': 37,\n",
       " 'tu': 38,\n",
       " 'loca': 39,\n",
       " 'le': 40,\n",
       " 'm√°s': 41,\n",
       " 'No': 42,\n",
       " 'del': 43,\n",
       " 'gorda': 44,\n",
       " 'al': 45,\n",
       " 'bien': 46,\n",
       " 'A': 47,\n",
       " '¬ø': 48,\n",
       " 'Y': 49,\n",
       " 'son': 50,\n",
       " 'Me': 51,\n",
       " 'o': 52,\n",
       " 'feas': 53,\n",
       " 'cuando': 54,\n",
       " 'Que': 55,\n",
       " ':': 56,\n",
       " 'yo': 57,\n",
       " 'les': 58,\n",
       " 'porque': 59,\n",
       " 'ni': 60,\n",
       " 'est√°': 61,\n",
       " 'ser': 62,\n",
       " 'estoy': 63,\n",
       " 'sus': 64,\n",
       " 'todos': 65,\n",
       " 'esta': 66,\n",
       " 'puta': 67,\n",
       " 'Ya': 68,\n",
       " 'todo': 69,\n",
       " 'pinche': 70,\n",
       " 'puto': 71,\n",
       " 'tan': 72,\n",
       " 'Si': 73,\n",
       " 'La': 74,\n",
       " 'qu√©': 75,\n",
       " '‚Ä¶': 76,\n",
       " 'eso': 77,\n",
       " 'muy': 78,\n",
       " 'soy': 79,\n",
       " 'hasta': 80,\n",
       " 'as√≠': 81,\n",
       " '¬°': 82,\n",
       " '<URL>': 83,\n",
       " 'mamar': 84,\n",
       " 'hay': 85,\n",
       " 'q': 86,\n",
       " 'DE': 87,\n",
       " 'mis': 88,\n",
       " 'joto': 89,\n",
       " 'hace': 90,\n",
       " 'este': 91,\n",
       " 'cosas': 92,\n",
       " 'Ô∏è': 93,\n",
       " 'vida': 94,\n",
       " 'nos': 95,\n",
       " 'ver': 96,\n",
       " 'mejor': 97,\n",
       " 'solo': 98,\n",
       " 'nada': 99,\n",
       " 'vale': 100,\n",
       " 'va': 101,\n",
       " 'quiero': 102,\n",
       " 'marica': 103,\n",
       " 'eres': 104,\n",
       " 'd√≠a': 105,\n",
       " 'siempre': 106,\n",
       " 'esa': 107,\n",
       " 'voy': 108,\n",
       " 'gente': 109,\n",
       " 'Yo': 110,\n",
       " 'üò≠': 111,\n",
       " 'vez': 112,\n",
       " 'El': 113,\n",
       " 'mierda': 114,\n",
       " '-': 115,\n",
       " 'tengo': 116,\n",
       " '(': 117,\n",
       " 'sin': 118,\n",
       " 'ese': 119,\n",
       " ')': 120,\n",
       " 'Es': 121,\n",
       " 'luchona': 122,\n",
       " 'üòç': 123,\n",
       " 'hdp': 124,\n",
       " 'ahora': 125,\n",
       " 'Por': 126,\n",
       " 'üò°': 127,\n",
       " '‚Äú': 128,\n",
       " 'tienen': 129,\n",
       " 'tiene': 130,\n",
       " 'pinches': 131,\n",
       " 'hacer': 132,\n",
       " 'tus': 133,\n",
       " 'tontas': 134,\n",
       " 'LA': 135,\n",
       " '‚Äù': 136,\n",
       " 'gusta': 137,\n",
       " 'Como': 138,\n",
       " 'sea': 139,\n",
       " 'HDP': 140,\n",
       " 'toda': 141,\n",
       " 'Se': 142,\n",
       " 'hoy': 143,\n",
       " 'Qu√©': 144,\n",
       " 'mamando': 145,\n",
       " 'est√°n': 146,\n",
       " 'cagado': 147,\n",
       " 'tonta': 148,\n",
       " 'Pero': 149,\n",
       " 'puedo': 150,\n",
       " 'mas': 151,\n",
       " 'üôÑ': 152,\n",
       " 'pendejo': 153,\n",
       " 'hijo': 154,\n",
       " 'NO': 155,\n",
       " 'En': 156,\n",
       " 'Mi': 157,\n",
       " 'mal': 158,\n",
       " 'estar': 159,\n",
       " 'QUE': 160,\n",
       " '..': 161,\n",
       " 'Lo': 162,\n",
       " 'algo': 163,\n",
       " 'PUTOS': 164,\n",
       " 'tener': 165,\n",
       " 'alguien': 166,\n",
       " 'Putos': 167,\n",
       " 'verdad': 168,\n",
       " 'mujer': 169,\n",
       " 'cabrona': 170,\n",
       " 'tambi√©n': 171,\n",
       " 'da': 172,\n",
       " 'puede': 173,\n",
       " 'decir': 174,\n",
       " 'madres': 175,\n",
       " 'mujeres': 176,\n",
       " 'maricon': 177,\n",
       " 'vas': 178,\n",
       " 'mucho': 179,\n",
       " 'dos': 180,\n",
       " 'MADRE': 181,\n",
       " '‚ù§': 182,\n",
       " 'van': 183,\n",
       " 's√©': 184,\n",
       " 'Estoy': 185,\n",
       " 'Cuando': 186,\n",
       " ';': 187,\n",
       " 's√≠': 188,\n",
       " 'otra': 189,\n",
       " 'est√°s': 190,\n",
       " 'a√±os': 191,\n",
       " 'Verga': 192,\n",
       " 'PUTAS': 193,\n",
       " 'Las': 194,\n",
       " 'ir': 195,\n",
       " 'chingada': 196,\n",
       " \"'\": 197,\n",
       " 'veces': 198,\n",
       " 't√∫': 199,\n",
       " 'hijos': 200,\n",
       " 'De': 201,\n",
       " 'quiere': 202,\n",
       " 'quien': 203,\n",
       " 'pues': 204,\n",
       " 'jajaja': 205,\n",
       " 'VERGA': 206,\n",
       " 'Te': 207,\n",
       " '3': 208,\n",
       " 'mundo': 209,\n",
       " 'menos': 210,\n",
       " '2': 211,\n",
       " 'ü§î': 212,\n",
       " 'uno': 213,\n",
       " 'nunca': 214,\n",
       " 'era': 215,\n",
       " 'cada': 216,\n",
       " 'd√≠as': 217,\n",
       " 'dice': 218,\n",
       " 'M√©xico': 219,\n",
       " 'todas': 220,\n",
       " 'esos': 221,\n",
       " 'amor': 222,\n",
       " 'Una': 223,\n",
       " 'tanto': 224,\n",
       " 'mam√°': 225,\n",
       " 'ganas': 226,\n",
       " 'esas': 227,\n",
       " 'O': 228,\n",
       " 'üòí': 229,\n",
       " 'tienes': 230,\n",
       " 'tiempo': 231,\n",
       " 'fue': 232,\n",
       " 'cuenta': 233,\n",
       " 'ME': 234,\n",
       " 'Jajajaja': 235,\n",
       " 'pendeja': 236,\n",
       " 'estaba': 237,\n",
       " 'dicen': 238,\n",
       " 'Madre': 239,\n",
       " 'wey': 240,\n",
       " 'pedo': 241,\n",
       " 'neta': 242,\n",
       " 'esto': 243,\n",
       " 'ti': 244,\n",
       " 'siento': 245,\n",
       " 'ma√±ana': 246,\n",
       " 'igual': 247,\n",
       " 'he': 248,\n",
       " 'd': 249,\n",
       " 'camote': 250,\n",
       " 'Los': 251,\n",
       " 'veo': 252,\n",
       " 'personas': 253,\n",
       " 'pasa': 254,\n",
       " 'hacen': 255,\n",
       " 'ha': 256,\n",
       " 'donde': 257,\n",
       " 'digo': 258,\n",
       " 'Hoy': 259,\n",
       " '/': 260,\n",
       " 'v': 261,\n",
       " 'unas': 262,\n",
       " 'bueno': 263,\n",
       " ':(': 264,\n",
       " 'fotos': 265,\n",
       " 'cabr√≥n': 266,\n",
       " 'Ahora': 267,\n",
       " 'otro': 268,\n",
       " 'mismo': 269,\n",
       " 'cabron': 270,\n",
       " 'buena': 271,\n",
       " 'ah√≠': 272,\n",
       " 'trabajo': 273,\n",
       " 'sabe': 274,\n",
       " 'nadie': 275,\n",
       " 'estas': 276,\n",
       " 'desde': 277,\n",
       " 'amigos': 278,\n",
       " 'alv': 279,\n",
       " 'Pinche': 280,\n",
       " 'm√≠': 281,\n",
       " 'hombres': 282,\n",
       " 'foto': 283,\n",
       " 'culo': 284,\n",
       " 'casa': 285,\n",
       " 'ardida': 286,\n",
       " 'Jajaja': 287,\n",
       " '*': 288,\n",
       " 'üòà': 289,\n",
       " 've': 290,\n",
       " 'valer': 291,\n",
       " 'han': 292,\n",
       " 'fuera': 293,\n",
       " 'e': 294,\n",
       " 'chingar': 295,\n",
       " 'Pues': 296,\n",
       " 'Esta': 297,\n",
       " 'pendejos': 298,\n",
       " 'mames': 299,\n",
       " 'ella': 300,\n",
       " 'cara': 301,\n",
       " 'ü§£': 302,\n",
       " 's√≥lo': 303,\n",
       " 'mil': 304,\n",
       " 's√∫per': 305,\n",
       " 'rico': 306,\n",
       " 'noche': 307,\n",
       " 'mundial': 308,\n",
       " 'luego': 309,\n",
       " 'jajajaja': 310,\n",
       " 'fin': 311,\n",
       " 'estos': 312,\n",
       " 'encanta': 313,\n",
       " 'amigo': 314,\n",
       " 'Tu': 315,\n",
       " 'EL': 316,\n",
       " 'üò©': 317,\n",
       " 'üé∂': 318,\n",
       " 'ustedes': 319,\n",
       " 'semana': 320,\n",
       " 'quieren': 321,\n",
       " 'pueden': 322,\n",
       " 'mientras': 323,\n",
       " 'aqu√≠': 324,\n",
       " 'antes': 325,\n",
       " 'dan': 326,\n",
       " 'Para': 327,\n",
       " 'Porque': 328,\n",
       " 'Gracias': 329,\n",
       " 'persona': 330,\n",
       " 'jaja': 331,\n",
       " 'gay': 332,\n",
       " 'dinero': 333,\n",
       " 'dijo': 334,\n",
       " 'creo': 335,\n",
       " 'buen': 336,\n",
       " 'amo': 337,\n",
       " 'YA': 338,\n",
       " 'PUTA': 339,\n",
       " 'ven': 340,\n",
       " 'sabes': 341,\n",
       " 'novio': 342,\n",
       " 'dar': 343,\n",
       " 'creen': 344,\n",
       " 'chingo': 345,\n",
       " 'caga': 346,\n",
       " 'Un': 347,\n",
       " 'Est√°': 348,\n",
       " 'Con': 349,\n",
       " 'As√≠': 350,\n",
       " 'unos': 351,\n",
       " 'putita': 352,\n",
       " 'perra': 353,\n",
       " 'peor': 354,\n",
       " 'gata': 355,\n",
       " 'falta': 356,\n",
       " 'digan': 357,\n",
       " 'deja': 358,\n",
       " 'a√∫n': 359,\n",
       " 'amiga': 360,\n",
       " 'Soy': 361,\n",
       " 'Putas': 362,\n",
       " 'EN': 363,\n",
       " '4': 364,\n",
       " '1': 365,\n",
       " 'üò†': 366,\n",
       " 'triste': 367,\n",
       " 'puedes': 368,\n",
       " 'mamen': 369,\n",
       " 'horas': 370,\n",
       " 'hablar': 371,\n",
       " 'entiendo': 372,\n",
       " 'dejar': 373,\n",
       " 'sean': 374,\n",
       " 'saben': 375,\n",
       " 'qui√©n': 376,\n",
       " 'pelan': 377,\n",
       " 'pa√≠s': 378,\n",
       " 'novia': 379,\n",
       " 'hora': 380,\n",
       " 'haciendo': 381,\n",
       " 'dormir': 382,\n",
       " 'despu√©s': 383,\n",
       " 'andar': 384,\n",
       " 'Ni': 385,\n",
       " '5': 386,\n",
       " 'üá≤üáΩ': 387,\n",
       " 'viendo': 388,\n",
       " 'vamos': 389,\n",
       " 'tarea': 390,\n",
       " 'tal': 391,\n",
       " 'prieta': 392,\n",
       " 'mamadas': 393,\n",
       " 'gusto': 394,\n",
       " 'cosa': 395,\n",
       " 'asi': 396,\n",
       " 'asco': 397,\n",
       " 'Ojal√°': 398,\n",
       " 'Hay': 399,\n",
       " 'üò§': 400,\n",
       " 'üòò': 401,\n",
       " 'vali√≥': 402,\n",
       " 'se√±ora': 403,\n",
       " 'saber': 404,\n",
       " 'rica': 405,\n",
       " 'palabra': 406,\n",
       " 'nalgas': 407,\n",
       " 'maric√≥n': 408,\n",
       " 'hago': 409,\n",
       " 'hab√≠a': 410,\n",
       " 'ellos': 411,\n",
       " 'c√≥mo': 412,\n",
       " 'boca': 413,\n",
       " 'SU': 414,\n",
       " 'LOS': 415,\n",
       " '#MasterChefMx': 416,\n",
       " 'üòè': 417,\n",
       " 'poca': 418,\n",
       " 'perro': 419,\n",
       " 'mandar': 420,\n",
       " 'llorar': 421,\n",
       " 'hombre': 422,\n",
       " 'chingas': 423,\n",
       " 'a√±o': 424,\n",
       " 'Todos': 425,\n",
       " 'Tengo': 426,\n",
       " 'Pinches': 427,\n",
       " 'üòå': 428,\n",
       " 'üíî': 429,\n",
       " 'üçÜ': 430,\n",
       " '√©l': 431,\n",
       " 'vergas': 432,\n",
       " 'sigue': 433,\n",
       " 'risa': 434,\n",
       " 're': 435,\n",
       " 'quieres': 436,\n",
       " 'queda': 437,\n",
       " 'puro': 438,\n",
       " 'ponen': 439,\n",
       " 'pone': 440,\n",
       " 'otras': 441,\n",
       " 'odio': 442,\n",
       " 'misma': 443,\n",
       " 'miedo': 444,\n",
       " 'iba': 445,\n",
       " 'hubiera': 446,\n",
       " 'golfa': 447,\n",
       " 'ex': 448,\n",
       " 'dejen': 449,\n",
       " 'debe': 450,\n",
       " 'bonito': 451,\n",
       " 'PARA': 452,\n",
       " 'siguen': 453,\n",
       " 'pobre': 454,\n",
       " 'parte': 455,\n",
       " 'importa': 456,\n",
       " 'hizo': 457,\n",
       " 'hija': 458,\n",
       " 'feo': 459,\n",
       " 'feliz': 460,\n",
       " 'fea': 461,\n",
       " 'favor': 462,\n",
       " 'culpa': 463,\n",
       " 'Quiero': 464,\n",
       " 'Este': 465,\n",
       " 'Alguien': 466,\n",
       " '10': 467,\n",
       " 'üî•': 468,\n",
       " 'vieja': 469,\n",
       " 'valiendo': 470,\n",
       " 'tarde': 471,\n",
       " 'seguro': 472,\n",
       " 'salir': 473,\n",
       " 'pu√±al': 474,\n",
       " 'poner': 475,\n",
       " 'pensar': 476,\n",
       " 'partido': 477,\n",
       " 'minutos': 478,\n",
       " 'lugar': 479,\n",
       " 'llega': 480,\n",
       " 'diga': 481,\n",
       " 'chinga': 482,\n",
       " 'canci√≥n': 483,\n",
       " 'ando': 484,\n",
       " 'anda': 485,\n",
       " 'RT': 486,\n",
       " 'PUTO': 487,\n",
       " 'Eso': 488,\n",
       " '$': 489,\n",
       " 'üôÉ': 490,\n",
       " 'üòû': 491,\n",
       " 'üòî': 492,\n",
       " 'somos': 493,\n",
       " 'sido': 494,\n",
       " 'pena': 495,\n",
       " 'parece': 496,\n",
       " 'momento': 497,\n",
       " 'mando': 498,\n",
       " 'lameculos': 499,\n",
       " 'huevos': 500,\n",
       " 'hermano': 501,\n",
       " 'familia': 502,\n",
       " 'entre': 503,\n",
       " 'contigo': 504,\n",
       " 'bonita': 505,\n",
       " 'agua': 506,\n",
       " 'acabo': 507,\n",
       " 'Siempre': 508,\n",
       " 'Neta': 509,\n",
       " 'Les': 510,\n",
       " 'Le': 511,\n",
       " 'Hasta': 512,\n",
       " 'ES': 513,\n",
       " 'ü§¶üèª\\u200d‚ôÄ': 514,\n",
       " 'üòé': 515,\n",
       " 'üí¶': 516,\n",
       " '|': 517,\n",
       " 'volver': 518,\n",
       " 'visto': 519,\n",
       " 'viejas': 520,\n",
       " 'ves': 521,\n",
       " 'valen': 522,\n",
       " 'sobre': 523,\n",
       " 'servicio': 524,\n",
       " 'seas': 525,\n",
       " 'sale': 526,\n",
       " 'primera': 527,\n",
       " 'pongo': 528,\n",
       " 'poder': 529,\n",
       " 'perros': 530,\n",
       " 'pasan': 531,\n",
       " 'pasado': 532,\n",
       " 'nuevo': 533,\n",
       " 'mama': 534,\n",
       " 'lado': 535,\n",
       " 'fui': 536,\n",
       " 'forma': 537,\n",
       " 'escuela': 538,\n",
       " 'escuchar': 539,\n",
       " 'dije': 540,\n",
       " 'dicho': 541,\n",
       " 'dices': 542,\n",
       " 'conmigo': 543,\n",
       " 'Twitter': 544,\n",
       " 'TU': 545,\n",
       " 'Solo': 546,\n",
       " 'SE': 547,\n",
       " 'Jajajajaja': 548,\n",
       " 'Gorda': 549,\n",
       " 'Dios': 550,\n",
       " 'C√≥mo': 551,\n",
       " 'Creo': 552,\n",
       " 'Ay': 553,\n",
       " 'Aqu√≠': 554,\n",
       " 'Ah': 555,\n",
       " '7': 556,\n",
       " '20': 557,\n",
       " 'üò±': 558,\n",
       " 'vista': 559,\n",
       " 'video': 560,\n",
       " 'valgo': 561,\n",
       " 'tanta': 562,\n",
       " 'sigo': 563,\n",
       " 'ser√°': 564,\n",
       " 'rato': 565,\n",
       " 'problema': 566,\n",
       " 'pa': 567,\n",
       " 'otros': 568,\n",
       " 'm√∫sica': 569,\n",
       " 'llevo': 570,\n",
       " 'lleva': 571,\n",
       " 'jajajajaja': 572,\n",
       " 'hecho': 573,\n",
       " 'haga': 574,\n",
       " 'haber': 575,\n",
       " 'grande': 576,\n",
       " 'gracias': 577,\n",
       " 'entonces': 578,\n",
       " 'doy': 579,\n",
       " 'diciendo': 580,\n",
       " 'chiflar': 581,\n",
       " 'cagan': 582,\n",
       " 'buenas': 583,\n",
       " 'andan': 584,\n",
       " 'amigas': 585,\n",
       " 'V': 586,\n",
       " 'Su': 587,\n",
       " 'POR': 588,\n",
       " 'PINCHE': 589,\n",
       " 'LOCA': 590,\n",
       " 'JAJAJA': 591,\n",
       " 'üôä': 592,\n",
       " 'üò£': 593,\n",
       " '‚òπ': 594,\n",
       " '√∫nico': 595,\n",
       " 'xD': 596,\n",
       " 'vi': 597,\n",
       " 'tres': 598,\n",
       " 'trabajar': 599,\n",
       " 'siendo': 600,\n",
       " 'seguir': 601,\n",
       " 'quiera': 602,\n",
       " 'quer√≠a': 603,\n",
       " 'primero': 604,\n",
       " 'pap√°': 605,\n",
       " 'padre': 606,\n",
       " 'ojal√°': 607,\n",
       " 'ni√±o': 608,\n",
       " 'ni√±as': 609,\n",
       " 'ni√±a': 610,\n",
       " 'meter': 611,\n",
       " 'hablando': 612,\n",
       " 'final': 613,\n",
       " 'estamos': 614,\n",
       " 'dem√°s': 615,\n",
       " 'das': 616,\n",
       " 'comer': 617,\n",
       " 'clase': 618,\n",
       " 'caso': 619,\n",
       " 'casi': 620,\n",
       " 'UN': 621,\n",
       " 'TE': 622,\n",
       " 'Mis': 623,\n",
       " 'Marica': 624,\n",
       " 'LO': 625,\n",
       " 'LAS': 626,\n",
       " 'Eres': 627,\n",
       " 'COMO': 628,\n",
       " 'üò¨': 629,\n",
       " 'üò¢': 630,\n",
       " 'üòä': 631,\n",
       " 'üòÅ': 632,\n",
       " 'viejo': 633,\n",
       " 'tipo': 634,\n",
       " 'sola': 635,\n",
       " 'respeto': 636,\n",
       " 'raz√≥n': 637,\n",
       " 'pasar': 638,\n",
       " 'palabras': 639,\n",
       " 'pagar': 640,\n",
       " 'nombre': 641,\n",
       " 'noches': 642,\n",
       " 'medio': 643,\n",
       " 'mano': 644,\n",
       " 'llama': 645,\n",
       " 'huevo': 646,\n",
       " 'hambre': 647,\n",
       " 'haces': 648,\n",
       " 'esperando': 649,\n",
       " 'equipo': 650,\n",
       " 'cualquier': 651,\n",
       " 'creer': 652,\n",
       " 'chinguen': 653,\n",
       " 'chica': 654,\n",
       " 'celular': 655,\n",
       " 'calle': 656,\n",
       " 'aunque': 657,\n",
       " 'ardidas': 658,\n",
       " 'S√≠': 659,\n",
       " 'Quien': 660,\n",
       " 'Puto': 661,\n",
       " 'Mira': 662,\n",
       " 'Ma√±ana': 663,\n",
       " 'Esa': 664,\n",
       " 'Al': 665,\n",
       " '6': 666,\n",
       " '&': 667,\n",
       " 'ü§§': 668,\n",
       " 'üòú': 669,\n",
       " 'vos': 670,\n",
       " 'videos': 671,\n",
       " 'vaya': 672,\n",
       " 'ten√≠a': 673,\n",
       " 'siente': 674,\n",
       " 'punto': 675,\n",
       " 'porqu√©': 676,\n",
       " 'poco': 677,\n",
       " 'paso': 678,\n",
       " 'partir': 679,\n",
       " 'parecen': 680,\n",
       " 'ojos': 681,\n",
       " 'nosotros': 682,\n",
       " 'maldito': 683,\n",
       " 'lluvia': 684,\n",
       " 'hice': 685,\n",
       " 'gordas': 686,\n",
       " 'estan': 687,\n",
       " 'estado': 688,\n",
       " 'eran': 689,\n",
       " 'ellas': 690,\n",
       " 'd√≥nde': 691,\n",
       " 'den': 692,\n",
       " 'deber√≠an': 693,\n",
       " 'culero': 694,\n",
       " 'coraz√≥n': 695,\n",
       " 'clases': 696,\n",
       " 'cargo': 697,\n",
       " 'cae': 698,\n",
       " 'ayer': 699,\n",
       " 'adem√°s': 700,\n",
       " 'Todo': 701,\n",
       " 'Teresa': 702,\n",
       " 'TODOS': 703,\n",
       " 'SI': 704,\n",
       " 'Puta': 705,\n",
       " 'Jajajajajaja': 706,\n",
       " 'HIJO': 707,\n",
       " 'CON': 708,\n",
       " '@': 709,\n",
       " 'üò™': 710,\n",
       " 'üòï': 711,\n",
       " 'üòã': 712,\n",
       " 'x': 713,\n",
       " 'vuelve': 714,\n",
       " 'viene': 715,\n",
       " 'usar': 716,\n",
       " 'tengan': 717,\n",
       " 'tenemos': 718,\n",
       " 'tambien': 719,\n",
       " 'tacos': 720,\n",
       " 'sue√±o': 721,\n",
       " 'serio': 722,\n",
       " 'salen': 723,\n",
       " 'sabemos': 724,\n",
       " 'pura': 725,\n",
       " 'pueblo': 726,\n",
       " 'poniendo': 727,\n",
       " 'pol√≠ticos': 728,\n",
       " 'pienso': 729,\n",
       " 'periodistas': 730,\n",
       " 'pendejas': 731,\n",
       " 'nivel': 732,\n",
       " 'meses': 733,\n",
       " 'mes': 734,\n",
       " 'mariquita': 735,\n",
       " 'lleno': 736,\n",
       " 'llegar': 737,\n",
       " 'juego': 738,\n",
       " 'historia': 739,\n",
       " 'hermoso': 740,\n",
       " 'hagan': 741,\n",
       " 'grupo': 742,\n",
       " 'fan': 743,\n",
       " 'espero': 744,\n",
       " 'escribir': 745,\n",
       " 'dio': 746,\n",
       " 'demasiado': 747,\n",
       " 'deje': 748,\n",
       " 'dado': 749,\n",
       " 'contra': 750,\n",
       " 'arruga': 751,\n",
       " 'Wey': 752,\n",
       " 'Son': 753,\n",
       " 'Qui√©n': 754,\n",
       " 'HIJOS': 755,\n",
       " 'Chinga': 756,\n",
       " 'ALV': 757,\n",
       " 'üôà': 758,\n",
       " 'üòª': 759,\n",
       " 'üòÖ': 760,\n",
       " 'üíï': 761,\n",
       " 'volviendo': 762,\n",
       " 'vato': 763,\n",
       " 'tuits': 764,\n",
       " 'tenga': 765,\n",
       " 'sali√≥': 766,\n",
       " 'putito': 767,\n",
       " 'primer': 768,\n",
       " 'pones': 769,\n",
       " 'pesos': 770,\n",
       " 'pela': 771,\n",
       " 'partidos': 772,\n",
       " 'nuestros': 773,\n",
       " 'nuestro': 774,\n",
       " 'necesito': 775,\n",
       " 'muchas': 776,\n",
       " 'morra': 777,\n",
       " 'moral': 778,\n",
       " 'mayor': 779,\n",
       " 'matar': 780,\n",
       " 'manos': 781,\n",
       " 'mamo': 782,\n",
       " 'mamaste': 783,\n",
       " 'maldita': 784,\n",
       " 'mala': 785,\n",
       " 'leche': 786,\n",
       " 'lados': 787,\n",
       " 'hermosa': 788,\n",
       " 'has': 789,\n",
       " 'gran': 790,\n",
       " 'golfas': 791,\n",
       " 'f√∫tbol': 792,\n",
       " 'fueron': 793,\n",
       " 'fr√≠o': 794,\n",
       " 'diario': 795,\n",
       " 'dejan': 796,\n",
       " 'darle': 797,\n",
       " 'dando': 798,\n",
       " 'costumbre': 799,\n",
       " 'cierto': 800,\n",
       " 'chingue': 801,\n",
       " 'cabeza': 802,\n",
       " 'buenos': 803,\n",
       " 'bola': 804,\n",
       " 'ba√±o': 805,\n",
       " 'acabar': 806,\n",
       " 'Vale': 807,\n",
       " 'T√∫': 808,\n",
       " 'SUS': 809,\n",
       " 'Mam√°': 810,\n",
       " 'MI': 811,\n",
       " 'Jaja': 812,\n",
       " 'Esos': 813,\n",
       " 'Esas': 814,\n",
       " 'Desde': 815,\n",
       " 'Bueno': 816,\n",
       " '>': 817,\n",
       " 'ü§∑üèª\\u200d‚ôÄ': 818,\n",
       " 'ü§ó': 819,\n",
       " 'üòë': 820,\n",
       " 'üòâ': 821,\n",
       " 'üëè': 822,\n",
       " 'xq': 823,\n",
       " 'we': 824,\n",
       " 'vivo': 825,\n",
       " 'vayan': 826,\n",
       " 'tuve': 827,\n",
       " 'tristes': 828,\n",
       " 'tantos': 829,\n",
       " 'tantas': 830,\n",
       " 'suerte': 831,\n",
       " 'siquiera': 832,\n",
       " 'sienten': 833,\n",
       " 'saca': 834,\n",
       " 'ropa': 835,\n",
       " 'puras': 836,\n",
       " 'perder': 837,\n",
       " 'pens√©': 838,\n",
       " 'paz': 839,\n",
       " 'pase': 840,\n",
       " 'nueva': 841,\n",
       " 'ni√±os': 842,\n",
       " 'ning√∫n': 843,\n",
       " 'mandan': 844,\n",
       " 'mam√≥': 845,\n",
       " 'mamada': 846,\n",
       " 'llorando': 847,\n",
       " 'jugar': 848,\n",
       " 'jam√°s': 849,\n",
       " 'hondure√±os': 850,\n",
       " 'hermana': 851,\n",
       " 'habla': 852,\n",
       " 'gustan': 853,\n",
       " 'gobierno': 854,\n",
       " 'frase': 855,\n",
       " 'examen': 856,\n",
       " 'empieza': 857,\n",
       " 'dijeron': 858,\n",
       " 'darme': 859,\n",
       " 'cual': 860,\n",
       " 'comiendo': 861,\n",
       " 'chile': 862,\n",
       " 'caracteres': 863,\n",
       " 'cagada': 864,\n",
       " 'bonitas': 865,\n",
       " 'Sabes': 866,\n",
       " 'Q': 867,\n",
       " 'Nunca': 868,\n",
       " 'Nada': 869,\n",
       " 'M√°s': 870,\n",
       " 'Messi': 871,\n",
       " 'Esto': 872,\n",
       " 'Ese': 873,\n",
       " 'DEL': 874,\n",
       " 'Chingas': 875,\n",
       " 'Chile': 876,\n",
       " 'Cada': 877,\n",
       " 'Amo': 878,\n",
       " '8': 879,\n",
       " 'üòñ': 880,\n",
       " 'üòê': 881,\n",
       " 'üíñ': 882,\n",
       " 'üëå': 883,\n",
       " '‚ò∫': 884,\n",
       " 'v√°yanse': 885,\n",
       " 'vuelven': 886,\n",
       " 'vivir': 887,\n",
       " 'venir': 888,\n",
       " 'vemos': 889,\n",
       " 'valga': 890,\n",
       " 'usan': 891,\n",
       " 'tweets': 892,\n",
       " 'trae': 893,\n",
       " 'todav√≠a': 894,\n",
       " 'temprano': 895,\n",
       " 'sigues': 896,\n",
       " 'ser√≠a': 897,\n",
       " 'seria': 898,\n",
       " 'salgo': 899,\n",
       " 'querer': 900,\n",
       " 'prietas': 901,\n",
       " 'peda': 902,\n",
       " 'pas√≥': 903,\n",
       " 'normal': 904,\n",
       " 'naturaleza': 905,\n",
       " 'm√≠a': 906,\n",
       " 'mucha': 907,\n",
       " 'mira': 908,\n",
       " 'mente': 909,\n",
       " 'mentada': 910,\n",
       " 'juntos': 911,\n",
       " 'ja': 912,\n",
       " 'imagen': 913,\n",
       " 'horrible': 914,\n",
       " 'haya': 915,\n",
       " 'hacerlo': 916,\n",
       " 'hablas': 917,\n",
       " 'hablan': 918,\n",
       " 'gringos': 919,\n",
       " 'gatos': 920,\n",
       " 'fuerte': 921,\n",
       " 'extra√±o': 922,\n",
       " 'etc': 923,\n",
       " 'est√©': 924,\n",
       " 'estar√≠a': 925,\n",
       " 'esperar': 926,\n",
       " 'empiezan': 927,\n",
       " 'edad': 928,\n",
       " 'duele': 929,\n",
       " 'diferente': 930,\n",
       " 'dieron': 931,\n",
       " 'derechos': 932,\n",
       " 'dentro': 933,\n",
       " 'cuerpo': 934,\n",
       " 'critican': 935,\n",
       " 'cree': 936,\n",
       " 'clima': 937,\n",
       " 'claro': 938,\n",
       " 'carro': 939,\n",
       " 'cagas': 940,\n",
       " 'ayuda': 941,\n",
       " 'am': 942,\n",
       " 'alg√∫n': 943,\n",
       " 'alguna': 944,\n",
       " 'alcohol': 945,\n",
       " 'acaba': 946,\n",
       " 'UNA': 947,\n",
       " 'Todas': 948,\n",
       " 'Tambi√©n': 949,\n",
       " 'S√≥lo': 950,\n",
       " 'Nos': 951,\n",
       " 'Mejor': 952,\n",
       " 'Loca': 953,\n",
       " 'Gente': 954,\n",
       " 'Facebook': 955,\n",
       " 'Est√°n': 956,\n",
       " 'Cosas': 957,\n",
       " 'Calcuta': 958,\n",
       " 'Buenos': 959,\n",
       " 'Ando': 960,\n",
       " '280': 961,\n",
       " 'ü§¶üèª\\u200d‚ôÇ': 962,\n",
       " 'üôÇ': 963,\n",
       " 'üòì': 964,\n",
       " 'üê∑': 965,\n",
       " 'üéµ': 966,\n",
       " '‚úäüèº': 967,\n",
       " '‚Äî': 968,\n",
       " '√∫nica': 969,\n",
       " 'verg√ºenza': 970,\n",
       " 'vergazos': 971,\n",
       " 'tr√°fico': 972,\n",
       " 'tobog√°n': 973,\n",
       " 'tel√©fono': 974,\n",
       " 't': 975,\n",
       " 'super': 976,\n",
       " 'subir': 977,\n",
       " 'sigan': 978,\n",
       " 'se√±or': 979,\n",
       " 'sexo': 980,\n",
       " 'sentir': 981,\n",
       " 'sentido': 982,\n",
       " 'selecci√≥n': 983,\n",
       " 'sab√≠a': 984,\n",
       " 'regreso': 985,\n",
       " 'realidad': 986,\n",
       " 'rateros': 987,\n",
       " 'queriendo': 988,\n",
       " 'puso': 989,\n",
       " 'puse': 990,\n",
       " 'puesto': 991,\n",
       " 'presidente': 992,\n",
       " 'porno': 993,\n",
       " 'ponerme': 994,\n",
       " 'placer': 995,\n",
       " 'piensan': 996,\n",
       " 'piel': 997,\n",
       " 'pesar': 998,\n",
       " 'pensando': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generamos las bolsas de palabrascon pesado binario  de training y de validaci√≥n \n",
    "binary_bow_tr=build_binary_bow(tr_txt,dict_freq,dict_indices)\n",
    "binary_boW_validacion=build_binary_bow(val_txt,dict_freq,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generamos las bolsas de palabras con pesado frecuencia de training y de validaci√≥n \n",
    "frequency_bow_tr=build_freq_bow(tr_txt,dict_freq,dict_indices)\n",
    "frequency_boW_validacion=build_freq_bow(val_txt,dict_freq,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generamos las bolsas de palabras con pesado tfidf de training y de validaci√≥n \n",
    "tfidf_bow_tr=build_tfidf_bow(tr_txt,dict_freq,dict_indices)\n",
    "tfidf_boW_validacion=build_tfidf_bow(val_txt,dict_freq,dict_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Diferencias en bolsa de palabras\n",
    "\n",
    "veamos la diferencia entre los pesos con cada esquema para un caso particular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el token @USUARIO corresponde al lugar 9 en el diccionario de frecuencias \n",
      "el primer tweet de los datos (es decir el documento 0) contiene tres veces este token\n",
      "El peso w_(0,9) es:\n",
      "con pesado binario: 1\n",
      "con pesado de frecnuencia: 3\n",
      "con pesado de tfidf: 5\n"
     ]
    }
   ],
   "source": [
    "#el token \"\"@USUARIO\"\" corresponde al lugar 9 en el diccionario de frecuencias \n",
    "x=dict_indices.get(\"@USUARIO\")\n",
    "print(\"el token \"\"@USUARIO\"\" corresponde al lugar\",x, \"en el diccionario de frecuencias \")\n",
    "print(\"el primer tweet de los datos (es decir el documento 0) contiene tres veces este token\")\n",
    "print(\"El peso w_(0,9) es:\")\n",
    "print(\"con pesado binario:\",binary_bow_tr[0][9])\n",
    "print(\"con pesado de frecnuencia:\", frequency_bow_tr[0][9])\n",
    "print(\"con pesado de tfidf:\", tfidf_bow_tr[0][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Clasificador}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras binaria y las etiqutas\n",
    "grid.fit(binary_bow_tr,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_binary=grid.predict(binary_boW_validacion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras de frecuencia y las etiqutas\n",
    "grid.fit(frequency_bow_tr,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_frequency=grid.predict(frequency_boW_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras TFIDF y las etiqutas\n",
    "grid.fit(tfidf_bow_tr,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_tfidf=grid.predict(tfidf_boW_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Binary BoW_____________\n",
      "[[356  62]\n",
      " [ 49 120]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.87       418\n",
      "           1       0.66      0.71      0.68       169\n",
      "\n",
      "    accuracy                           0.81       587\n",
      "   macro avg       0.77      0.78      0.77       587\n",
      "weighted avg       0.82      0.81      0.81       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________Binary BoW_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_binary))\n",
    "print(metrics.classification_report(val_labels,y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Frequency BoW_____________\n",
      "[[362  56]\n",
      " [ 45 124]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88       418\n",
      "           1       0.69      0.73      0.71       169\n",
      "\n",
      "    accuracy                           0.83       587\n",
      "   macro avg       0.79      0.80      0.79       587\n",
      "weighted avg       0.83      0.83      0.83       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________Frequency BoW_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_frequency))\n",
    "print(metrics.classification_report(val_labels,y_pred_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________TfiDf BoW_____________\n",
      "[[356  62]\n",
      " [ 71  98]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       418\n",
      "           1       0.61      0.58      0.60       169\n",
      "\n",
      "    accuracy                           0.77       587\n",
      "   macro avg       0.72      0.72      0.72       587\n",
      "weighted avg       0.77      0.77      0.77       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________TfiDf BoW_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_tfidf))\n",
    "print(metrics.classification_report(val_labels,y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "4,5,6. Evalue BoW con pesado binario, de frecuencia y tfidf con normalizado l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Bolsas de palabras con distintos equemas de pesado con normalizado L2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizacion_dos(X):\n",
    "    norm = np.linalg.norm(X, ord=2, axis=1)  # Calcula la norma L2 de cada fila\n",
    "    norm = np.where(norm == 0, 1, norm) \n",
    "    norm = norm.reshape(-1, 1)  # Reshape para que la divisi√≥n sea compatible\n",
    "    return X / norm  # Divide cada elemento de X por la norma correspondiente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizamos las bolsas de palabrascon pesado binario  de training y de validaci√≥n \n",
    "binary_bow_tr_norm=normalizacion_dos(binary_bow_tr)\n",
    "binary_boW_validacion_norm=normalizacion_dos(binary_boW_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizamos las bolsas de palabrascon pesado frecuencia  de training y de validaci√≥n \n",
    "frequency_bow_tr_norm=normalizacion_dos(frequency_bow_tr)\n",
    "frequency_boW_validacion_norm=normalizacion_dos(frequency_boW_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizamos las bolsas de palabrascon pesado tfidf  de training y de validaci√≥n \n",
    "tfidf_bow_tr_norm=normalizacion_dos(tfidf_bow_tr)\n",
    "tfidf_boW_validacion_norm=normalizacion_dos(tfidf_boW_validacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Diferencias en bolsa de palabras\n",
    "\n",
    "veamos la diferencia entre los pesos con cada esquema para un caso particular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el token @USUARIO corresponde al lugar 9 en el diccionario de frecuencias \n",
      "el primer tweet de los datos (es decir el documento 0) contiene tres veces este token\n",
      "El peso w_(0,9) es:\n",
      "con pesado binario: 0.20412414523193154\n",
      "con pesado de frecnuencia: 0.48666426339228763\n",
      "con pesado de tfidf: 0.21128856368212914\n"
     ]
    }
   ],
   "source": [
    "#el token \"\"@USUARIO\"\" corresponde al lugar 9 en el diccionario de frecuencias \n",
    "x=dict_indices.get(\"@USUARIO\")\n",
    "print(\"el token \"\"@USUARIO\"\" corresponde al lugar\",x, \"en el diccionario de frecuencias \")\n",
    "print(\"el primer tweet de los datos (es decir el documento 0) contiene tres veces este token\")\n",
    "print(\"El peso w_(0,9) es:\")\n",
    "print(\"con pesado binario:\",binary_bow_tr_norm[0][9])\n",
    "print(\"con pesado de frecnuencia:\", frequency_bow_tr_norm[0][9])\n",
    "print(\"con pesado de tfidf:\", tfidf_bow_tr_norm[0][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Clasificador}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras binaria y las etiqutas\n",
    "grid.fit(binary_bow_tr_norm,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_binary_norm=grid.predict(binary_boW_validacion_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras de frecuencia y las etiqutas\n",
    "grid.fit(frequency_bow_tr_norm,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_frequency_norm=grid.predict(frequency_boW_validacion_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras TFIDF y las etiqutas\n",
    "grid.fit(tfidf_bow_tr_norm,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_tfidf_norm=grid.predict(tfidf_boW_validacion_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Binary BoW L2 normalization_____________\n",
      "[[356  62]\n",
      " [ 47 122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.87       418\n",
      "           1       0.66      0.72      0.69       169\n",
      "\n",
      "    accuracy                           0.81       587\n",
      "   macro avg       0.77      0.79      0.78       587\n",
      "weighted avg       0.82      0.81      0.82       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________Binary BoW L2 normalization_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_binary_norm))\n",
    "print(metrics.classification_report(val_labels,y_pred_binary_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Frequency BoW L2 normalization_____________\n",
      "[[358  60]\n",
      " [ 38 131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88       418\n",
      "           1       0.69      0.78      0.73       169\n",
      "\n",
      "    accuracy                           0.83       587\n",
      "   macro avg       0.79      0.82      0.80       587\n",
      "weighted avg       0.84      0.83      0.84       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________Frequency BoW L2 normalization_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_frequency_norm))\n",
    "print(metrics.classification_report(val_labels,y_pred_frequency_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________TfiDf BoW L2 normalization_____________\n",
      "[[356  62]\n",
      " [ 40 129]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.87       418\n",
      "           1       0.68      0.76      0.72       169\n",
      "\n",
      "    accuracy                           0.83       587\n",
      "   macro avg       0.79      0.81      0.80       587\n",
      "weighted avg       0.83      0.83      0.83       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________TfiDf BoW L2 normalization_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_tfidf_norm))\n",
    "print(metrics.classification_report(val_labels,y_pred_tfidf_norm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "7. Ponga una tabla comparativa a modo de resumen con las seis entradas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesado binario \n",
      " Accuracy 0.82 \n",
      " Precision (0) 0.88 \n",
      " Precision (1) 0.66\n",
      "Pesado binario normlizado \n",
      " Accuracy 0.82 \n",
      " Precision (0) 0.88 \n",
      " Precision (1) 0.66\n",
      "Pesado frecuencia \n",
      " Accuracy 0.83 \n",
      " Precision (0) 0.89 \n",
      " Precision (1) 0.69\n",
      "Pesado frecuencia normalizado \n",
      " Accuracy 0.84 \n",
      " Precision (0) 0.9 \n",
      " Precision (1) 0.69\n",
      "Pesado tfidf \n",
      " Accuracy 0.77 \n",
      " Precision (0) 0.83 \n",
      " Precision (1) 0.61\n",
      "Pesado tfidf normalizado \n",
      " Accuracy 0.83 \n",
      " Precision (0) 0.9 \n",
      " Precision (1) 0.68\n"
     ]
    }
   ],
   "source": [
    "lista_predicciones=[y_pred_binary,y_pred_binary_norm,y_pred_frequency,y_pred_frequency_norm,y_pred_tfidf,y_pred_tfidf_norm]\n",
    "lista_pesados=[\"Pesado binario\",\"Pesado binario normlizado\",\"Pesado frecuencia\",\"Pesado frecuencia normalizado\",\"Pesado tfidf\",\"Pesado tfidf normalizado\"]\n",
    "for predict, pesado in zip(lista_predicciones, lista_pesados):\n",
    "    m = metrics.classification_report(val_labels, predict)\n",
    "    accuracy = float(m.split('\\n')[-2].split()[2])\n",
    "    precision_0 = float(m.split('\\n')[2].split()[1])  \n",
    "    precision_1 = float(m.split('\\n')[3].split()[1])  \n",
    "    \n",
    "    print(pesado, \"\\n Accuracy\", accuracy, \"\\n Precision (0)\",precision_0,\"\\n Precision (1)\",precision_1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El accuracy m√°s alto se obtene para el pesado *frecuencia normalizado*, con 0.84. Como tenemos un conjunto desbalanceado buscaremos maximizar el recall para la clase minoritaria (etiqueta 1) para capturar la mayor cantidad posible de casos positivos verdaderos, as√≠ que comparremos con la el siguiente pesado con mayor accuracy que es *Pesado tfidf normalizado*. Notemos a precisi√≥n para la etiqueta 0 es la misma: 0.9, miestras que para la etiqueta uno varia un poco (0.1), siendo mayor para Frecuecia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesado frecuencia normalizado \n",
      " Accuracy 0.84 \n",
      " Recall (0) 0.86 \n",
      " Recall (1) 0.78\n",
      "Pesado tfidf normalizado \n",
      " Accuracy 0.83 \n",
      " Recall (0) 0.85 \n",
      " Recall (1) 0.76\n"
     ]
    }
   ],
   "source": [
    "lista_predicciones=[y_pred_frequency_norm,y_pred_tfidf_norm]\n",
    "lista_pesados=[\"Pesado frecuencia normalizado\",\"Pesado tfidf normalizado\"]\n",
    "for predict, pesado in zip(lista_predicciones, lista_pesados):\n",
    "    m = metrics.classification_report(val_labels, predict)\n",
    "    accuracy = float(m.split('\\n')[-2].split()[2])\n",
    "    precision_0 = float(m.split('\\n')[2].split()[2])  \n",
    "    precision_1 = float(m.split('\\n')[3].split()[2])  \n",
    "    \n",
    "    print(pesado, \"\\n Accuracy\", accuracy, \"\\n Recall (0)\",precision_0,\"\\n Recall (1)\",precision_1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As√≠ que el mejor resultado se obtuvo con pesado defrecuencia y normalizaci√≥n L2, mayor accuracy, mayor precision e incluso mayor recall para la etiqueta minoritaria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "8. De las configuraciones anteriores elija la mejor y eval√∫ela con m√°s y menos t√©rminos (e.g., 1000 y 7000). Ponga una tabla d√≥nde compare las tres configuraciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos diccionario de indices y de frecuencias haciendo corte en las mil primeras palabras\n",
    "dict_freq_1000=create_dic_freq(corpus_palabras,1000)\n",
    "dict_indices_1000=create_dic_ranking(dict_freq_1000)\n",
    "\n",
    "#Bolsas de palabras con pesado frecuencia de training y de validaci√≥n usando los diccionarios anteriores\n",
    "bow_tr_1000=build_freq_bow(tr_txt,dict_freq_1000,dict_indices_1000)\n",
    "boW_validacion_1000=build_freq_bow(val_txt,dict_freq_1000,dict_indices_1000)\n",
    "\n",
    "#normalizamos \n",
    "bow_tr_norm_1000=normalizacion_dos(bow_tr_1000)\n",
    "boW_validacion_norm_1000=normalizacion_dos(boW_validacion_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras \n",
    "grid.fit(bow_tr_norm_1000,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_1000=grid.predict(boW_validacion_norm_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Frequency BoW (1000) L2 normalization_____________\n",
      "[[346  72]\n",
      " [ 39 130]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86       418\n",
      "           1       0.64      0.77      0.70       169\n",
      "\n",
      "    accuracy                           0.81       587\n",
      "   macro avg       0.77      0.80      0.78       587\n",
      "weighted avg       0.83      0.81      0.82       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________Frequency BoW (1000) L2 normalization_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_1000))\n",
    "print(metrics.classification_report(val_labels,y_pred_1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos diccionario de indices y de frecuencias haciendo corte en las mil primeras palabras\n",
    "dict_freq_7000=create_dic_freq(corpus_palabras,7000)\n",
    "dict_indices_7000=create_dic_ranking(dict_freq_7000)\n",
    "\n",
    "#Bolsas de palabras con pesado frecuencia de training y de validaci√≥n usando los diccionarios anteriores\n",
    "bow_tr_7000=build_freq_bow(tr_txt,dict_freq_7000,dict_indices_7000)\n",
    "boW_validacion_7000=build_freq_bow(val_txt,dict_freq_7000,dict_indices_7000)\n",
    "\n",
    "#normalizamos \n",
    "bow_tr_norm_7000=normalizacion_dos(bow_tr_7000)\n",
    "boW_validacion_norm_7000=normalizacion_dos(boW_validacion_7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras \n",
    "grid.fit(bow_tr_norm_7000,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_7000=grid.predict(boW_validacion_norm_7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Frequency BoW (7000) L2 normalization_____________\n",
      "[[359  59]\n",
      " [ 39 130]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88       418\n",
      "           1       0.69      0.77      0.73       169\n",
      "\n",
      "    accuracy                           0.83       587\n",
      "   macro avg       0.79      0.81      0.80       587\n",
      "weighted avg       0.84      0.83      0.84       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________Frequency BoW (7000) L2 normalization_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_7000))\n",
    "print(metrics.classification_report(val_labels,y_pred_7000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comparaci√≥n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bolsa depalabras con pesado de frecuencia y normalizaci√≥n L2\n",
      "\n",
      "palabras tomadas para BoW: 1000 \n",
      " Accuracy 0.83 \n",
      " Precision (0) 0.9 \n",
      " Precision (1) 0.64\n",
      "palabras tomadas para BoW: 5000 \n",
      " Accuracy 0.84 \n",
      " Precision (0) 0.9 \n",
      " Precision (1) 0.69\n",
      "palabras tomadas para BoW: 7000 \n",
      " Accuracy 0.84 \n",
      " Precision (0) 0.9 \n",
      " Precision (1) 0.69\n"
     ]
    }
   ],
   "source": [
    "lista_predicciones=[y_pred_1000,y_pred_frequency_norm,y_pred_7000]\n",
    "lista_pesados=[\"1000\",\"5000\",\"7000\"]\n",
    "print(\"Bolsa depalabras con pesado de frecuencia y normalizaci√≥n L2\\n\")\n",
    "for predict, pesado in zip(lista_predicciones, lista_pesados):\n",
    "    m = metrics.classification_report(val_labels, predict)\n",
    "    accuracy = float(m.split('\\n')[-2].split()[2])\n",
    "    precision_0 = float(m.split('\\n')[2].split()[1])  \n",
    "    precision_1 = float(m.split('\\n')[3].split()[1])  \n",
    "    \n",
    "    print(\"palabras tomadas para BoW:\",pesado, \"\\n Accuracy\", accuracy, \"\\n Precision (0)\",precision_0,\"\\n Precision (1)\",precision_1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "9. Utilice el recurso l√©xico del Consejo Nacional de Investigaci√≥n de Canad√° llamado \"EmoLex\" (https://www.saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) para construir una \"Bolsa de Emociones\" de los Tweets de agresividad (Debe usar EmoLex en Espa√±ol). Para esto, una estrategia sencilla ser√≠a enmascarar cada palabra con su emoci√≥n, y despu√©s construir la Bolsa de Emociones (BoE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Descargamos emolex en espa√±ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emolex(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()[1:]\n",
    "\n",
    "    emolex = {}\n",
    "    for line in lines:\n",
    "        row = line.split(\"\\t\")\n",
    "        emotions, word = row[1:-1], row[-1].replace(\"\\n\", \"\")\n",
    "        emotions = [float(e) for e in emotions]\n",
    "        emolex[word] = emotions\n",
    "\n",
    "    return emolex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "emolex_dict=load_emolex(\"/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/NRC-Emotion-Lexicon/OneFilePerLanguage/Spanish-NRC-EmoLex.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detras': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abaco': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'abandonar': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abandonado': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'abandono': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0],\n",
       " 'disminuir': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'disminucion': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abba': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'abad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'abreviar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abreviatura': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abdomen': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abdominal': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'secuestro': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'aberrante': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aberraci√≥n': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'suspensi√≥n': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aborrecer': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aborrecible': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'cumplir': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'capacidad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abyecto': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ablaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ardiendo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anormal': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'a bordo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'morada': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abolir': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abolici√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abominable': [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abominaci√≥n': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aborigen': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abortar': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aborto': [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'abortivo': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'abundar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anteriormente mencionado': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'abrasi√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'extranjero': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abrogar': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abrupto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'abruptamente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'absceso': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'ausencia': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'ausente': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'absentismo': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'Ajenjo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'absoluto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'absoluci√≥n': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'absorbido': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'absorbente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'absorci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abstenerse': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abstenci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abstinencia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'resumen': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abstracci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'absurdo': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'abundancia': [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'abundante': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abuso': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'contrafuerte': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aby': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abismal': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'abismo': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acad√©mico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'academia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'acceder a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acelerar': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aceleraci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acento': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acentuar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aceptar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'aceptable': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'aceptaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'acceso': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'accesible': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'adhesi√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'accesorio': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'accidente': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0],\n",
       " 'accidental': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'accidentalmente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'espaldarazo': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0],\n",
       " 'acomodar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alojamiento': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'acompa√±amiento': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acompa√±ar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'c√≥mplice': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'realizar': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'logrado': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'logro': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'acuerdo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'conformidad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'acorde√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'cuenta': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'responsabilidad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'responsable': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'contador': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'contabilidad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'cuentas': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'autorizado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'acreci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acumularse': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'accueil': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'acumular': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acumulaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'precisi√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'preciso': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'maldito': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acusaci√≥n': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acusativo': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acusado': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'acusador': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acusando': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acostumbrado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'as': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'ac√©tico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'dolor': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'lograr': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " '√°cido': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acidez': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'reconocer': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'admitido': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'Reconocimiento': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'cumbre': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ac√∫stico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ac√∫stica': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'familiarizar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'conocido': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'familiarizado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aquiescencia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adquirir': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'adquisidor': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'adquisici√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adquisiciones': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'superficie en acres': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'hect√°reas': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acr√≥bata': [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'Actuar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'interino': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'procesable': [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'activo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'actividad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'actor': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'actual': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'realidad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'actuario': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agudeza': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'perspicacia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'acupuntura': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'extremadamente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adagio': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'firme': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0],\n",
       " 'adaptar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'adaptable': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'agregar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adicional': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ap√©ndice': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'sumador': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'adiccion': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'suma': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aditivo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'Direcci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'destinatario': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'direcciones': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'adepto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'adecuaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'adecuado': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'adherirse': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adherencia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adherente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adherido': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'adhesivo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adi√≥s': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adiposo': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'proximidad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adyacente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adjetivo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'contiguo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aplazar': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aplazamiento': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'juzgar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adjudicaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adjunto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ajustar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ajustamiento': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'auxiliar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'administrar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'administraci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'administrativo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'admirable': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'almirante': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'almirantazgo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'admiraci√≥n': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'admirar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'admirador': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'admisibilidad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'admisible': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'admisi√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'admitir': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'entrada': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aceptado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'admitiendo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mezcla': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'amonestaci√≥n': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alharaca': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adobe': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adolescencia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adolescente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adoptar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adopci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adorable': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'adoraci√≥n': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'adorar': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'adornar': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adorno': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'a la deriva': [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'adulto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'espurio': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adulterio': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'ventaja': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'avanzado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'avance': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'avanzando': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ventajoso': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'adviento': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'aventura': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'aventurero': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'adversario': [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adverso': [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adversidad': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'anunciar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anuncio publicitario': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'consejo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'aconsejable': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'aconsejado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'asesoramiento': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asesor': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'Abogac√≠a': [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'defensor': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " '√©gida': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aireaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'a√©reo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aer√≥dromo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aerodin√°mica': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aeron√°utica': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'avi√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'est√©tico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'est√©tica': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'etiolog√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'lejos': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'afable': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'asunto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'conmovedor': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'cari√±o': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'afectos': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'afiche': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'declaraci√≥n jurada': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asociado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'afiliaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'afinidad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'afirmar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'afirmaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'afirmativo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'afirmativamente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'afijo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'afligir': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'afligido': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'aflicci√≥n': [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'afluencia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'afluente': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'solventar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'afrenta': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0],\n",
       " 'al campo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'a proa': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'citado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'antedicho': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atemorizado': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'de nuevo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'en popa': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'secuelas': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'tarde': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'regusto': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'idea tard√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aga': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " '√°gata': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'a√±os': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'Envejecido': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agencia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aglomeraci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agravado': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agravante': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'agravaci√≥n': [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agregaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agresi√≥n': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agresivo': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agresor': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0],\n",
       " 'espantado': [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
       " '√°gil': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agilidad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'agitado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agitaci√≥n': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'agn√≥stico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atr√°s': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agonizante': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agon√≠a': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'agradable': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'acordado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'estar de acuerdo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'convenio': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'agr√≠cola': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agricultura': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'encallado': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agua': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adelante': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ayuda': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'ayudando': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'enfermo': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'enfermedad': [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'apuntar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'sin objetivo': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aire': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'bolsa de aire': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'puente a√©reo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aerol√≠nea': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aviador': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aeropuerto': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aires': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'dirigible': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'pasillo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'espera': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'un frasco': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'parecido': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'alabastro': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'alarma': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'alarmante': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'alba': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " '√°lbum': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alquimia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alcohol': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alcoholismo': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'hueco': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'alerta': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'vigilancia': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'alertas': [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'alfalfa': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " '√°lgebra': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'algebraico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'algoritmo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'coartada': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'extraterrestre': [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'enajenar': [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alienado': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'alienaci√≥n': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'posarse': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alineado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alineaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'similar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alimentaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'pensi√≥n alimenticia': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'al√≠cuota': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'viva': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " '√°lcali': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alcaloides': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aliviar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'alegaci√≥n': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alegar': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'presunto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'lealtad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'aleg√≥rico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alegor√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alegro': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'alivio': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'callej√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'Alianza': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'aliado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'caim√°n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asignar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asignaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'tolerancia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'permiti√≥': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aleaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'seducir': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'seductor': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alusi√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aluvial': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'almanaque': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'todopoderoso': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'en alto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aloha': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'junto a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'a distancia': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'en voz alta': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alfabeto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alfab√©tico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'altar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alterar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'modificaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'altercado': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alterado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alterno': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alternativa': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'altitud': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'Alto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'en total': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'antiguo alumno': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alveolar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'amalgama': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'amalgamaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aficionado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asombro': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0],\n",
       " 'Asombrosamente': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0],\n",
       " 'embajador': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " '√°mbar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ambiente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ambig√ºedad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ambiguo': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " '√°mbito': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ambici√≥n': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'ambicioso': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'ambulancia': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'emboscada': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'mejorar': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'am√©n': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'd√≥cil': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'enmendar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'enmienda': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'compensaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'amenidad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'amatista': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'amable': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'amistoso': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'entre': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'en medio de': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'amon√≠aco': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'munici√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'amnesia': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'amnist√≠a': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'amorfo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'amortizaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'Monto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aventura amorosa': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'ampersand': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anfetaminas': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anfibio': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anfiteatro': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'amplificaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'amplificar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'amplitud': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ampliamente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'amputaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'amuleto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'entretener': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'divertido': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'diversi√≥n': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'entretenido': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'Ana': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anaconda': [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anestesia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anest√©sico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anal': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'analg√©sico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'an√°logo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 't√©rmino an√°logo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'analog√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'an√°lisis': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'analista': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'anal√≠tico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'analizar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'analizador': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anarquismo': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anarquista': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anarqu√≠a': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anastomosis': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anatema': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'anat√≥mico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anatom√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'antepasado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ancestral': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'ascendencia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'ancla': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'anclaje': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0],\n",
       " 'antiguo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'andr√≥gino': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'an√©mona': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " '√°ngel': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0],\n",
       " 'angelical': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'enfado': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'angina de pecho': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'angiograf√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " '√°ngulo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'pesca con ca√±a': [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'enfadado': [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'angustia': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'angular': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anhidro': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'animal': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'animar': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0],\n",
       " 'animado': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'animaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'animosidad': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " '√°nimo': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'tobillo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anales': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anexo': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anexi√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aniquilar': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aniquilado': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'aniquilaci√≥n': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'anotar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anotaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anuncio': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'enojarse': [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'molestia': [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'irritante': [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anualidad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anular': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anulaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'anillo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'unci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'an√≥malo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anomal√≠a': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'luego': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'an√≥nimo': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'responder': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'respondiendo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'hormiga': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'antagonismo': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'antagonista': [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'apuesta inicial': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'antecedente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ant√≠lope': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'antena': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anterior': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'himno': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0],\n",
       " 'antolog√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " '√°ntrax': [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'antropolog√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'antibi√≥tico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'antibi√≥ticos': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'antecristo': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anticipaci√≥n': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anticipado': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ant√≠doto': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'antif√∫ngico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'antimonio': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'antipat√≠a': [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anticuario': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anticuado': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'antig√ºedad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'antis√©ptico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'antisocial': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'ant√≠tesis': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'antit√©tico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'antiv√≠rico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'cuerno': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'yunque': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ansiedad': [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'ansioso': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0],\n",
       " 'aorta': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'r√°pidamente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'apache': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'Departamento': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ap√°tico': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'apat√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'mono': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abertura': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " '√°fido': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'una pieza': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aplomo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'apocal√≠ptico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'apolog√©tico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'apologista': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'disculparse': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0],\n",
       " 'disculpa': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'apostas√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ap√≥stata': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ap√≥stol': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'apost√≥lico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'ap√≥strofe': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'p√©simo': [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aparato': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'vestir': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'aparentemente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aparici√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'apelaci√≥n': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'apariencia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'apaciguar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'apelante': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'adjuntar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'apendicitis': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'apetito': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aperitivo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aplausos': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'manzana': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'accesorios': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aplicabilidad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aplicable': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'solicitante': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'solicitud': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aplicar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'nombrar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'cita': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'equipo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'prorratear': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'prorrateo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'evaluar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'apreciable': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'apreciaci√≥n': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'aprehender': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'detenci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'aprensivo': [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aprendiz': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aprendizaje': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'Acercarse': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'que se acerca': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aprobaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'apropiaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aprobar': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'aprobatorio': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'aproximado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aproximadamente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aproximando': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aproximaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'anexidades': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'delantal': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'apto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'aptitud': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'aguamarina': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acuario': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acu√°tico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acueducto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'acuoso': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'cultivable': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " '√°rbitro': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'arbitrar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arbitraje': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'cenador': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arco': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arcada': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arqueol√≥gico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arque√≥logo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arqueolog√≠a': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'arcaico': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arzobispo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arqueado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arquero': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'tiro al arco': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arquetipo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'archipi√©lago': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arquitecto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arquitectura': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'archivo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " '√°rtico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ardiente': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'ardor': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'arduo': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " '√°rea': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arena': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'areola': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arg√©n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'discutir': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'argumento': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'argumentaci√≥n': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'discutidor': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'argumentos': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " '√°rido': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'aristocracia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'arist√≥crata': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aristocr√°tico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'aritm√©tica': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arca': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'brazo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'armada': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'armamento': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'armadura': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'armado': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'blindado': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arsenal': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'brazos': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ej√©rcito': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aroma': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'excitaci√≥n': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0],\n",
       " 'despertar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arreglar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'organizado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'arreglo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'formaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atrasos': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arrestar': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'llegada': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'llegar': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arrogancia': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'arrogante': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'flecha': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ars√©nico': [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'incendio provocado': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'Arte': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0],\n",
       " 'arter√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ingenioso': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'artr√≥podo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alcachofa': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'art√≠culo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'art√≠culos': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'articular': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'articulaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'artificio': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'artiller√≠a': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'artesano': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'artista': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'art√≠stico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'artistas': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ascender': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ascensi√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ascenso': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'cerciorarse': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'averiguado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asc√©tico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ceniza': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'avergonzado': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'cenizas': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'dormido': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " '√°spid': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aspartamo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aspecto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aspectos': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asfalto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aspiraci√≥n': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0],\n",
       " 'aspirar': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'culo': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'atacar': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'asesino': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0],\n",
       " 'asesinar': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asesinato': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0],\n",
       " 'ensayo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'conjunto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'armar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ensamblado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asamblea': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'asentir': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'afirmando': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'evaluaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0],\n",
       " 'activos': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'Est√∫pido': [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'cesionario': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'asimilar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asimilaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asistir': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'asistencia': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asistente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'asociaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'clasificado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'surtido': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'calmar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'ficticio': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asumiendo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'suposici√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'garant√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'asegurar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'seguro': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'ciertamente': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'asterisco': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asteroides': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'astigmatismo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asombrosamente': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'astral': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'por mal camino': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'astringente': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'astr√≥logo': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'astrolog√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'astronauta': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'astr√≥nomo': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'astronom√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'astuto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'en pedazos': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asilo': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asim√©trico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asimetr√≠a': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'asint√≥tico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'taller': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ate√≠smo': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ateo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aterosclerosis': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'atleta': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'atl√©tico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'atletismo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atlas': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atm√≥sfera': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atmosf√©rico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atol√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " '√°tomo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'at√≥mico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'expiar': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'expiaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'atrio': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atroz': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atrocidad': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'atrofia': [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'archivo adjunto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'ataque': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alcanzar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alcanzable': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'intentar': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atenci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atento': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'atenuar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atenuado': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atenuaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'dar fe': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'atestaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " '√°tico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atuendo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'actitud': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'abogado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atracci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'atractivo': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0],\n",
       " 'atribuible': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atributo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'atribuci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'desgaste': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'casta√±o': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'subasta': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'subastador': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'audaz': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'audacia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'audible': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'audiencia': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'auditor√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'audici√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'auditor': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'sala': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'auditivo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'algo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aumentar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aumento': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'agosto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 't√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'aura': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'Aurora': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'auspicios': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'propicio': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'austero': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'austeridad': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aut√©ntico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'autenticar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'autenticaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'autenticidad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'autor': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'autoritario': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'autoridad': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'autorizaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'autorizar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'paternidad literaria': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'auto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'autobiograf√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'autocr√°tico': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aut√≥grafo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'autom√°tico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'autom√≥vil': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'autonom√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'autopsia': [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'oto√±o': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'aprovechar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'avalancha': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0],\n",
       " 'avaricia': [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'avatar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'vengador': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'avenida': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'promedio': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'contrario': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aversi√≥n': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'evitar': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'pajarera': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'aviaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " '√°vido': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'palta': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'evitaci√≥n': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'evitando': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'esperar': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0],\n",
       " 'despierto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'otorgar': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'temor': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'horrible': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'torpeza': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'toldo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'torcido': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'axial': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'axioma': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'axiom√°tico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'eje': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 's√≠': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'azimut': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'azur': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'balbucear': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'balbuceo': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'babuino': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'beb√©': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'ni√±era': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'bachillerato': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'bacar√°': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'soltero': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'espalda': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'columna vertebral': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'partidario': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'petardeo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'chaquete': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'antecedentes': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'retroexcavadora': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mochilero': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'retractarse': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'hacia atr√°s': [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'remanso': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'bacterias': [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'bacteria': [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'malo': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'Insignia': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'tej√≥n': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'gravemente': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'maldad': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'deflector': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'bolsa': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'equipaje': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'holgado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'gaita': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'fianza': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'alguacil': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'carnada': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'hornear': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'panader√≠a': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'horneando': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'bola': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'balance': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'equilibrado': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'balc√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'calvo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'bala': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'obst√°culo': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'pelota': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'balada': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'lastre': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ballet': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'globo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'votaci√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'sal√≥n de baile': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'b√°lsamo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'bals√°mico': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'prohibici√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'pl√°tano': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'banda': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'vendaje': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'bandido': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'carro': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'perdici√≥n': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'estallido': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'petardo': [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
       " 'desterrar': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'alejado': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'destierro': [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'banjo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'banco': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'banquero': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " 'arruinado': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'bancarrota': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'bandera': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'banquete': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'alma en pena': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'broma': [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'bautismo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'bautismal': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'bar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'leng√ºeta': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'b√°rbaro': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'barbarie': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'parilla': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mordaz': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'bardo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'a pelo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'descalzo': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'apenas': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'vomitar': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'negociar': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " 'barcaza': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'bar√≠tono': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'ladrar': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'granero': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'barney': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'bar√≥metro': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'bar√≥n': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'barroco': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'barraca': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'prohibido': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'barril': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'est√©ril': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'barricada': [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'barrera': [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'excepto': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'carretilla': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'barman': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " ...}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emolex_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* enmascaramos cada palabra del diccionario con su emocion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "10. Eval√∫a t√∫ BoE clasificando con SVM. Ponga una tabla comparativa a modo de resumen con los tres pesados, normalize cada uno si lo cree conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4.5 color='lightblue'>\n",
    "\n",
    "$\\textit{Ejercicio 3. Recurso L√≠nguistico de Emociones Mexicano}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "11. Utiliceelrecursol√©xicollamado\"SpanishEmotionLexicon(SEL)\"delDr.GrigoriSidorov, profesor del Centro de Investigaci√≥n en Computaci√≥n (CIC) del Instituto Polit√©cnico Nacional (http://www.cic.ipn.mx/‚àºsidorov/), para enmascarar cada palabra con su emo- ci√≥n, y despu√©s construir la Bolsa de Emociones con alg√∫n pesado (e.g., binario, tf, tfidf). Proponga alguna estrategia para incorporar el \"valor\" del \"Probability Factor of Affective use\" en su representaci√≥n vectorial del documento. Eval√∫a y escribe una tabla compara- tiva a modo de resumen con al menos tres pesados: binario, frecuencia, tfidf. Normalize cada pesado seg√∫n lo crea conveniente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "12. En un comentario aparte, discuta sobre la estrateg√≠a que utiliz√≥ para incorporar el \"Probability Factor of Affective use\". No m√°s de 5 renglones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4.5 color='lightblue'>\n",
    "\n",
    "$\\textit{Ejercicio 4. ¬øPodemos mejorar con Bigramas?}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "13. Hacer un experimento d√≥nde concatene una buena BoW seg√∫n sus experimentos anteri- ores con otra BoW construida a partir de los 1000 bigramas m√°s frecuentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "14. Hacer un experimento con las Bolsas de Emociones, Bolsa de Palabras y Bolsa de Bi- gramas; usted elige las dimensionalidades. Para construir la representaci√≥n final del documento utilice la concatenaci√≥n de las representaciones seg√∫n sus observaciones (e.g., Bolsa de Palabras + Bolsa de Bigramas + Bolsa de Sentimientos de Canad√° + Bolsa de Sentimientos de Grigori), y alim√©ntelas a un SVM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "15. Elabore conclusiones sobre toda esta Tarea, incluyendo observaciones, comentarios y posibles mejoras futuras. Discuta el comportamiento de la BoW de usar solo palabras a integrar bigramas, y luego a integrar todo ¬øayud√≥? o ¬øempeor√≥?. Discuta tambi√©n brevemente el costo computacional de los experimentos ¬øVali√≥ la Pena tener todo?. Sea breve: todo en NO m√°s de dos p√°rrafos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
