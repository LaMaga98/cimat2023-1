{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{PLN. Tarea 2: Miner√≠a de texto b√°sica}$$\n",
    "$$\\textit{Y. Sarahi Garc√≠a Goz√°lez}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import Counter\n",
    "import matplotlib.pylab as plt\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "#clasificacion\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,precision_recall_fscore_support,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarea realizada en MacOs. \n",
      "Las versiones de las librer√≠as y de python utilizadas fueron:\n",
      "\n",
      "Python version 3.11.5\n",
      "Numpy version 1.23.5\n",
      "NLTK version 3.8.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Tarea realizada en MacOs. \\nLas versiones de las librer√≠as y de python utilizadas fueron:\\n\")\n",
    "from platform import python_version\n",
    "print(\"Python version\", python_version())\n",
    "print(\"Numpy version\", np.__version__)\n",
    "print(\"NLTK version\", nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4.5 color='lightblue'>\n",
    "\n",
    "$\\textit{Ejercicio 2. Bolsas de Palabras, Bigramas y Emociones}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Preparaci√≥n de texto, corpus y diccionarios}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_text_from_file(path_corpus,path_truth):\n",
    "\n",
    "    tr_txt=[]\n",
    "    tr_labels=[]\n",
    "\n",
    "    with open(path_corpus, \"r\") as f_corpus,open(path_truth, \"r\") as f_truth:\n",
    "        for tweet in f_corpus:\n",
    "            tr_txt += [tweet]\n",
    "        for label in f_truth:\n",
    "            tr_labels += [label]   \n",
    "             \n",
    "    return tr_txt, tr_labels\n",
    "\n",
    "\n",
    "def create_corpus_from_text(text,tokenizer):\n",
    "    corpus_palabras=[]\n",
    "    for documento in text:\n",
    "        corpus_palabras+=tokenizer.tokenize(documento)\n",
    "\n",
    "    return corpus_palabras\n",
    "\n",
    "\n",
    "def create_dic_freq(corpus,n):\n",
    "    fdist = nltk.FreqDist(corpus)\n",
    "    aux=[(fdist[key],key) for key in fdist]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    aux=aux[:n]\n",
    "\n",
    "    return aux\n",
    "\n",
    "def create_dic_ranking(dic_freq):\n",
    "    dict_indices=dict()\n",
    "    cont = 0\n",
    "    for weight, word in dic_freq:\n",
    "        dict_indices[word]= cont\n",
    "        cont+= 1\n",
    "\n",
    "    return dict_indices\n",
    "\n",
    "#Clasificador\n",
    "\n",
    "parameters={'C':[0.05,0.12,0.25,0.5,1,2,4]} #par√°metro de complejidad\n",
    "\n",
    "#llamamos el clasificador SVM\n",
    "#nota: cambi√© el max_iter y dual porque me aparec√≠an una serie de warnings pero no cambiaron los resultados\n",
    "svr=svm.LinearSVC(class_weight='balanced',dual='auto',max_iter=3000) \n",
    "\n",
    "#grid search recide el objeto de clasificacion\n",
    "grid=GridSearchCV(estimator=svr,param_grid=parameters,n_jobs=8,scoring=\"f1_macro\",cv=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardamos los textos de entrenamiento y validaci√≥n\n",
    "tr_txt,tr_labels=get_text_from_file(\"/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/practicas/03_practica/mex20_train.txt\",\"/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/practicas/03_practica/mex20_train_labels.txt\")\n",
    "val_txt,val_labels=get_text_from_file(\"/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/practicas/03_practica/mex20_val.txt\",\"/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/practicas/03_practica/mex20_val_labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connvertimos a lista las etiquetas\n",
    "tr_labels=list(map(int,tr_labels))\n",
    "val_labels=list(map(int,val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizamos\n",
    "tokenizer=TweetTokenizer()\n",
    "#Generamos el corpus\n",
    "corpus_palabras=create_corpus_from_text(tr_txt,tokenizer)\n",
    "#Generamos diccionario de frecuencias con las primeras 5 mil palabras\n",
    "dict_freq=create_dic_freq(corpus_palabras,5000)\n",
    "#Generamos diccionario de indices\n",
    "dict_indices=create_dic_ranking(dict_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me - 10\n",
      "el - 11\n",
      "en - 12\n",
      "se - 13\n",
      "es - 14\n",
      "con - 15\n",
      "? - 16\n",
      "verga - 17\n",
      "los - 18\n",
      "madre - 19\n",
      "por - 20\n",
      "las - 21\n",
      "\" - 22\n",
      "un - 23\n",
      "te - 24\n",
      "mi - 25\n",
      "lo - 26\n",
      "putas - 27\n",
      "una - 28\n",
      "... - 29\n"
     ]
    }
   ],
   "source": [
    "#imprimimos algunos elementos del diccionario de indices\n",
    "lista_indices = list(dict_indices.items())\n",
    "for clave, valor in lista_indices[10:30]:\n",
    "    print(clave, \"-\", valor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3.5 color='lightblue'>\n",
    "\n",
    "1,2,3. Evalue BoW con pesado binario, de frecuencia y tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Bolsas de palabras con distintos equemas de pesado y sin normalizar}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#________________________________PESADO_BINARIO________________________________#\n",
    "\n",
    "\n",
    "#matriz_nxm de bolsa de palabras n=numero de textos, m=numero de palabras (5000)\n",
    "def build_binary_bow(tr_txt,dic_freq,dic_indices):\n",
    "    '''\n",
    "    Funci√≥n que genera la bolsa de palabras con pesado binario\n",
    "\n",
    "    '''\n",
    "    BoW=np.zeros((len(tr_txt),len(dic_freq)),dtype=int) #construimos bolsta de palabras en ceros\n",
    "    cont_documento=0 #indice que recorre las FILAS\n",
    "\n",
    "    for tr in tr_txt:#recorremos cada documento\n",
    "        fdist_doc=nltk.FreqDist(tokenizer.tokenize(tr))#hacemos freqDist (tokenizado con el tokenizador ya definido) de cada documento\n",
    "        for word in fdist_doc: #para cada palabra en el diccionario del documento\n",
    "            if word in dic_indices: #si la palabra est√° en el corte final\n",
    "                #AGREGAMOS un uno en el lugar correspondiente\n",
    "                BoW[cont_documento,dic_indices[word]] = 1 \n",
    "\n",
    "        cont_documento+=1\n",
    "\n",
    "    return BoW\n",
    "\n",
    "#________________________________PESADO_FRECUENCIA________________________________#\n",
    "\n",
    "#matriz_nxm de bolsa de palabras n=numero de textos, m=numero de palabras (5000)\n",
    "def build_freq_bow(tr_txt,dic_feq,dic_inices):\n",
    "    BoW=np.zeros((len(tr_txt),len(dic_feq)),dtype=int) #construimos bolsta de palabras en ceros\n",
    "    cont_documento=0 #indice que recorre las FILAS\n",
    " \n",
    "    for tr in tr_txt:#recorremos cada documento\n",
    "        fdist_doc=nltk.FreqDist(tokenizer.tokenize(tr))#hacemos freqDist (tokenizado con el tokenizador ya definido) de cada documento\n",
    "        for word in fdist_doc: #para cada palabra en el diccionario del documento\n",
    "            if word in dic_inices: #si la palabra est√° en el corte final\n",
    "                #le asignamos su frecuencia\n",
    "                BoW[cont_documento,dic_inices[word]] = fdist_doc[word]\n",
    "\n",
    "        cont_documento+=1\n",
    "\n",
    "    return BoW\n",
    "\n",
    "#________________________________PESADO_TFIDF________________________________#\n",
    "\n",
    "#primero calculamos el num de documentos en que aparece cada palabra\n",
    "def df(tr_txt, dic_indices):\n",
    "    df = np.zeros(5000, dtype=int)\n",
    "\n",
    "    # comp list para las frecuencias de cada documento\n",
    "    fdist_docs = [nltk.FreqDist(tokenizer.tokenize(tr)) for tr in tr_txt]\n",
    "\n",
    "    #iteramos sobre cada palbra\n",
    "    for word in dic_indices:\n",
    "        #obtenemos el indice de la palabra\n",
    "        index = dic_indices[word] \n",
    "        for j,fdist_doc in enumerate(fdist_docs): #iteramos osbre cada documento\n",
    "            if word in fdist_doc: #si la palabra est√° en el documento actual\n",
    "                df[index] += 1 #agredamos uno al conador de esa palabra\n",
    "\n",
    "    return df\n",
    "\n",
    "#\n",
    "def build_tfidf_bow(tr_txt, dic_freq, dic_indices):\n",
    "    BoW = build_freq_bow(tr_txt, dic_freq, dic_indices) #la bolsa de palabras de frecuencias es el tf\n",
    "    df_list = df(tr_txt, dic_indices) #llamamos a la duncion df\n",
    "    N = len(tr_txt)\n",
    "   \n",
    "    for i in range(N):\n",
    "        for j,df_val in enumerate(df_list): \n",
    "            if df_val!=0: #si df es distinto de ceo (puede ser cero para el conjunto de validacion, por ejemplo que contiene menos lementos )\n",
    "                BoW[i][j] *= np.log(N / df_val) #tfxdf\n",
    "\n",
    "    return BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 0,\n",
       " 'de': 1,\n",
       " 'que': 2,\n",
       " '.': 3,\n",
       " 'la': 4,\n",
       " 'a': 5,\n",
       " 'y': 6,\n",
       " '!': 7,\n",
       " 'no': 8,\n",
       " '@USUARIO': 9,\n",
       " 'me': 10,\n",
       " 'el': 11,\n",
       " 'en': 12,\n",
       " 'se': 13,\n",
       " 'es': 14,\n",
       " 'con': 15,\n",
       " '?': 16,\n",
       " 'verga': 17,\n",
       " 'los': 18,\n",
       " 'madre': 19,\n",
       " 'por': 20,\n",
       " 'las': 21,\n",
       " '\"': 22,\n",
       " 'un': 23,\n",
       " 'te': 24,\n",
       " 'mi': 25,\n",
       " 'lo': 26,\n",
       " 'putas': 27,\n",
       " 'una': 28,\n",
       " '...': 29,\n",
       " 'putos': 30,\n",
       " 'para': 31,\n",
       " 'üòÇ': 32,\n",
       " 'si': 33,\n",
       " 'ya': 34,\n",
       " 'como': 35,\n",
       " 'su': 36,\n",
       " 'pero': 37,\n",
       " 'tu': 38,\n",
       " 'loca': 39,\n",
       " 'le': 40,\n",
       " 'm√°s': 41,\n",
       " 'No': 42,\n",
       " 'del': 43,\n",
       " 'gorda': 44,\n",
       " 'al': 45,\n",
       " 'bien': 46,\n",
       " 'A': 47,\n",
       " '¬ø': 48,\n",
       " 'Y': 49,\n",
       " 'son': 50,\n",
       " 'Me': 51,\n",
       " 'o': 52,\n",
       " 'feas': 53,\n",
       " 'cuando': 54,\n",
       " 'Que': 55,\n",
       " ':': 56,\n",
       " 'yo': 57,\n",
       " 'les': 58,\n",
       " 'porque': 59,\n",
       " 'ni': 60,\n",
       " 'est√°': 61,\n",
       " 'ser': 62,\n",
       " 'estoy': 63,\n",
       " 'sus': 64,\n",
       " 'todos': 65,\n",
       " 'esta': 66,\n",
       " 'puta': 67,\n",
       " 'Ya': 68,\n",
       " 'todo': 69,\n",
       " 'pinche': 70,\n",
       " 'puto': 71,\n",
       " 'tan': 72,\n",
       " 'Si': 73,\n",
       " 'La': 74,\n",
       " 'qu√©': 75,\n",
       " '‚Ä¶': 76,\n",
       " 'eso': 77,\n",
       " 'muy': 78,\n",
       " 'soy': 79,\n",
       " 'hasta': 80,\n",
       " 'as√≠': 81,\n",
       " '¬°': 82,\n",
       " '<URL>': 83,\n",
       " 'mamar': 84,\n",
       " 'hay': 85,\n",
       " 'q': 86,\n",
       " 'DE': 87,\n",
       " 'mis': 88,\n",
       " 'joto': 89,\n",
       " 'hace': 90,\n",
       " 'este': 91,\n",
       " 'cosas': 92,\n",
       " 'Ô∏è': 93,\n",
       " 'vida': 94,\n",
       " 'nos': 95,\n",
       " 'ver': 96,\n",
       " 'mejor': 97,\n",
       " 'solo': 98,\n",
       " 'nada': 99,\n",
       " 'vale': 100,\n",
       " 'va': 101,\n",
       " 'quiero': 102,\n",
       " 'marica': 103,\n",
       " 'eres': 104,\n",
       " 'd√≠a': 105,\n",
       " 'siempre': 106,\n",
       " 'esa': 107,\n",
       " 'voy': 108,\n",
       " 'gente': 109,\n",
       " 'Yo': 110,\n",
       " 'üò≠': 111,\n",
       " 'vez': 112,\n",
       " 'El': 113,\n",
       " 'mierda': 114,\n",
       " '-': 115,\n",
       " 'tengo': 116,\n",
       " '(': 117,\n",
       " 'sin': 118,\n",
       " 'ese': 119,\n",
       " ')': 120,\n",
       " 'Es': 121,\n",
       " 'luchona': 122,\n",
       " 'üòç': 123,\n",
       " 'hdp': 124,\n",
       " 'ahora': 125,\n",
       " 'Por': 126,\n",
       " 'üò°': 127,\n",
       " '‚Äú': 128,\n",
       " 'tienen': 129,\n",
       " 'tiene': 130,\n",
       " 'pinches': 131,\n",
       " 'hacer': 132,\n",
       " 'tus': 133,\n",
       " 'tontas': 134,\n",
       " 'LA': 135,\n",
       " '‚Äù': 136,\n",
       " 'gusta': 137,\n",
       " 'Como': 138,\n",
       " 'sea': 139,\n",
       " 'HDP': 140,\n",
       " 'toda': 141,\n",
       " 'Se': 142,\n",
       " 'hoy': 143,\n",
       " 'Qu√©': 144,\n",
       " 'mamando': 145,\n",
       " 'est√°n': 146,\n",
       " 'cagado': 147,\n",
       " 'tonta': 148,\n",
       " 'Pero': 149,\n",
       " 'puedo': 150,\n",
       " 'mas': 151,\n",
       " 'üôÑ': 152,\n",
       " 'pendejo': 153,\n",
       " 'hijo': 154,\n",
       " 'NO': 155,\n",
       " 'En': 156,\n",
       " 'Mi': 157,\n",
       " 'mal': 158,\n",
       " 'estar': 159,\n",
       " 'QUE': 160,\n",
       " '..': 161,\n",
       " 'Lo': 162,\n",
       " 'algo': 163,\n",
       " 'PUTOS': 164,\n",
       " 'tener': 165,\n",
       " 'alguien': 166,\n",
       " 'Putos': 167,\n",
       " 'verdad': 168,\n",
       " 'mujer': 169,\n",
       " 'cabrona': 170,\n",
       " 'tambi√©n': 171,\n",
       " 'da': 172,\n",
       " 'puede': 173,\n",
       " 'decir': 174,\n",
       " 'madres': 175,\n",
       " 'mujeres': 176,\n",
       " 'maricon': 177,\n",
       " 'vas': 178,\n",
       " 'mucho': 179,\n",
       " 'dos': 180,\n",
       " 'MADRE': 181,\n",
       " '‚ù§': 182,\n",
       " 'van': 183,\n",
       " 's√©': 184,\n",
       " 'Estoy': 185,\n",
       " 'Cuando': 186,\n",
       " ';': 187,\n",
       " 's√≠': 188,\n",
       " 'otra': 189,\n",
       " 'est√°s': 190,\n",
       " 'a√±os': 191,\n",
       " 'Verga': 192,\n",
       " 'PUTAS': 193,\n",
       " 'Las': 194,\n",
       " 'ir': 195,\n",
       " 'chingada': 196,\n",
       " \"'\": 197,\n",
       " 'veces': 198,\n",
       " 't√∫': 199,\n",
       " 'hijos': 200,\n",
       " 'De': 201,\n",
       " 'quiere': 202,\n",
       " 'quien': 203,\n",
       " 'pues': 204,\n",
       " 'jajaja': 205,\n",
       " 'VERGA': 206,\n",
       " 'Te': 207,\n",
       " '3': 208,\n",
       " 'mundo': 209,\n",
       " 'menos': 210,\n",
       " '2': 211,\n",
       " 'ü§î': 212,\n",
       " 'uno': 213,\n",
       " 'nunca': 214,\n",
       " 'era': 215,\n",
       " 'cada': 216,\n",
       " 'd√≠as': 217,\n",
       " 'dice': 218,\n",
       " 'M√©xico': 219,\n",
       " 'todas': 220,\n",
       " 'esos': 221,\n",
       " 'amor': 222,\n",
       " 'Una': 223,\n",
       " 'tanto': 224,\n",
       " 'mam√°': 225,\n",
       " 'ganas': 226,\n",
       " 'esas': 227,\n",
       " 'O': 228,\n",
       " 'üòí': 229,\n",
       " 'tienes': 230,\n",
       " 'tiempo': 231,\n",
       " 'fue': 232,\n",
       " 'cuenta': 233,\n",
       " 'ME': 234,\n",
       " 'Jajajaja': 235,\n",
       " 'pendeja': 236,\n",
       " 'estaba': 237,\n",
       " 'dicen': 238,\n",
       " 'Madre': 239,\n",
       " 'wey': 240,\n",
       " 'pedo': 241,\n",
       " 'neta': 242,\n",
       " 'esto': 243,\n",
       " 'ti': 244,\n",
       " 'siento': 245,\n",
       " 'ma√±ana': 246,\n",
       " 'igual': 247,\n",
       " 'he': 248,\n",
       " 'd': 249,\n",
       " 'camote': 250,\n",
       " 'Los': 251,\n",
       " 'veo': 252,\n",
       " 'personas': 253,\n",
       " 'pasa': 254,\n",
       " 'hacen': 255,\n",
       " 'ha': 256,\n",
       " 'donde': 257,\n",
       " 'digo': 258,\n",
       " 'Hoy': 259,\n",
       " '/': 260,\n",
       " 'v': 261,\n",
       " 'unas': 262,\n",
       " 'bueno': 263,\n",
       " ':(': 264,\n",
       " 'fotos': 265,\n",
       " 'cabr√≥n': 266,\n",
       " 'Ahora': 267,\n",
       " 'otro': 268,\n",
       " 'mismo': 269,\n",
       " 'cabron': 270,\n",
       " 'buena': 271,\n",
       " 'ah√≠': 272,\n",
       " 'trabajo': 273,\n",
       " 'sabe': 274,\n",
       " 'nadie': 275,\n",
       " 'estas': 276,\n",
       " 'desde': 277,\n",
       " 'amigos': 278,\n",
       " 'alv': 279,\n",
       " 'Pinche': 280,\n",
       " 'm√≠': 281,\n",
       " 'hombres': 282,\n",
       " 'foto': 283,\n",
       " 'culo': 284,\n",
       " 'casa': 285,\n",
       " 'ardida': 286,\n",
       " 'Jajaja': 287,\n",
       " '*': 288,\n",
       " 'üòà': 289,\n",
       " 've': 290,\n",
       " 'valer': 291,\n",
       " 'han': 292,\n",
       " 'fuera': 293,\n",
       " 'e': 294,\n",
       " 'chingar': 295,\n",
       " 'Pues': 296,\n",
       " 'Esta': 297,\n",
       " 'pendejos': 298,\n",
       " 'mames': 299,\n",
       " 'ella': 300,\n",
       " 'cara': 301,\n",
       " 'ü§£': 302,\n",
       " 's√≥lo': 303,\n",
       " 'mil': 304,\n",
       " 's√∫per': 305,\n",
       " 'rico': 306,\n",
       " 'noche': 307,\n",
       " 'mundial': 308,\n",
       " 'luego': 309,\n",
       " 'jajajaja': 310,\n",
       " 'fin': 311,\n",
       " 'estos': 312,\n",
       " 'encanta': 313,\n",
       " 'amigo': 314,\n",
       " 'Tu': 315,\n",
       " 'EL': 316,\n",
       " 'üò©': 317,\n",
       " 'üé∂': 318,\n",
       " 'ustedes': 319,\n",
       " 'semana': 320,\n",
       " 'quieren': 321,\n",
       " 'pueden': 322,\n",
       " 'mientras': 323,\n",
       " 'aqu√≠': 324,\n",
       " 'antes': 325,\n",
       " 'dan': 326,\n",
       " 'Para': 327,\n",
       " 'Porque': 328,\n",
       " 'Gracias': 329,\n",
       " 'persona': 330,\n",
       " 'jaja': 331,\n",
       " 'gay': 332,\n",
       " 'dinero': 333,\n",
       " 'dijo': 334,\n",
       " 'creo': 335,\n",
       " 'buen': 336,\n",
       " 'amo': 337,\n",
       " 'YA': 338,\n",
       " 'PUTA': 339,\n",
       " 'ven': 340,\n",
       " 'sabes': 341,\n",
       " 'novio': 342,\n",
       " 'dar': 343,\n",
       " 'creen': 344,\n",
       " 'chingo': 345,\n",
       " 'caga': 346,\n",
       " 'Un': 347,\n",
       " 'Est√°': 348,\n",
       " 'Con': 349,\n",
       " 'As√≠': 350,\n",
       " 'unos': 351,\n",
       " 'putita': 352,\n",
       " 'perra': 353,\n",
       " 'peor': 354,\n",
       " 'gata': 355,\n",
       " 'falta': 356,\n",
       " 'digan': 357,\n",
       " 'deja': 358,\n",
       " 'a√∫n': 359,\n",
       " 'amiga': 360,\n",
       " 'Soy': 361,\n",
       " 'Putas': 362,\n",
       " 'EN': 363,\n",
       " '4': 364,\n",
       " '1': 365,\n",
       " 'üò†': 366,\n",
       " 'triste': 367,\n",
       " 'puedes': 368,\n",
       " 'mamen': 369,\n",
       " 'horas': 370,\n",
       " 'hablar': 371,\n",
       " 'entiendo': 372,\n",
       " 'dejar': 373,\n",
       " 'sean': 374,\n",
       " 'saben': 375,\n",
       " 'qui√©n': 376,\n",
       " 'pelan': 377,\n",
       " 'pa√≠s': 378,\n",
       " 'novia': 379,\n",
       " 'hora': 380,\n",
       " 'haciendo': 381,\n",
       " 'dormir': 382,\n",
       " 'despu√©s': 383,\n",
       " 'andar': 384,\n",
       " 'Ni': 385,\n",
       " '5': 386,\n",
       " 'üá≤üáΩ': 387,\n",
       " 'viendo': 388,\n",
       " 'vamos': 389,\n",
       " 'tarea': 390,\n",
       " 'tal': 391,\n",
       " 'prieta': 392,\n",
       " 'mamadas': 393,\n",
       " 'gusto': 394,\n",
       " 'cosa': 395,\n",
       " 'asi': 396,\n",
       " 'asco': 397,\n",
       " 'Ojal√°': 398,\n",
       " 'Hay': 399,\n",
       " 'üò§': 400,\n",
       " 'üòò': 401,\n",
       " 'vali√≥': 402,\n",
       " 'se√±ora': 403,\n",
       " 'saber': 404,\n",
       " 'rica': 405,\n",
       " 'palabra': 406,\n",
       " 'nalgas': 407,\n",
       " 'maric√≥n': 408,\n",
       " 'hago': 409,\n",
       " 'hab√≠a': 410,\n",
       " 'ellos': 411,\n",
       " 'c√≥mo': 412,\n",
       " 'boca': 413,\n",
       " 'SU': 414,\n",
       " 'LOS': 415,\n",
       " '#MasterChefMx': 416,\n",
       " 'üòè': 417,\n",
       " 'poca': 418,\n",
       " 'perro': 419,\n",
       " 'mandar': 420,\n",
       " 'llorar': 421,\n",
       " 'hombre': 422,\n",
       " 'chingas': 423,\n",
       " 'a√±o': 424,\n",
       " 'Todos': 425,\n",
       " 'Tengo': 426,\n",
       " 'Pinches': 427,\n",
       " 'üòå': 428,\n",
       " 'üíî': 429,\n",
       " 'üçÜ': 430,\n",
       " '√©l': 431,\n",
       " 'vergas': 432,\n",
       " 'sigue': 433,\n",
       " 'risa': 434,\n",
       " 're': 435,\n",
       " 'quieres': 436,\n",
       " 'queda': 437,\n",
       " 'puro': 438,\n",
       " 'ponen': 439,\n",
       " 'pone': 440,\n",
       " 'otras': 441,\n",
       " 'odio': 442,\n",
       " 'misma': 443,\n",
       " 'miedo': 444,\n",
       " 'iba': 445,\n",
       " 'hubiera': 446,\n",
       " 'golfa': 447,\n",
       " 'ex': 448,\n",
       " 'dejen': 449,\n",
       " 'debe': 450,\n",
       " 'bonito': 451,\n",
       " 'PARA': 452,\n",
       " 'siguen': 453,\n",
       " 'pobre': 454,\n",
       " 'parte': 455,\n",
       " 'importa': 456,\n",
       " 'hizo': 457,\n",
       " 'hija': 458,\n",
       " 'feo': 459,\n",
       " 'feliz': 460,\n",
       " 'fea': 461,\n",
       " 'favor': 462,\n",
       " 'culpa': 463,\n",
       " 'Quiero': 464,\n",
       " 'Este': 465,\n",
       " 'Alguien': 466,\n",
       " '10': 467,\n",
       " 'üî•': 468,\n",
       " 'vieja': 469,\n",
       " 'valiendo': 470,\n",
       " 'tarde': 471,\n",
       " 'seguro': 472,\n",
       " 'salir': 473,\n",
       " 'pu√±al': 474,\n",
       " 'poner': 475,\n",
       " 'pensar': 476,\n",
       " 'partido': 477,\n",
       " 'minutos': 478,\n",
       " 'lugar': 479,\n",
       " 'llega': 480,\n",
       " 'diga': 481,\n",
       " 'chinga': 482,\n",
       " 'canci√≥n': 483,\n",
       " 'ando': 484,\n",
       " 'anda': 485,\n",
       " 'RT': 486,\n",
       " 'PUTO': 487,\n",
       " 'Eso': 488,\n",
       " '$': 489,\n",
       " 'üôÉ': 490,\n",
       " 'üòû': 491,\n",
       " 'üòî': 492,\n",
       " 'somos': 493,\n",
       " 'sido': 494,\n",
       " 'pena': 495,\n",
       " 'parece': 496,\n",
       " 'momento': 497,\n",
       " 'mando': 498,\n",
       " 'lameculos': 499,\n",
       " 'huevos': 500,\n",
       " 'hermano': 501,\n",
       " 'familia': 502,\n",
       " 'entre': 503,\n",
       " 'contigo': 504,\n",
       " 'bonita': 505,\n",
       " 'agua': 506,\n",
       " 'acabo': 507,\n",
       " 'Siempre': 508,\n",
       " 'Neta': 509,\n",
       " 'Les': 510,\n",
       " 'Le': 511,\n",
       " 'Hasta': 512,\n",
       " 'ES': 513,\n",
       " 'ü§¶üèª\\u200d‚ôÄ': 514,\n",
       " 'üòé': 515,\n",
       " 'üí¶': 516,\n",
       " '|': 517,\n",
       " 'volver': 518,\n",
       " 'visto': 519,\n",
       " 'viejas': 520,\n",
       " 'ves': 521,\n",
       " 'valen': 522,\n",
       " 'sobre': 523,\n",
       " 'servicio': 524,\n",
       " 'seas': 525,\n",
       " 'sale': 526,\n",
       " 'primera': 527,\n",
       " 'pongo': 528,\n",
       " 'poder': 529,\n",
       " 'perros': 530,\n",
       " 'pasan': 531,\n",
       " 'pasado': 532,\n",
       " 'nuevo': 533,\n",
       " 'mama': 534,\n",
       " 'lado': 535,\n",
       " 'fui': 536,\n",
       " 'forma': 537,\n",
       " 'escuela': 538,\n",
       " 'escuchar': 539,\n",
       " 'dije': 540,\n",
       " 'dicho': 541,\n",
       " 'dices': 542,\n",
       " 'conmigo': 543,\n",
       " 'Twitter': 544,\n",
       " 'TU': 545,\n",
       " 'Solo': 546,\n",
       " 'SE': 547,\n",
       " 'Jajajajaja': 548,\n",
       " 'Gorda': 549,\n",
       " 'Dios': 550,\n",
       " 'C√≥mo': 551,\n",
       " 'Creo': 552,\n",
       " 'Ay': 553,\n",
       " 'Aqu√≠': 554,\n",
       " 'Ah': 555,\n",
       " '7': 556,\n",
       " '20': 557,\n",
       " 'üò±': 558,\n",
       " 'vista': 559,\n",
       " 'video': 560,\n",
       " 'valgo': 561,\n",
       " 'tanta': 562,\n",
       " 'sigo': 563,\n",
       " 'ser√°': 564,\n",
       " 'rato': 565,\n",
       " 'problema': 566,\n",
       " 'pa': 567,\n",
       " 'otros': 568,\n",
       " 'm√∫sica': 569,\n",
       " 'llevo': 570,\n",
       " 'lleva': 571,\n",
       " 'jajajajaja': 572,\n",
       " 'hecho': 573,\n",
       " 'haga': 574,\n",
       " 'haber': 575,\n",
       " 'grande': 576,\n",
       " 'gracias': 577,\n",
       " 'entonces': 578,\n",
       " 'doy': 579,\n",
       " 'diciendo': 580,\n",
       " 'chiflar': 581,\n",
       " 'cagan': 582,\n",
       " 'buenas': 583,\n",
       " 'andan': 584,\n",
       " 'amigas': 585,\n",
       " 'V': 586,\n",
       " 'Su': 587,\n",
       " 'POR': 588,\n",
       " 'PINCHE': 589,\n",
       " 'LOCA': 590,\n",
       " 'JAJAJA': 591,\n",
       " 'üôä': 592,\n",
       " 'üò£': 593,\n",
       " '‚òπ': 594,\n",
       " '√∫nico': 595,\n",
       " 'xD': 596,\n",
       " 'vi': 597,\n",
       " 'tres': 598,\n",
       " 'trabajar': 599,\n",
       " 'siendo': 600,\n",
       " 'seguir': 601,\n",
       " 'quiera': 602,\n",
       " 'quer√≠a': 603,\n",
       " 'primero': 604,\n",
       " 'pap√°': 605,\n",
       " 'padre': 606,\n",
       " 'ojal√°': 607,\n",
       " 'ni√±o': 608,\n",
       " 'ni√±as': 609,\n",
       " 'ni√±a': 610,\n",
       " 'meter': 611,\n",
       " 'hablando': 612,\n",
       " 'final': 613,\n",
       " 'estamos': 614,\n",
       " 'dem√°s': 615,\n",
       " 'das': 616,\n",
       " 'comer': 617,\n",
       " 'clase': 618,\n",
       " 'caso': 619,\n",
       " 'casi': 620,\n",
       " 'UN': 621,\n",
       " 'TE': 622,\n",
       " 'Mis': 623,\n",
       " 'Marica': 624,\n",
       " 'LO': 625,\n",
       " 'LAS': 626,\n",
       " 'Eres': 627,\n",
       " 'COMO': 628,\n",
       " 'üò¨': 629,\n",
       " 'üò¢': 630,\n",
       " 'üòä': 631,\n",
       " 'üòÅ': 632,\n",
       " 'viejo': 633,\n",
       " 'tipo': 634,\n",
       " 'sola': 635,\n",
       " 'respeto': 636,\n",
       " 'raz√≥n': 637,\n",
       " 'pasar': 638,\n",
       " 'palabras': 639,\n",
       " 'pagar': 640,\n",
       " 'nombre': 641,\n",
       " 'noches': 642,\n",
       " 'medio': 643,\n",
       " 'mano': 644,\n",
       " 'llama': 645,\n",
       " 'huevo': 646,\n",
       " 'hambre': 647,\n",
       " 'haces': 648,\n",
       " 'esperando': 649,\n",
       " 'equipo': 650,\n",
       " 'cualquier': 651,\n",
       " 'creer': 652,\n",
       " 'chinguen': 653,\n",
       " 'chica': 654,\n",
       " 'celular': 655,\n",
       " 'calle': 656,\n",
       " 'aunque': 657,\n",
       " 'ardidas': 658,\n",
       " 'S√≠': 659,\n",
       " 'Quien': 660,\n",
       " 'Puto': 661,\n",
       " 'Mira': 662,\n",
       " 'Ma√±ana': 663,\n",
       " 'Esa': 664,\n",
       " 'Al': 665,\n",
       " '6': 666,\n",
       " '&': 667,\n",
       " 'ü§§': 668,\n",
       " 'üòú': 669,\n",
       " 'vos': 670,\n",
       " 'videos': 671,\n",
       " 'vaya': 672,\n",
       " 'ten√≠a': 673,\n",
       " 'siente': 674,\n",
       " 'punto': 675,\n",
       " 'porqu√©': 676,\n",
       " 'poco': 677,\n",
       " 'paso': 678,\n",
       " 'partir': 679,\n",
       " 'parecen': 680,\n",
       " 'ojos': 681,\n",
       " 'nosotros': 682,\n",
       " 'maldito': 683,\n",
       " 'lluvia': 684,\n",
       " 'hice': 685,\n",
       " 'gordas': 686,\n",
       " 'estan': 687,\n",
       " 'estado': 688,\n",
       " 'eran': 689,\n",
       " 'ellas': 690,\n",
       " 'd√≥nde': 691,\n",
       " 'den': 692,\n",
       " 'deber√≠an': 693,\n",
       " 'culero': 694,\n",
       " 'coraz√≥n': 695,\n",
       " 'clases': 696,\n",
       " 'cargo': 697,\n",
       " 'cae': 698,\n",
       " 'ayer': 699,\n",
       " 'adem√°s': 700,\n",
       " 'Todo': 701,\n",
       " 'Teresa': 702,\n",
       " 'TODOS': 703,\n",
       " 'SI': 704,\n",
       " 'Puta': 705,\n",
       " 'Jajajajajaja': 706,\n",
       " 'HIJO': 707,\n",
       " 'CON': 708,\n",
       " '@': 709,\n",
       " 'üò™': 710,\n",
       " 'üòï': 711,\n",
       " 'üòã': 712,\n",
       " 'x': 713,\n",
       " 'vuelve': 714,\n",
       " 'viene': 715,\n",
       " 'usar': 716,\n",
       " 'tengan': 717,\n",
       " 'tenemos': 718,\n",
       " 'tambien': 719,\n",
       " 'tacos': 720,\n",
       " 'sue√±o': 721,\n",
       " 'serio': 722,\n",
       " 'salen': 723,\n",
       " 'sabemos': 724,\n",
       " 'pura': 725,\n",
       " 'pueblo': 726,\n",
       " 'poniendo': 727,\n",
       " 'pol√≠ticos': 728,\n",
       " 'pienso': 729,\n",
       " 'periodistas': 730,\n",
       " 'pendejas': 731,\n",
       " 'nivel': 732,\n",
       " 'meses': 733,\n",
       " 'mes': 734,\n",
       " 'mariquita': 735,\n",
       " 'lleno': 736,\n",
       " 'llegar': 737,\n",
       " 'juego': 738,\n",
       " 'historia': 739,\n",
       " 'hermoso': 740,\n",
       " 'hagan': 741,\n",
       " 'grupo': 742,\n",
       " 'fan': 743,\n",
       " 'espero': 744,\n",
       " 'escribir': 745,\n",
       " 'dio': 746,\n",
       " 'demasiado': 747,\n",
       " 'deje': 748,\n",
       " 'dado': 749,\n",
       " 'contra': 750,\n",
       " 'arruga': 751,\n",
       " 'Wey': 752,\n",
       " 'Son': 753,\n",
       " 'Qui√©n': 754,\n",
       " 'HIJOS': 755,\n",
       " 'Chinga': 756,\n",
       " 'ALV': 757,\n",
       " 'üôà': 758,\n",
       " 'üòª': 759,\n",
       " 'üòÖ': 760,\n",
       " 'üíï': 761,\n",
       " 'volviendo': 762,\n",
       " 'vato': 763,\n",
       " 'tuits': 764,\n",
       " 'tenga': 765,\n",
       " 'sali√≥': 766,\n",
       " 'putito': 767,\n",
       " 'primer': 768,\n",
       " 'pones': 769,\n",
       " 'pesos': 770,\n",
       " 'pela': 771,\n",
       " 'partidos': 772,\n",
       " 'nuestros': 773,\n",
       " 'nuestro': 774,\n",
       " 'necesito': 775,\n",
       " 'muchas': 776,\n",
       " 'morra': 777,\n",
       " 'moral': 778,\n",
       " 'mayor': 779,\n",
       " 'matar': 780,\n",
       " 'manos': 781,\n",
       " 'mamo': 782,\n",
       " 'mamaste': 783,\n",
       " 'maldita': 784,\n",
       " 'mala': 785,\n",
       " 'leche': 786,\n",
       " 'lados': 787,\n",
       " 'hermosa': 788,\n",
       " 'has': 789,\n",
       " 'gran': 790,\n",
       " 'golfas': 791,\n",
       " 'f√∫tbol': 792,\n",
       " 'fueron': 793,\n",
       " 'fr√≠o': 794,\n",
       " 'diario': 795,\n",
       " 'dejan': 796,\n",
       " 'darle': 797,\n",
       " 'dando': 798,\n",
       " 'costumbre': 799,\n",
       " 'cierto': 800,\n",
       " 'chingue': 801,\n",
       " 'cabeza': 802,\n",
       " 'buenos': 803,\n",
       " 'bola': 804,\n",
       " 'ba√±o': 805,\n",
       " 'acabar': 806,\n",
       " 'Vale': 807,\n",
       " 'T√∫': 808,\n",
       " 'SUS': 809,\n",
       " 'Mam√°': 810,\n",
       " 'MI': 811,\n",
       " 'Jaja': 812,\n",
       " 'Esos': 813,\n",
       " 'Esas': 814,\n",
       " 'Desde': 815,\n",
       " 'Bueno': 816,\n",
       " '>': 817,\n",
       " 'ü§∑üèª\\u200d‚ôÄ': 818,\n",
       " 'ü§ó': 819,\n",
       " 'üòë': 820,\n",
       " 'üòâ': 821,\n",
       " 'üëè': 822,\n",
       " 'xq': 823,\n",
       " 'we': 824,\n",
       " 'vivo': 825,\n",
       " 'vayan': 826,\n",
       " 'tuve': 827,\n",
       " 'tristes': 828,\n",
       " 'tantos': 829,\n",
       " 'tantas': 830,\n",
       " 'suerte': 831,\n",
       " 'siquiera': 832,\n",
       " 'sienten': 833,\n",
       " 'saca': 834,\n",
       " 'ropa': 835,\n",
       " 'puras': 836,\n",
       " 'perder': 837,\n",
       " 'pens√©': 838,\n",
       " 'paz': 839,\n",
       " 'pase': 840,\n",
       " 'nueva': 841,\n",
       " 'ni√±os': 842,\n",
       " 'ning√∫n': 843,\n",
       " 'mandan': 844,\n",
       " 'mam√≥': 845,\n",
       " 'mamada': 846,\n",
       " 'llorando': 847,\n",
       " 'jugar': 848,\n",
       " 'jam√°s': 849,\n",
       " 'hondure√±os': 850,\n",
       " 'hermana': 851,\n",
       " 'habla': 852,\n",
       " 'gustan': 853,\n",
       " 'gobierno': 854,\n",
       " 'frase': 855,\n",
       " 'examen': 856,\n",
       " 'empieza': 857,\n",
       " 'dijeron': 858,\n",
       " 'darme': 859,\n",
       " 'cual': 860,\n",
       " 'comiendo': 861,\n",
       " 'chile': 862,\n",
       " 'caracteres': 863,\n",
       " 'cagada': 864,\n",
       " 'bonitas': 865,\n",
       " 'Sabes': 866,\n",
       " 'Q': 867,\n",
       " 'Nunca': 868,\n",
       " 'Nada': 869,\n",
       " 'M√°s': 870,\n",
       " 'Messi': 871,\n",
       " 'Esto': 872,\n",
       " 'Ese': 873,\n",
       " 'DEL': 874,\n",
       " 'Chingas': 875,\n",
       " 'Chile': 876,\n",
       " 'Cada': 877,\n",
       " 'Amo': 878,\n",
       " '8': 879,\n",
       " 'üòñ': 880,\n",
       " 'üòê': 881,\n",
       " 'üíñ': 882,\n",
       " 'üëå': 883,\n",
       " '‚ò∫': 884,\n",
       " 'v√°yanse': 885,\n",
       " 'vuelven': 886,\n",
       " 'vivir': 887,\n",
       " 'venir': 888,\n",
       " 'vemos': 889,\n",
       " 'valga': 890,\n",
       " 'usan': 891,\n",
       " 'tweets': 892,\n",
       " 'trae': 893,\n",
       " 'todav√≠a': 894,\n",
       " 'temprano': 895,\n",
       " 'sigues': 896,\n",
       " 'ser√≠a': 897,\n",
       " 'seria': 898,\n",
       " 'salgo': 899,\n",
       " 'querer': 900,\n",
       " 'prietas': 901,\n",
       " 'peda': 902,\n",
       " 'pas√≥': 903,\n",
       " 'normal': 904,\n",
       " 'naturaleza': 905,\n",
       " 'm√≠a': 906,\n",
       " 'mucha': 907,\n",
       " 'mira': 908,\n",
       " 'mente': 909,\n",
       " 'mentada': 910,\n",
       " 'juntos': 911,\n",
       " 'ja': 912,\n",
       " 'imagen': 913,\n",
       " 'horrible': 914,\n",
       " 'haya': 915,\n",
       " 'hacerlo': 916,\n",
       " 'hablas': 917,\n",
       " 'hablan': 918,\n",
       " 'gringos': 919,\n",
       " 'gatos': 920,\n",
       " 'fuerte': 921,\n",
       " 'extra√±o': 922,\n",
       " 'etc': 923,\n",
       " 'est√©': 924,\n",
       " 'estar√≠a': 925,\n",
       " 'esperar': 926,\n",
       " 'empiezan': 927,\n",
       " 'edad': 928,\n",
       " 'duele': 929,\n",
       " 'diferente': 930,\n",
       " 'dieron': 931,\n",
       " 'derechos': 932,\n",
       " 'dentro': 933,\n",
       " 'cuerpo': 934,\n",
       " 'critican': 935,\n",
       " 'cree': 936,\n",
       " 'clima': 937,\n",
       " 'claro': 938,\n",
       " 'carro': 939,\n",
       " 'cagas': 940,\n",
       " 'ayuda': 941,\n",
       " 'am': 942,\n",
       " 'alg√∫n': 943,\n",
       " 'alguna': 944,\n",
       " 'alcohol': 945,\n",
       " 'acaba': 946,\n",
       " 'UNA': 947,\n",
       " 'Todas': 948,\n",
       " 'Tambi√©n': 949,\n",
       " 'S√≥lo': 950,\n",
       " 'Nos': 951,\n",
       " 'Mejor': 952,\n",
       " 'Loca': 953,\n",
       " 'Gente': 954,\n",
       " 'Facebook': 955,\n",
       " 'Est√°n': 956,\n",
       " 'Cosas': 957,\n",
       " 'Calcuta': 958,\n",
       " 'Buenos': 959,\n",
       " 'Ando': 960,\n",
       " '280': 961,\n",
       " 'ü§¶üèª\\u200d‚ôÇ': 962,\n",
       " 'üôÇ': 963,\n",
       " 'üòì': 964,\n",
       " 'üê∑': 965,\n",
       " 'üéµ': 966,\n",
       " '‚úäüèº': 967,\n",
       " '‚Äî': 968,\n",
       " '√∫nica': 969,\n",
       " 'verg√ºenza': 970,\n",
       " 'vergazos': 971,\n",
       " 'tr√°fico': 972,\n",
       " 'tobog√°n': 973,\n",
       " 'tel√©fono': 974,\n",
       " 't': 975,\n",
       " 'super': 976,\n",
       " 'subir': 977,\n",
       " 'sigan': 978,\n",
       " 'se√±or': 979,\n",
       " 'sexo': 980,\n",
       " 'sentir': 981,\n",
       " 'sentido': 982,\n",
       " 'selecci√≥n': 983,\n",
       " 'sab√≠a': 984,\n",
       " 'regreso': 985,\n",
       " 'realidad': 986,\n",
       " 'rateros': 987,\n",
       " 'queriendo': 988,\n",
       " 'puso': 989,\n",
       " 'puse': 990,\n",
       " 'puesto': 991,\n",
       " 'presidente': 992,\n",
       " 'porno': 993,\n",
       " 'ponerme': 994,\n",
       " 'placer': 995,\n",
       " 'piensan': 996,\n",
       " 'piel': 997,\n",
       " 'pesar': 998,\n",
       " 'pensando': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generamos las bolsas de palabrascon pesado binario  de training y de validaci√≥n \n",
    "binary_bow_tr=build_binary_bow(tr_txt,dict_freq,dict_indices)\n",
    "binary_boW_validacion=build_binary_bow(val_txt,dict_freq,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generamos las bolsas de palabras con pesado frecuencia de training y de validaci√≥n \n",
    "frequency_bow_tr=build_freq_bow(tr_txt,dict_freq,dict_indices)\n",
    "frequency_boW_validacion=build_freq_bow(val_txt,dict_freq,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generamos las bolsas de palabras con pesado tfidf de training y de validaci√≥n \n",
    "tfidf_bow_tr=build_tfidf_bow(tr_txt,dict_freq,dict_indices)\n",
    "tfidf_boW_validacion=build_tfidf_bow(val_txt,dict_freq,dict_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Diferencias en bolsa de palabras\n",
    "\n",
    "veamos la diferencia entre los pesos con cada esquema para un caso particular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el token @USUARIO corresponde al lugar 9 en el diccionario de frecuencias \n",
      "el primer tweet de los datos (es decir el documento 0) contiene tres veces este token\n",
      "El peso w_(0,9) es:\n",
      "con pesado binario: 1\n",
      "con pesado de frecnuencia: 3\n",
      "con pesado de tfidf: 5\n"
     ]
    }
   ],
   "source": [
    "#el token \"\"@USUARIO\"\" corresponde al lugar 9 en el diccionario de frecuencias \n",
    "x=dict_indices.get(\"@USUARIO\")\n",
    "print(\"el token \"\"@USUARIO\"\" corresponde al lugar\",x, \"en el diccionario de frecuencias \")\n",
    "print(\"el primer tweet de los datos (es decir el documento 0) contiene tres veces este token\")\n",
    "print(\"El peso w_(0,9) es:\")\n",
    "print(\"con pesado binario:\",binary_bow_tr[0][9])\n",
    "print(\"con pesado de frecnuencia:\", frequency_bow_tr[0][9])\n",
    "print(\"con pesado de tfidf:\", tfidf_bow_tr[0][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Clasificador}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras binaria y las etiqutas\n",
    "grid.fit(binary_bow_tr,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_binary=grid.predict(binary_boW_validacion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras de frecuencia y las etiqutas\n",
    "grid.fit(frequency_bow_tr,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_frequency=grid.predict(frequency_boW_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/.conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras TFIDF y las etiqutas\n",
    "grid.fit(tfidf_bow_tr,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_tfidf=grid.predict(tfidf_boW_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Binary BoW_____________\n",
      "[[356  62]\n",
      " [ 49 120]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.87       418\n",
      "           1       0.66      0.71      0.68       169\n",
      "\n",
      "    accuracy                           0.81       587\n",
      "   macro avg       0.77      0.78      0.77       587\n",
      "weighted avg       0.82      0.81      0.81       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________Binary BoW_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_binary))\n",
    "print(metrics.classification_report(val_labels,y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Frequency BoW_____________\n",
      "[[362  56]\n",
      " [ 45 124]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88       418\n",
      "           1       0.69      0.73      0.71       169\n",
      "\n",
      "    accuracy                           0.83       587\n",
      "   macro avg       0.79      0.80      0.79       587\n",
      "weighted avg       0.83      0.83      0.83       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________Frequency BoW_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_frequency))\n",
    "print(metrics.classification_report(val_labels,y_pred_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________TfiDf BoW_____________\n",
      "[[356  62]\n",
      " [ 71  98]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       418\n",
      "           1       0.61      0.58      0.60       169\n",
      "\n",
      "    accuracy                           0.77       587\n",
      "   macro avg       0.72      0.72      0.72       587\n",
      "weighted avg       0.77      0.77      0.77       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________TfiDf BoW_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_tfidf))\n",
    "print(metrics.classification_report(val_labels,y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "4,5,6. Evalue BoW con pesado binario, de frecuencia y tfidf con normalizado l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Bolsas de palabras con distintos equemas de pesado con normalizado L2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizacion_dos(X):\n",
    "    norm = np.linalg.norm(X, ord=2, axis=1)  # Calcula la norma L2 de cada fila\n",
    "    norm = np.where(norm == 0, 1, norm) \n",
    "    norm = norm.reshape(-1, 1)  # Reshape para que la divisi√≥n sea compatible\n",
    "    return X / norm  # Divide cada elemento de X por la norma correspondiente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizamos las bolsas de palabrascon pesado binario  de training y de validaci√≥n \n",
    "binary_bow_tr_norm=normalizacion_dos(binary_bow_tr)\n",
    "binary_boW_validacion_norm=normalizacion_dos(binary_boW_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizamos las bolsas de palabrascon pesado frecuencia  de training y de validaci√≥n \n",
    "frequency_bow_tr_norm=normalizacion_dos(frequency_bow_tr)\n",
    "frequency_boW_validacion_norm=normalizacion_dos(frequency_boW_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizamos las bolsas de palabrascon pesado tfidf  de training y de validaci√≥n \n",
    "tfidf_bow_tr_norm=normalizacion_dos(tfidf_bow_tr)\n",
    "tfidf_boW_validacion_norm=normalizacion_dos(tfidf_boW_validacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Diferencias en bolsa de palabras\n",
    "\n",
    "veamos la diferencia entre los pesos con cada esquema para un caso particular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el token @USUARIO corresponde al lugar 9 en el diccionario de frecuencias \n",
      "el primer tweet de los datos (es decir el documento 0) contiene tres veces este token\n",
      "El peso w_(0,9) es:\n",
      "con pesado binario: 0.20412414523193154\n",
      "con pesado de frecnuencia: 0.48666426339228763\n",
      "con pesado de tfidf: 0.21128856368212914\n"
     ]
    }
   ],
   "source": [
    "#el token \"\"@USUARIO\"\" corresponde al lugar 9 en el diccionario de frecuencias \n",
    "x=dict_indices.get(\"@USUARIO\")\n",
    "print(\"el token \"\"@USUARIO\"\" corresponde al lugar\",x, \"en el diccionario de frecuencias \")\n",
    "print(\"el primer tweet de los datos (es decir el documento 0) contiene tres veces este token\")\n",
    "print(\"El peso w_(0,9) es:\")\n",
    "print(\"con pesado binario:\",binary_bow_tr_norm[0][9])\n",
    "print(\"con pesado de frecnuencia:\", frequency_bow_tr_norm[0][9])\n",
    "print(\"con pesado de tfidf:\", tfidf_bow_tr_norm[0][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Clasificador}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras binaria y las etiqutas\n",
    "grid.fit(binary_bow_tr_norm,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_binary_norm=grid.predict(binary_boW_validacion_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras de frecuencia y las etiqutas\n",
    "grid.fit(frequency_bow_tr_norm,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_frequency_norm=grid.predict(frequency_boW_validacion_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras TFIDF y las etiqutas\n",
    "grid.fit(tfidf_bow_tr_norm,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_tfidf_norm=grid.predict(tfidf_boW_validacion_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Binary BoW L2 normalization_____________\n",
      "[[356  62]\n",
      " [ 47 122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.87       418\n",
      "           1       0.66      0.72      0.69       169\n",
      "\n",
      "    accuracy                           0.81       587\n",
      "   macro avg       0.77      0.79      0.78       587\n",
      "weighted avg       0.82      0.81      0.82       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________Binary BoW L2 normalization_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_binary_norm))\n",
    "print(metrics.classification_report(val_labels,y_pred_binary_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Frequency BoW L2 normalization_____________\n",
      "[[358  60]\n",
      " [ 38 131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88       418\n",
      "           1       0.69      0.78      0.73       169\n",
      "\n",
      "    accuracy                           0.83       587\n",
      "   macro avg       0.79      0.82      0.80       587\n",
      "weighted avg       0.84      0.83      0.84       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________Frequency BoW L2 normalization_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_frequency_norm))\n",
    "print(metrics.classification_report(val_labels,y_pred_frequency_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________TfiDf BoW L2 normalization_____________\n",
      "[[356  62]\n",
      " [ 40 129]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.87       418\n",
      "           1       0.68      0.76      0.72       169\n",
      "\n",
      "    accuracy                           0.83       587\n",
      "   macro avg       0.79      0.81      0.80       587\n",
      "weighted avg       0.83      0.83      0.83       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________TfiDf BoW L2 normalization_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_tfidf_norm))\n",
    "print(metrics.classification_report(val_labels,y_pred_tfidf_norm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "7. Ponga una tabla comparativa a modo de resumen con las seis entradas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesado binario \n",
      " Accuracy 0.82 \n",
      " Precision (0) 0.88 \n",
      " Precision (1) 0.66\n",
      "Pesado binario normlizado \n",
      " Accuracy 0.82 \n",
      " Precision (0) 0.88 \n",
      " Precision (1) 0.66\n",
      "Pesado frecuencia \n",
      " Accuracy 0.83 \n",
      " Precision (0) 0.89 \n",
      " Precision (1) 0.69\n",
      "Pesado frecuencia normalizado \n",
      " Accuracy 0.84 \n",
      " Precision (0) 0.9 \n",
      " Precision (1) 0.69\n",
      "Pesado tfidf \n",
      " Accuracy 0.77 \n",
      " Precision (0) 0.83 \n",
      " Precision (1) 0.61\n",
      "Pesado tfidf normalizado \n",
      " Accuracy 0.83 \n",
      " Precision (0) 0.9 \n",
      " Precision (1) 0.68\n"
     ]
    }
   ],
   "source": [
    "lista_predicciones=[y_pred_binary,y_pred_binary_norm,y_pred_frequency,y_pred_frequency_norm,y_pred_tfidf,y_pred_tfidf_norm]\n",
    "lista_pesados=[\"Pesado binario\",\"Pesado binario normlizado\",\"Pesado frecuencia\",\"Pesado frecuencia normalizado\",\"Pesado tfidf\",\"Pesado tfidf normalizado\"]\n",
    "for predict, pesado in zip(lista_predicciones, lista_pesados):\n",
    "    m = metrics.classification_report(val_labels, predict)\n",
    "    accuracy = float(m.split('\\n')[-2].split()[2])\n",
    "    precision_0 = float(m.split('\\n')[2].split()[1])  \n",
    "    precision_1 = float(m.split('\\n')[3].split()[1])  \n",
    "    \n",
    "    print(pesado, \"\\n Accuracy\", accuracy, \"\\n Precision (0)\",precision_0,\"\\n Precision (1)\",precision_1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El accuracy m√°s alto se obtene para el pesado *frecuencia normalizado*, con 0.84. Como tenemos un conjunto desbalanceado buscaremos maximizar el recall para la clase minoritaria (etiqueta 1) para capturar la mayor cantidad posible de casos positivos verdaderos, as√≠ que comparremos con la el siguiente pesado con mayor accuracy que es *Pesado tfidf normalizado*. Notemos a precisi√≥n para la etiqueta 0 es la misma: 0.9, miestras que para la etiqueta uno varia un poco (0.1), siendo mayor para Frecuecia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesado frecuencia normalizado \n",
      " Accuracy 0.84 \n",
      " Recall (0) 0.86 \n",
      " Recall (1) 0.78\n",
      "Pesado tfidf normalizado \n",
      " Accuracy 0.83 \n",
      " Recall (0) 0.85 \n",
      " Recall (1) 0.76\n"
     ]
    }
   ],
   "source": [
    "lista_predicciones=[y_pred_frequency_norm,y_pred_tfidf_norm]\n",
    "lista_pesados=[\"Pesado frecuencia normalizado\",\"Pesado tfidf normalizado\"]\n",
    "for predict, pesado in zip(lista_predicciones, lista_pesados):\n",
    "    m = metrics.classification_report(val_labels, predict)\n",
    "    accuracy = float(m.split('\\n')[-2].split()[2])\n",
    "    precision_0 = float(m.split('\\n')[2].split()[2])  \n",
    "    precision_1 = float(m.split('\\n')[3].split()[2])  \n",
    "    \n",
    "    print(pesado, \"\\n Accuracy\", accuracy, \"\\n Recall (0)\",precision_0,\"\\n Recall (1)\",precision_1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As√≠ que el mejor resultado se obtuvo con pesado defrecuencia y normalizaci√≥n L2, mayor accuracy, mayor precision e incluso mayor recall para la etiqueta minoritaria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "8. De las configuraciones anteriores elija la mejor y eval√∫ela con m√°s y menos t√©rminos (e.g., 1000 y 7000). Ponga una tabla d√≥nde compare las tres configuraciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos diccionario de indices y de frecuencias haciendo corte en las mil primeras palabras\n",
    "dict_freq_1000=create_dic_freq(corpus_palabras,1000)\n",
    "dict_indices_1000=create_dic_ranking(dict_freq_1000)\n",
    "\n",
    "#Bolsas de palabras con pesado frecuencia de training y de validaci√≥n usando los diccionarios anteriores\n",
    "bow_tr_1000=build_freq_bow(tr_txt,dict_freq_1000,dict_indices_1000)\n",
    "boW_validacion_1000=build_freq_bow(val_txt,dict_freq_1000,dict_indices_1000)\n",
    "\n",
    "#normalizamos \n",
    "bow_tr_norm_1000=normalizacion_dos(bow_tr_1000)\n",
    "boW_validacion_norm_1000=normalizacion_dos(boW_validacion_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras \n",
    "grid.fit(bow_tr_norm_1000,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_1000=grid.predict(boW_validacion_norm_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Frequency BoW (1000) L2 normalization_____________\n",
      "[[346  72]\n",
      " [ 39 130]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86       418\n",
      "           1       0.64      0.77      0.70       169\n",
      "\n",
      "    accuracy                           0.81       587\n",
      "   macro avg       0.77      0.80      0.78       587\n",
      "weighted avg       0.83      0.81      0.82       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________Frequency BoW (1000) L2 normalization_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_1000))\n",
    "print(metrics.classification_report(val_labels,y_pred_1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos diccionario de indices y de frecuencias haciendo corte en las mil primeras palabras\n",
    "dict_freq_7000=create_dic_freq(corpus_palabras,7000)\n",
    "dict_indices_7000=create_dic_ranking(dict_freq_7000)\n",
    "\n",
    "#Bolsas de palabras con pesado frecuencia de training y de validaci√≥n usando los diccionarios anteriores\n",
    "bow_tr_7000=build_freq_bow(tr_txt,dict_freq_7000,dict_indices_7000)\n",
    "boW_validacion_7000=build_freq_bow(val_txt,dict_freq_7000,dict_indices_7000)\n",
    "\n",
    "#normalizamos \n",
    "bow_tr_norm_7000=normalizacion_dos(bow_tr_7000)\n",
    "boW_validacion_norm_7000=normalizacion_dos(boW_validacion_7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llamamos al clasificador y le pasamos la bolsa de palabras \n",
    "grid.fit(bow_tr_norm_7000,tr_labels)\n",
    "#predicci√≥n\n",
    "y_pred_7000=grid.predict(boW_validacion_norm_7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Frequency BoW (7000) L2 normalization_____________\n",
      "[[359  59]\n",
      " [ 39 130]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88       418\n",
      "           1       0.69      0.77      0.73       169\n",
      "\n",
      "    accuracy                           0.83       587\n",
      "   macro avg       0.79      0.81      0.80       587\n",
      "weighted avg       0.84      0.83      0.84       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#medidas, reporte de clasificaci√≥n,matriz de confusi√≥n\n",
    "print(\"_____________Frequency BoW (7000) L2 normalization_____________\")\n",
    "print(confusion_matrix(val_labels,y_pred_7000))\n",
    "print(metrics.classification_report(val_labels,y_pred_7000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comparaci√≥n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bolsa depalabras con pesado de frecuencia y normalizaci√≥n L2\n",
      "\n",
      "palabras tomadas para BoW: 1000 \n",
      " Accuracy 0.83 \n",
      " Precision (0) 0.9 \n",
      " Precision (1) 0.64\n",
      "palabras tomadas para BoW: 5000 \n",
      " Accuracy 0.84 \n",
      " Precision (0) 0.9 \n",
      " Precision (1) 0.69\n",
      "palabras tomadas para BoW: 7000 \n",
      " Accuracy 0.84 \n",
      " Precision (0) 0.9 \n",
      " Precision (1) 0.69\n"
     ]
    }
   ],
   "source": [
    "lista_predicciones=[y_pred_1000,y_pred_frequency_norm,y_pred_7000]\n",
    "lista_pesados=[\"1000\",\"5000\",\"7000\"]\n",
    "print(\"Bolsa depalabras con pesado de frecuencia y normalizaci√≥n L2\\n\")\n",
    "for predict, pesado in zip(lista_predicciones, lista_pesados):\n",
    "    m = metrics.classification_report(val_labels, predict)\n",
    "    accuracy = float(m.split('\\n')[-2].split()[2])\n",
    "    precision_0 = float(m.split('\\n')[2].split()[1])  \n",
    "    precision_1 = float(m.split('\\n')[3].split()[1])  \n",
    "    \n",
    "    print(\"palabras tomadas para BoW:\",pesado, \"\\n Accuracy\", accuracy, \"\\n Precision (0)\",precision_0,\"\\n Precision (1)\",precision_1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "9. Utilice el recurso l√©xico del Consejo Nacional de Investigaci√≥n de Canad√° llamado \"EmoLex\" (https://www.saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) para construir una \"Bolsa de Emociones\" de los Tweets de agresividad (Debe usar EmoLex en Espa√±ol). Para esto, una estrategia sencilla ser√≠a enmascarar cada palabra con su emoci√≥n, y despu√©s construir la Bolsa de Emociones (BoE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "10. Eval√∫a t√∫ BoE clasificando con SVM. Ponga una tabla comparativa a modo de resumen con los tres pesados, normalize cada uno si lo cree conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4.5 color='lightblue'>\n",
    "\n",
    "$\\textit{Ejercicio 3. Recurso L√≠nguistico de Emociones Mexicano}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "11. Utiliceelrecursol√©xicollamado\"SpanishEmotionLexicon(SEL)\"delDr.GrigoriSidorov, profesor del Centro de Investigaci√≥n en Computaci√≥n (CIC) del Instituto Polit√©cnico Nacional (http://www.cic.ipn.mx/‚àºsidorov/), para enmascarar cada palabra con su emo- ci√≥n, y despu√©s construir la Bolsa de Emociones con alg√∫n pesado (e.g., binario, tf, tfidf). Proponga alguna estrategia para incorporar el \"valor\" del \"Probability Factor of Affective use\" en su representaci√≥n vectorial del documento. Eval√∫a y escribe una tabla compara- tiva a modo de resumen con al menos tres pesados: binario, frecuencia, tfidf. Normalize cada pesado seg√∫n lo crea conveniente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "12. En un comentario aparte, discuta sobre la estrateg√≠a que utiliz√≥ para incorporar el \"Probability Factor of Affective use\". No m√°s de 5 renglones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4.5 color='lightblue'>\n",
    "\n",
    "$\\textit{Ejercicio 4. ¬øPodemos mejorar con Bigramas?}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "13. Hacer un experimento d√≥nde concatene una buena BoW seg√∫n sus experimentos anteri- ores con otra BoW construida a partir de los 1000 bigramas m√°s frecuentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "14. Hacer un experimento con las Bolsas de Emociones, Bolsa de Palabras y Bolsa de Bi- gramas; usted elige las dimensionalidades. Para construir la representaci√≥n final del documento utilice la concatenaci√≥n de las representaciones seg√∫n sus observaciones (e.g., Bolsa de Palabras + Bolsa de Bigramas + Bolsa de Sentimientos de Canad√° + Bolsa de Sentimientos de Grigori), y alim√©ntelas a un SVM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2.5 color='lightblue'>\n",
    "\n",
    "15. Elabore conclusiones sobre toda esta Tarea, incluyendo observaciones, comentarios y posibles mejoras futuras. Discuta el comportamiento de la BoW de usar solo palabras a integrar bigramas, y luego a integrar todo ¬øayud√≥? o ¬øempeor√≥?. Discuta tambi√©n brevemente el costo computacional de los experimentos ¬øVali√≥ la Pena tener todo?. Sea breve: todo en NO m√°s de dos p√°rrafos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
