{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{PLN. Práctica 5: Bengio Model}$$\n",
    "$$\\textit{Y. Sarahi García Gozález}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4.5 color='lightblue'>\n",
    "\n",
    "$\\textit{Librerías}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple \n",
    "from argparse import Namespace #objeto que me ayude a guardar espacio de variables\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "#preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk import FreqDist\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#pyTorch\n",
    "import torch\n",
    "from torch.utils.data import dataloader,TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#sckit-learns\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarea realizada en MacOs. \n",
      "Las versiones de las librerías y de python utilizadas fueron:\n",
      "\n",
      "Python version: 3.11.0\n",
      "NumPy version: 1.23.5\n",
      "NLTK version: 3.8.1\n",
      "Pandas version: 2.1.4\n",
      "Scikit-learn version: 1.3.0\n",
      "Pythorch version: 2.2.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Tarea realizada en MacOs. \\nLas versiones de las librerías y de python utilizadas fueron:\\n\")\n",
    "from platform import python_version\n",
    "print(\"Python version:\", python_version())\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"NLTK version:\", nltk.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Scikit-learn version:\", sklearn.__version__)\n",
    "print(\"Pythorch version:\",torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Preparación de texto, corpus y diccionarios}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buscamos que los resultados sean reproducibles por lo que\n",
    "#definimos una semillas constantes:\n",
    "seed=111\n",
    "random.seed(seed) #python seed\n",
    "np.random.seed(seed) #numpy seed\n",
    "torch.manual_seed(seed) #torch seed\n",
    "torch.backends.cudnn.benchmark= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertimos el texto de train a una lista de tuits\n",
    "X_train=pd.read_csv(\"/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/practicas/03_practica/mex20_train.txt\",sep='\\r\\n',engine='python',header=None).iloc[:,0].values.tolist()\n",
    "#convertimos el texto de validacion a una lista de tuits\n",
    "X_val=pd.read_csv(\"/Users/ely/Documents/Maestria/segundo_semestre/cimat2023-1/lenguaje/practicas/03_practica/mex20_val.txt\",sep='\\r\\n',engine='python',header=None).iloc[:,0].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos espacio de parámetros\n",
    "args= Namespace()\n",
    "args.N=4 #primera variable: odelo de lenguaje de tretagramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramData():\n",
    "        \n",
    "      ## Constructor ##\n",
    "      def __init__(self, N:int,vocab_max:5000,tokenizer=None,embeddings_model=None):\n",
    "            \n",
    "            #tokenizador\n",
    "            self.tokenizer = tokenizer if tokenizer else self.default_tokenizer #en caso de no recibir tokenizador, creamos un por default\n",
    "            #signos de puntuación\n",
    "            self.punct=set((string.punctuation + '¡¿«»'))# (agregamos signos en español:¡¿«» )\n",
    "            self.punct.add('@USUARIO')\n",
    "            self.punct.add('@usuario')\n",
    "            #orden del modelo\n",
    "            self.N=N\n",
    "            #vocabulario maximo\n",
    "            self.vocab_max=vocab_max\n",
    "            #tokens especiales\n",
    "            self.UNK=\"<unk>\"\n",
    "            self.SOS=\"<s>\"\n",
    "            self.EOS=\"<\\s>\"\n",
    "            #vectores preentrenados\n",
    "            self.embeddings_mode=embeddings_model\n",
    "\n",
    "        ## Métodos ##\n",
    "      def default_tokenizer(self,doc: str)->list: #si no se da un tokenizador definimos uno por default\n",
    "            return doc.split(\" \") \n",
    "\n",
    "      def remove_word(self,word: str)->bool:\n",
    "            word=word.lower()\n",
    "            is_punct = True if word in self.punct else False\n",
    "            is_digit=word.isnumeric()\n",
    "            return is_digit or is_punct\n",
    "\n",
    "      def sortFreqDict(self,freq_dist)-> list:\n",
    "            freq_dict=dict(freq_dist)\n",
    "            return sorted(freq_dict,key=freq_dict.get,reverse=True)\n",
    "\n",
    "      def get_vocab(self,corpus:list)-> set:\n",
    "            Freq_Dist=FreqDist([w.lower() for sentence in corpus for w in self.tokenizer(sentence) if not self.remove_word(w)])\n",
    "            sorted_words=self.sortFreqDict(Freq_Dist)[:self.vocab_max-3]\n",
    "            return(set(sorted_words))\n",
    "\n",
    "      def fit(self,corpus:list)-> None: #esta funcion extrae el vocabulario\n",
    "              \n",
    "            self.vocab=self.get_vocab(corpus)\n",
    "            self.vocab.add(self.UNK)\n",
    "            self.vocab.add(self.SOS)\n",
    "            self.vocab.add(self.EOS)\n",
    "\n",
    "             #construimos los diccionarios de mapeo   \n",
    "            self.w2id = {}\n",
    "            self.id2w = {}\n",
    "            if self.embeddings_mode is not None:\n",
    "                  self.embeddings_matrix=np.empty([len(self.vocab),self.embeddings_mode.vector_size])\n",
    "            id=0\n",
    "            for doc in corpus:\n",
    "                  for word in self.tokenizer(doc):\n",
    "                        word_=word.lower()\n",
    "                        if word_ in self.vocab and not word_ in self.w2id:\n",
    "                              self.w2id[word_]=id\n",
    "                              self.id2w[id] = word_\n",
    "\n",
    "                              if self.embeddings_mode is not None:\n",
    "                                  if word_ in self.embeddings_mode:\n",
    "                                      \n",
    "                                      self.embeddings_matrix[id] = self.embeddings_mode[word_]\n",
    "                                  else:\n",
    "                                        self.embeddings_matrix[id] = np.random.rand(self.embeddings_mode.vector_size)\n",
    "\n",
    "                              id += 1\n",
    "\n",
    "            #siempre hay que agregar los tokens especiales\n",
    "                              \n",
    "            self.w2id.update(\n",
    "                  {\n",
    "                        self.UNK:id,\n",
    "                        self.SOS:id+1,\n",
    "                        self.EOS:id+2\n",
    "                  }\n",
    "            )\n",
    "            self.id2w.update(\n",
    "                  {\n",
    "                        id:self.UNK,\n",
    "                        id+1:self.SOS,\n",
    "                        id+2:self.EOS\n",
    "                  }\n",
    "            )\n",
    "\n",
    "      def get_ngram_doc(self,doc:list) -> list:\n",
    "            doc_tokens = self.tokenizer(doc)\n",
    "            doc_tokens = self.replace_unk(doc_tokens)\n",
    "            doc_tokens = [w.lower() for w in doc_tokens]\n",
    "            doc_tokens = [self.SOS]*(self.N - 1) + doc_tokens + [self.EOS]\n",
    "            return list(ngrams(doc_tokens,self.N))\n",
    "\n",
    "      def replace_unk(self,doc_tokens:list)-> list:\n",
    "            for i,token in enumerate(doc_tokens):\n",
    "                  if token.lower() not in self.vocab:\n",
    "                        doc_tokens[i]=self.UNK\n",
    "            return doc_tokens\n",
    "            \n",
    "\n",
    "      def transform(self,corpus:list)->Tuple[np.ndarray,np.ndarray]:\n",
    "\n",
    "            x_ngrams = []\n",
    "            y_labels = []\n",
    "\n",
    "            for doc in corpus:\n",
    "                  doc_ngram =self.get_ngram_doc(doc)\n",
    "                  for words_window in doc_ngram:\n",
    "                        words_window_ids= [self.w2id[w] for w in words_window]\n",
    "                        x_ngrams.append(list(words_window_ids[:-1]))\n",
    "                        y_labels.append(words_window_ids[-1])\n",
    "\n",
    "            return np.array(x_ngrams),np.array(y_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizamos\n",
    "tokenizer=TweetTokenizer()\n",
    "ngram_data=NgramData(args.N,5000,tokenizer.tokenize)\n",
    "ngram_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ngram_train,y_ngram_train=ngram_data.transform(X_train)\n",
    "x_ngram_val,y_ngram_val=ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', '<s>', '<s>'],\n",
       " ['<s>', '<s>', '<unk>'],\n",
       " ['<s>', '<unk>', '<unk>'],\n",
       " ['<unk>', '<unk>', '<unk>'],\n",
       " ['<unk>', '<unk>', 'q'],\n",
       " ['<unk>', 'q', 'se'],\n",
       " ['q', 'se', 'puede'],\n",
       " ['se', 'puede', 'esperar'],\n",
       " ['puede', 'esperar', 'del'],\n",
       " ['esperar', 'del', 'maricon'],\n",
       " ['del', 'maricon', 'de'],\n",
       " ['maricon', 'de', 'closet'],\n",
       " ['de', 'closet', 'de'],\n",
       " ['closet', 'de', 'la'],\n",
       " ['de', 'la', 'yañez'],\n",
       " ['la', 'yañez', 'aun'],\n",
       " ['yañez', 'aun', 'recuerdo'],\n",
       " ['aun', 'recuerdo', 'esa'],\n",
       " ['recuerdo', 'esa', 'ves'],\n",
       " ['esa', 'ves', 'q'],\n",
       " ['ves', 'q', 'lo'],\n",
       " ['q', 'lo', 'vi']]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[ngram_data.id2w[w] for w in tw] for tw in x_ngram_train[:22]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
