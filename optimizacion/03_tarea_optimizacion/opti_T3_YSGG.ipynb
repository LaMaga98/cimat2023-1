{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{Optimización I. Tarea 3}$$\n",
    "$$\\textit{Y. Sarahi García Gozález}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{lightblue}{Librerías \\space }$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarea realizada en MacOs. \n",
      "Las versiones de las librerías y de python utilizadas fueron:\n",
      "\n",
      "Python version 3.11.7\n",
      "Numpy version 1.26.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Tarea realizada en MacOs. \\nLas versiones de las librerías y de python utilizadas fueron:\\n\")\n",
    "from platform import python_version\n",
    "print(\"Python version\", python_version())\n",
    "print(\"Numpy version\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon de la máquina: 2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "#imprimimos el epsilon de la máquina\n",
    "epsilon = np.finfo(float).eps\n",
    "print(\"Epsilon de la máquina:\", epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{lightblue}{Ejercicio \\space 1}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Programar la función que implementa el algoritmo de backtracking \n",
    "   (Algoritmo 2 de la Clase 6) que usa la condición de descenso suficiente\n",
    "   (condición de Armijo) para seleccionar el tamaño de paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(alpha_ini,rho,c,x_k,f,f_k,df_k,p_k,iter_max):\n",
    "\n",
    "    '''\n",
    "        Esta funcion parte de un tamaño de paso inicial alpha_ini y lo va recortando hasta que\n",
    "        cumple la cond de descenso suficiente\n",
    "\n",
    "        parametros:\n",
    "            valores (float): alpha_ini, rho entre (0,1), f(x_k), Df(x_k) (gradiente en el punto x_k), c_1, \n",
    "            direccion de descenso (np.rray): p_k \n",
    "\n",
    "        returns:\n",
    "            el tamaño de paso a_k\n",
    "            numero de iteraciones realizadas i_k\n",
    "    '''\n",
    "\n",
    "    alpha=alpha_ini #fijamos alpha como el alpha inicial\n",
    "    gp=c*np.dot(df_k,p_k) #hacemos el producto gradiente por direccion de descenso p\n",
    "    \n",
    "    for i in range(iter_max):\n",
    "        x_kp=x_k+alpha*p_k\n",
    "\n",
    "        #si la condicion de descenso se cumple, terminamos\n",
    "        if f(x_kp)<=(f_k + alpha*gp):\n",
    "            return alpha,i,True\n",
    "\n",
    "        alpha=alpha*rho #si no se cumple la cond, hacemos alpha*rho\n",
    "        print(alpha)\n",
    "\n",
    "        \n",
    "    return  alpha,i,False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Programar la función que implementa el algoritmo de descenso máximo con\n",
    "   backtracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La función $f(\\mathbf{x})$, \n",
    "- el gradiente $\\nabla f(\\mathbf{x})$ de la función $f$, \n",
    "- un punto inicial $\\mathbf{x}_{0}$, \n",
    "- las tolerancia $\\tau>0$, \n",
    "- el número máximo de iteraciones $N$ para el algoritmo de descenso máximo, \n",
    "- el valor inicial $\\alpha_{ini}$,\n",
    "- el valor $\\rho \\in (0,1)$,\n",
    "- la constante $c_1 \\in (0,1)$ para la condición de Armijo y\n",
    "- el número máximo de iteraciones $N_{gs}$ para el método de la sección dorada.\n",
    "\n",
    "La función devuelve \n",
    "- El último punto $\\mathbf{x}_{k}$ generado por el algoritmo,\n",
    "- el número $k$ de iteraciones realizadas y\n",
    "- Una variable indicadora que es $True$ si el algoritmo termina por \n",
    "  cumplirse la condición de paro ($\\|\\alpha_k \\mathbf{p}_{k}\\| < \\tau$) o\n",
    "  $False$ si termina porque se alcanzó el número máximo de iteraciones.\n",
    "- Si $n\\ne 2$, devuelve un arreglo vació. En caso contrario, devuelve un \n",
    "  arreglo que contiene las componentes de los puntos de la secuencia, el\n",
    "  tamaño de paso y la cantidad de iteraciones que hizo el algoritmo \n",
    "  de backtracking en cada iteración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def descenso_maximo_backtracking(f,df,x0,alpha_ini,rho,c,tau,max_iter,max_iter_b):\n",
    "    '''\n",
    "    Esta funcion busca el minimo de la funcion f usando la tecnica de backtracking\n",
    "\n",
    "    Parametros: \n",
    "        f: función a optimizar\n",
    "        df: gradiente de f\n",
    "        x_0: valor inicial\n",
    "        tau,N: tolerancia y numero maximo de iteraciones (descenso)\n",
    "        parametros de \n",
    "\n",
    "        NOTA:los argumentos predeterminados son específicos para este ejercicio y n=2\n",
    "    \n",
    "    returns:\n",
    "        x_k: ultimo punto de la sucesión que genera el algoritmo\n",
    "        k: número de iteraciones\n",
    "        True/False: Indica si se satisfizo la condición de tolerancia\n",
    "        x1,x2...xk: sucesión de puntos (np.array)\n",
    "        \n",
    "    '''\n",
    "    n=len(x0)\n",
    "    x_k=x0\n",
    "    indicador=False\n",
    "    f_k=f(x_k)\n",
    "    df_k=df(x_k)\n",
    "\n",
    "\n",
    "    if n==2:\n",
    "        m = np.zeros((max_iter+1,4))\n",
    "        m[0,:] = x0[0],x0[1],1,0\n",
    "    \n",
    " \n",
    "    for k in range(max_iter):\n",
    "        #calculmos pk y ak\n",
    "        p_k=-df_k\n",
    "        a_k,i,ind=backtracking(alpha_ini,rho,c,x_k,f,f_k,df_k,p_k,max_iter_b)\n",
    "   \n",
    "        if not ind:\n",
    "            print('Insuficientes iteraciones Backtracking', a_k)\n",
    "\n",
    "        x_k=x_k+(a_k*p_k)\n",
    "        \n",
    "        if n==2:\n",
    "            m[k,:]= x_k[0],x_k[1],a_k,i\n",
    "\n",
    "\n",
    "        if norm(a_k*p_k)<tau:#si se cumple la condicion de tolerania:\n",
    "            indicador=True #indicador verdadero\n",
    "            break #y romepos el ciclo\n",
    "\n",
    "        f_k=f(x_k)\n",
    "        df_k=df(x_k)\n",
    "\n",
    "        \n",
    "    if n==2:\n",
    "        return x_k,k,indicador,m\n",
    "    \n",
    "    return x_k,k,indicador,None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Para probar el algoritmo, programe las siguientes funciones, calcule su gradiente \n",
    "   de manera analítica y programe la función correspondiente. Use cada punto \n",
    "   $\\mathbf{x}_0$ como punto inicial del algoritmo.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = sqrt(epsilon) #tolerancia (después multiplicaremos por sqrt(n))\n",
    "aini = 1.0 \n",
    "rho  = 0.8 \n",
    "c1   = 0.1\n",
    "N    = 30000 #iter maximas para descenso\n",
    "Nb   = 600 #iter maximas para backtracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función de Himmelblau:** Para $\\mathbf{x}=(x_1,x_2)$\n",
    "\n",
    "$$ f(\\mathbf{x}) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2. $$\n",
    "$$ \\mathbf{x}_0 = (2.,4.) $$\n",
    "$$ \\mathbf{x}_0 = (0.,0.) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función objetivo\n",
    "def Himmelblau(x):\n",
    "    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2;\n",
    "\n",
    "# Gradiente de la función objetivo\n",
    "def D_Himmelblau(x):\n",
    "    gx = 4*x[0]*(x[0]**2 + x[1] - 11) + 2*(x[0] + x[1]**2 - 7)\n",
    "    gy = 2*(x[0]**2 + x[1] - 11) + 4*x[1]*(x[0] + x[1]**2 - 7)\n",
    "    return np.array([gx, gy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cada caso imprima los resultados:\n",
    "- El número de iteraciones realizadas $k$\n",
    "- El punto $\\mathbf{x}_k$ obtenido\n",
    "- $f(\\mathbf{x}_k)$\n",
    "- $\\|\\nabla f(\\mathbf{x}_k)\\|$\n",
    "- La variable que indica si el algoritmo terminó porque se cumplió el criterio de paro o no.\n",
    "Además, si $n=2$, imprima\n",
    "- El valor promedio de los tamaños de paso $\\alpha_0, \\alpha_1, ..., \\alpha_k$.\n",
    "- El valor promedio de las iteraciones $i_0, i_1, ..., i_k$ realizadas por el algoritmo de backtracking.\n",
    "- La gráfica de los contornos de nivel de la función y la trayectoria\n",
    "  de los puntos $\\mathbf{x}_0, \\mathbf{x}_1, ..., \\mathbf{x}_k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num.Iter =  29\n",
      "xk       =  [3. 2.]\n"
     ]
    }
   ],
   "source": [
    "x0  = np.array([2,4])\n",
    "n   = len(x0)\n",
    "tau = np.sqrt(n) * tau\n",
    "\n",
    "xk, k, indicador, m = descenso_maximo_backtracking(Himmelblau,D_Himmelblau,x0,aini,rho,c1,tau,N,Nb)\n",
    "\n",
    "print('Num.Iter = ', k)\n",
    "print('xk       = ', xk)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.00000000e+00, 4.00000000e+00, 6.03833988e-19, 1.88000000e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función de Beale :** Para $\\mathbf{x}=(x_1,x_2)$\n",
    "\n",
    "$$ f(\\mathbf{x}) = (1.5-x_1 + x_1x_2)^2 + (2.25 - x_1 + x_1x_2^2)^2 + (2.625 - x_1 + x_1x_2^3)^2.$$\n",
    "$$ \\mathbf{x}_0 = (2.,3.) $$\n",
    "$$ \\mathbf{x}_0 = (2.,4.) $$\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función f\n",
    "def Beale(x):\n",
    "    return (1.5 - (x[0]) + (x[0])*(x[1]))**2 + (2.25 - (x[0]) + (x[0])*(x[1])**2)**2 + (2.625 - (x[0]) + (x[0])*(x[1])**3)**2\n",
    "\n",
    "def D_Beale(x):\n",
    "    Partial_Beale_dx1 = 2*(1.5 - (x[0]) + (x[0])*(x[1]))*(-1 + (x[1])) + 2*(2.25 - (x[0]) + (x[0])*(x[1])**2)*(-1 + (x[1])**2) + 2*(2.625 - (x[0]) + (x[0])*(x[1])**3)*(-1 + (x[1])**3)\n",
    "    Partial_Beale_dx2 = 2*(1.5 - (x[0]) + (x[0])*(x[1]))*((x[0])) + 2*(2.25 - (x[0]) + (x[0])*(x[1])**2)*(2*(x[0])*(x[1])) + 2*(2.625 - (x[0]) + (x[0])*(x[1])**3)*(3*(x[0])*(x[1])**2)\n",
    "    return np.array([Partial_Beale_dx1, Partial_Beale_dx2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función de Rosenbrock:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\sum_{i=1}^{n-1} \\left[100(x_{i+1} - x_i^2)^2 + (1-x_i)^2 \\right]\n",
    "\\quad n\\geq 2.$$\n",
    "$$ \\mathbf{x}_0 = (-2.1, 4.5) $$\n",
    "$$ \\mathbf{x}_0 = (-1.2, 1.0) $$\n",
    "$$ \\mathbf{x}_0 = (-2.1, 4.5, -2.1, 4.5, -2.1, 4.5, -2.1, 4.5, -2.1, 4.5) $$\n",
    "$$ \\mathbf{x}_0 = (-1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rosenbrock(x):\n",
    "    n = len(x)\n",
    "    suma = 0\n",
    "    for i in range(n-1):\n",
    "        suma += 100 * (x[i+1] - x[i]**2)**2 + (1 - x[i])**2\n",
    "    return suma\n",
    "\n",
    "def D_Rosenbrock(x):\n",
    "    n = len(x)\n",
    "    gradient = np.zeros(n)\n",
    "    for i in range(n-1):\n",
    "        gradient[i] += -400 * x[i] * (x[i+1] - x[i]**2) - 2 * (1 - x[i])\n",
    "        gradient[i+1] += 200 * (x[i+1] - x[i]**2)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Nocedal sugiere que la constante $c_1$ sea del orden de $0.0001$. \n",
    "   Use $c_1=0.0001$ y  repeta la prueba con la función de Beale y \n",
    "   explique en qué casos conviene usar un valor grande o pequeño de $c_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{lightblue}{Ejercicio \\space 2}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{lightblue}{Ejercicio \\space 3}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Verificamos si $p_0$ es dirección de descenso:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El gradiente de f:\n",
    "\\begin{align*}\n",
    "f(x_1, x_2) &= 5 + (x_1)^2 + (x_2)^2 \\\\\n",
    "\\nabla f(x_1, x_2) &= \\nabla f{x_1}{x_2} \\\\\n",
    "&= (2x_1, 2x_2)\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\implies \\nabla f(\\textbf{x}_0)= (-2, 2)\n",
    "\\end{align*}\n",
    "\n",
    "* Dirección p:\n",
    "\n",
    "\\begin{align*}\n",
    "\\textbf{p}_0=(1,0) \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "&\\implies \\nabla f(\\textbf{x}_0)\\textbf{p}_0=(-2,2)^T (1,0)=-2+0=-2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "$$\\implies \\nabla f(\\textbf{x}_0)\\textbf{p}_0 <0$$\n",
    "\n",
    "Por lo que $p_0$ es dirección de descenso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Encontramos $\\alpha_{max}$ que satisface la dirección de descenso:\n",
    "\n",
    "\n",
    "* f evaluada en $x_0 + p_0$ \n",
    "$$f(x_0 + p_0)=f((-1,1)+(1,0))=f(0,1)=6$$\n",
    "\n",
    "* f evaluada en $x_0$\n",
    "\n",
    "$$f(x_0)=f(-1,1)=7$$\n",
    "\n",
    "*Establecemos la desigualdad de la condición de descenso suficiente:\n",
    "\n",
    "$$f(x_0 + p_0) \\leq f(x_0) + c_1\\alpha \\textbf{p}_0^T \\nabla f(x_0) $$\n",
    "\n",
    "$$6 \\leq 7+ 10^{-4} \\alpha (-2)$$\n",
    "\n",
    "*despejamos $\\alpha$\n",
    "\n",
    "$$\\alpha \\leq 20^{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{lightblue}{Ejercicio \\space 4}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $f ∶ Rn → R$ y S una matriz no singular de tamaño $n \\times n$. Si $x = Sy$ para $y \\in Rn$ y definimos\n",
    "$g(y) = f(Sy)$.\n",
    "\n",
    "* A plicando la regla de la cadena muestre que $\\nabla g(y) = S^{⊤} \\nabla f(x)$\n",
    "\n",
    "\n",
    "Partimos del gradiente de $g(y)$:\n",
    "$$\\nabla _y g(y)= \\nabla _y f(Sy)=\\nabla _y f(x)$$\n",
    "\n",
    "$$\\implies \\nabla _y g(y)= \\nabla _x f(x) * \\nabla _y x$$\n",
    "\n",
    "$$\\implies \\nabla _y g(y)= \\nabla _x f(x) * \\nabla _y Sy $$\n",
    "\n",
    "S es una matriz y $ \\nabla _y y=(1,1,1,1...)$, por lo que \n",
    "\n",
    "$$\\nabla _y g(y)= S^{⊤}  \\nabla _x f(x)   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Muestre que $−D\\nabla𝑓(x_k)$ con $D=SS^{⊤}$ es una dirección de descenso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partimos del producto entre eñ grandiente y  $ -D\\nabla f(x)$\n",
    "\n",
    "\n",
    "\n",
    "$$\\nabla f(x)^{⊤} [-D\\nabla f(x)]=-\\nabla f(x)^{⊤}  SS^{⊤}  \\nabla f(x)$$\n",
    "\n",
    "$$\\implies -[\\nabla f(x)^{⊤}  S][S^{⊤}  \\nabla f(x) ]$$\n",
    "\n",
    "$$\\implies -[\\nabla f(x)  S^{⊤}]^{⊤}[S^{⊤}  \\nabla f(x) ]$$\n",
    "\n",
    "\n",
    "Pero usando que $\\nabla _y g(y)= S^{⊤}  \\nabla _x f(x) $ (resultado anterior)\n",
    "\n",
    "\n",
    "$$\\implies \\nabla f(x)^{⊤} [-D\\nabla f(x)]=-[\\nabla _y g(y)]^{⊤}[\\nabla _y g(y)]=-||\\nabla _y g(y)||^2$$\n",
    "\n",
    "Y la norma siempre es mayor o igual que cero, por lo que \n",
    "\n",
    "$$\\nabla f(x)^{⊤} [-D\\nabla f(x)] \\leq 0$$\n",
    "\n",
    "Así que $-D\\nabla f(x)$ es una dirección de descenso"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
