{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curso de Optimización I\n",
    "## Tarea 7\n",
    "\n",
    "| Descripción:                         | Fechas               |\n",
    "|--------------------------------------|----------------------|\n",
    "| Fecha de publicación del documento:  | **Abril 15, 2024**   |\n",
    "| Fecha límite de entrega de la tarea: | **Abril 23, 2024**   |\n",
    "\n",
    "\n",
    "### Indicaciones\n",
    "\n",
    "- Envie el notebook con los códigos y las pruebas realizadas de cada ejercicio.\n",
    "- Si se requiren algunos scripts adicionales para poder reproducir las pruebas,\n",
    "  agreguelos en un ZIP junto con el notebook.\n",
    "- Genere un PDF del notebook y envielo por separado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicio 1 (4 puntos)\n",
    "\n",
    "Programar el método de Newton truncado descrito en el Algoritmo 1 y 2 de la Clase 20.\n",
    "   \n",
    "1. Programar la función que implementa el Algoritmo 1, que calcula una aproximación\n",
    "   de la solución del sistema de Newton. \n",
    "- Haga que la función devuelva la dirección $\\mathbf{p}_k$ y el número de iteraciones realizadas.\n",
    "\n",
    "2. Programar la función que implementa el Algoritmo 2. \n",
    "- Use el algoritmo de backtracking con la condición de descenso suficiente para \n",
    "  calcular el tamaño de paso $\\alpha_k$.\n",
    "- Defina la variable binaria $res$ de modo que $True$ si se cumple la condición de salida\n",
    "  $\\|\\mathbf{g}_k\\|<\\tau$ y $False$ si termina por iteraciones.\n",
    "- Calcule el promedio de las iteraciones realizadas por el Algoritmo 1 \n",
    "- Haga que la función devuelva $\\mathbf{x}_k, \\mathbf{g}_k, k, res$ y el\n",
    "  promedio de la iteraciones realizadas por el Algoritmo 1.\n",
    "\n",
    "3. Pruebe el algoritmo para minimizar las siguientes funciones usando los parámetros\n",
    "   $N=5000$, $\\tau = \\sqrt{n}\\epsilon_m^{1/3}$, donde $n$ es la dimensión\n",
    "   de la variable $\\mathbf{x}$ y $\\epsilon_m$ es el épsilon máquina. \n",
    "   Para backtracking use $\\rho=0.5$, $c_1=0.001$ y el número máximo de iteraciones $N_b=500$.\n",
    "   \n",
    "   En cada caso imprima los siguientes datos:\n",
    "   \n",
    "- la dimensión $n$,\n",
    "- $f(\\mathbf{x}_0)$,\n",
    "- el  número $k$ de iteraciones realizadas,\n",
    "- $f(\\mathbf{x}_k)$,\n",
    "- las primeras y últimas 4 entradas del punto $\\mathbf{x}_k$ que devuelve el algoritmo,\n",
    "- la norma del vector gradiente $\\mathbf{g}_k$, \n",
    "- el promedio del número de iteraciones realizadas por el Algoritmo 1.\n",
    "- la variable $res$ para saber si el algoritmo puedo converger.\n",
    "  \n",
    "\n",
    "\n",
    "**Función de cuadrática 1:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "- $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top\\mathbf{A}_1\\mathbf{x} - \\mathbf{b}_1^\\top\\mathbf{x}$,\n",
    "  donde $\\mathbf{A}_1$ y $\\mathbf{b}_1$ están definidas por\n",
    "  \n",
    "\n",
    "$$ \\mathbf{A}_1 = n\\mathbf{I} + \\mathbf{1} = \n",
    "\\left[\\begin{array}{llll} n      & 0      & \\cdots & 0 \\\\\n",
    "                       0      & n      & \\cdots & 0 \\\\ \n",
    "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                       0      & 0      & \\cdots & n \\end{array}\\right]\n",
    "+ \\left[\\begin{array}{llll} 1    & 1      & \\cdots & 1 \\\\\n",
    "                       1      & 1      & \\cdots & 1 \\\\ \n",
    "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                       1      & 1      & \\cdots & 1 \\end{array}\\right],  \\qquad\n",
    "\\mathbf{b}_1 = \\left[\\begin{array}{l} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right], $$\n",
    "\n",
    "donde $\\mathbf{I}$ es la matriz identidad y $\\mathbf{1}$ es la matriz llena de 1's,\n",
    "ambas de tamaño $n$, usando los puntos iniciales   \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{10}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{100}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{1000}$ \n",
    "\n",
    "---\n",
    "\n",
    "**Función de cuadrática 2:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "- $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top\\mathbf{A}_2\\mathbf{x} - \\mathbf{b}_2^\\top\\mathbf{x}$,\n",
    "  donde $\\mathbf{A}_2= [a_{ij}]$ y $\\mathbf{b}_2$ están definidas por\n",
    "  \n",
    "$$ a_{ij} = exp\\left(-0.25(i-j)^2 \\right),  \\qquad\n",
    "\\mathbf{b}_2 = \\left[\\begin{array}{l} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] $$\n",
    "\n",
    "usando los puntos iniciales:\n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{10}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{100}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{1000}$ \n",
    "\n",
    "---\n",
    "\n",
    "**Función de Beale :** Para $\\mathbf{x}=(x_1,x_2)$\n",
    "\n",
    "$$f(\\mathbf{x}) = (1.5-x_1 + x_1x_2)^2 + (2.25 - x_1 + x_1x_2^2)^2 + (2.625 - x_1 + x_1x_2^3)^2.$$\n",
    "- $\\mathbf{x}_0 = (2,3)$  \n",
    "   \n",
    "---\n",
    "\n",
    "**Función de Himmelblau:** Para $\\mathbf{x}=(x_1,x_2)$\n",
    "\n",
    "$$f(\\mathbf{x}) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2. $$\n",
    "- $\\mathbf{x}_0 = (2,4)$\n",
    "\n",
    "---\n",
    "\n",
    "**Función de Rosenbrock:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\sum_{i=1}^{n-1} \\left[100(x_{i+1} - x_i^2)^2 + (1-x_i)^2 \\right]\n",
    "\\quad n\\geq 2.$$\n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0)\\in \\mathbb{R}^{2}$  \n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0, ..., -1.2, 1.0) \\in \\mathbb{R}^{20}$  \n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0, ..., -1.2, 1.0) \\in \\mathbb{R}^{40}$ \n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2 (3 puntos)\n",
    "\n",
    "Programar las funciones que calcule el gradiente y la Hessiana usando el método\n",
    "de diferencias finitas.\n",
    "\n",
    "1. Programe la función que calcule una aproximación del gradiente de una función\n",
    "   $f(\\mathbf{x})$ en un punto $\\mathbf{x}\\in\\mathbb{R}^n$ dado usando el esquema de \n",
    "   diferencias finitas hacia adelante    (Página 20 de la Clase 20).\n",
    "   \n",
    "- La función recibe como parámetros la función $f$, el punto $\\mathbf{x}$ y\n",
    "  el incremento $h$ y devuelve el arreglo de tamaño $n$ con las aproximaciones\n",
    "  de  aproximaciones de las derivadas parciales en el punto $\\mathbf{x}$.\n",
    "  \n",
    "2. Programe la función que calcule una aproximación de la Hessiana de una función\n",
    "   $f(\\mathbf{x})$ en un punto $\\mathbf{x}\\in\\mathbb{R}^n$ dado usando el esquema de \n",
    "   diferencias finitas de la Página 22  de la Clase 20.\n",
    "\n",
    "- La función recibe como parámetros la función $f$, el punto $\\mathbf{x}$ y\n",
    "  el incremento $h$ y devuelve una matriz simétrica de tamaño $n$ que tiene\n",
    "  las aproximaciones de las segundas derivadas parciales de $f$  en el punto $\\mathbf{x}$.\n",
    "  \n",
    "3. Modifique la función `errorRelativo_grad`  para reportar estadísticas del\n",
    "   error relativo de la implementación del gradiente analítico `gradf` de \n",
    "   una función respecto al gradiente calculado con `autograd`, para que mida \n",
    "   el error relativo entre la función `gradf` y la aproximación del gradiente \n",
    "   usando diferencias finitas.\n",
    "   Hay que agregar como parámetro de `errorRelativo_grad` el incremento $h$\n",
    "   para que se pueda llamar la función del Punto 1.\n",
    "   \n",
    "4. Programar la función `errorRelativo_hess`, similar a la función del punto anterior,\n",
    "   para que reporte estadísticas del error relativo entre una función que calcula\n",
    "   la Hessiana de $f$ de manera analítica en un punto  $\\mathbf{x}$ y la aproximación\n",
    "   de la Hessiana en  $\\mathbf{x}$ usando diferencias finitas.\n",
    "   \n",
    "5. Pruebe las funciones `errorRelativo_grad` con cada una de las funciones \n",
    "   del Ejercicio 1 usando $h=10^{-5}, 10^{-6}, 10^{-7}, 10^{-8}$.\n",
    "   ¿Cuál es el valor de $h$ que conviene usar para aproximar el gradiente y cuál\n",
    "   para aproximar la Hessiana?\n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def errorRelativo_grad(fncf, gradf, n, nt):\n",
    "    '''\n",
    "    Imprime estadísticas sobre el error relativo entre el gradiente analítico\n",
    "    de una función y el cálculado con autograd.\n",
    "    fncf  - Función f\n",
    "    gradf - Implementación del gradiente analítico de la función f\n",
    "    n     - Dimensión de la variable n\n",
    "    nt    - Número de puntos aleatorios en donde se comparan los gradientes \n",
    "    '''\n",
    "    ve = np.zeros(nt)\n",
    "    print('\\nErrores relativos en el cálculo del gradiente:')\n",
    "    g_f = egrad(fncf)  # Funcion gradiente generada con autograd\n",
    "    for it in range(nt):\n",
    "        x0  = np.random.randn(n)\n",
    "        g0  = gradf(x0)\n",
    "        ga  = g_f(x0)\n",
    "        ve[it] = np.linalg.norm(g0-ga)/np.linalg.norm(ga)\n",
    "    print('Min: %.2e   Media: %.2e    Max: %.2e' %(np.min(ve), np.mean(ve), np.max(ve)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Ejercicio 3 (3 puntos)\n",
    "\n",
    "Seleccionar un artículo para el proyecto final.\n",
    "\n",
    "- El proyecto final se puede presentar de manera individual o en equipo \n",
    "  formado por dos estudiantes.\n",
    "- La entrega del proyecto consiste programar el algoritmo descrito \n",
    "  en el artículo seleccionado y realizar pruebas para reproducir algunos resultados\n",
    "  presentados en el artículo o diseñar los experimentos de prueba. El objetivo es\n",
    "  mostrar las ventajas o limitaciones que tiene el algoritmo propuesto.\n",
    "- Es válido delimitar el alcance, de manera que si aparecen varios algoritmos\n",
    "  en el artículo, se puede seleccionar alguno de ellos para su implementación y validación.\n",
    "- Hay que elaborar un reporte en el que se dé una introducción, \n",
    "  algunos fundamentos teóricos, el planteamiento del problema, la descripción del algoritmo, \n",
    "  los resultados obtenidos y las conclusiones.\n",
    "- Hay que hacer una presentación de unos 15 minutos en el día acordado y \n",
    "  entregar el reporte, el código y las pruebas realizadas.\n",
    "- Se puede entregar un notebook como el reporte y usarlo en la presentación,\n",
    "  para que no tener que elaborar un documento con el reporte, otro con el script \n",
    "  del código y pruebas y otro para la presentación.\n",
    "- Habrá dos fechas de entrega. La primera fecha es para los estudiantes de posgrado \n",
    "  que será entre el 27 de mayo y el 4 de junio. La segunda fecha es para los estudiantes \n",
    "  de licenciatura que será entre el 3 de junio y el 10 de junio.\n",
    "- Si el equipo está formado por un estudiante de licenciatura y otro de posgrado\n",
    "  tendrá que presentar el proyecto en la primera fecha.\n",
    "- Para la selección se puede tomar uno de los artículos de la lista \n",
    "  que se presenta a continuación. \n",
    "- Estos artículos son una referencia. También pueden proponer algún artículo adicional,\n",
    "  pero recomienda que cuiden que para entenderlo no tengan que revisar otras fuentes\n",
    "  o que tengan que implementar algoritmos que requieran de temas que no fueron cubiertos\n",
    "  en el curso y que les consuma demasiado tiempo hacer esa revisión, por ejemplo, \n",
    "  en temas de optimización combinatoria, entera, mixta, multiobjetivo, etc.\n",
    "\n",
    "1. Escriba el nombre de los miembros del equipo junto con el nombre del programa académico.\n",
    "2. Escriba el título del artículo seleccionado\n",
    "3. Si no es un artículo de la lista o que esté en el Classroom, agregue el PDF\n",
    "   como parte de la entrega de la Tarea 7.\n",
    "   \n",
    "------\n",
    "\n",
    "### Lista de artículos\n",
    "\n",
    "1. An improvement of the Goldstein line search. <br>\n",
    "   Arnold NeumaierMorteza Kimiaei. 2023.<br>\n",
    "   https://optimization-online.org/2022/11/an-efficient-gradient-free-line-search/\n",
    "2. A harmonic framework for stepsize selection in gradient methods.<br>\n",
    "   Giulia FerrandiMichiel E. HochstenbachNataša Krejić. 2022.<br>\n",
    "   https://optimization-online.org/2022/02/8803/\n",
    "3. An Adaptive Trust-Region Method Without Function Evaluations.<br>\n",
    "   Geovani Nunes GrapigliaGabriel Stella. 2022.<br>\n",
    "   https://optimization-online.org/2022/02/8787/\n",
    "4. Accelerated Scaled Memory-less SR1 method for Unconstrained Optimization.<br>\n",
    "   Neculai Andrei. 2021.<br>\n",
    "   https://camo.ici.ro/neculai/XXITR5.pdf\n",
    "5. Secant Penalized BFGS: A Noise Robust Quasi-Newton Method Via Penalizing The Secant Condition.<br>\n",
    "   Brian Irwin, Eldad Haber. <br>\n",
    "   https://arxiv.org/abs/2010.01275\n",
    "6. Regularized Step Directions in Nonlinear Conjugate Gradient Methods.<br>\n",
    "   Cassidy K. Buhler, Hande Y. Benson, David F. Shanno. 2023.<br>\n",
    "   https://arxiv.org/abs/2110.06308\n",
    "7. A Levenberg-Marquardt Method for Nonsmooth Regularized Least Squares.<br>\n",
    "   Aleksandr Y. Aravkin, Robert Baraldi, Dominique Orban. 2023.<br>\n",
    "   https://arxiv.org/abs/2301.02347\n",
    "8. Globally linearly convergent nonlinear conjugate gradients without Wolfe line search.<br>\n",
    "   Arnold NeumaierMorteza KimiaeiBehzad Azmi. 2023.<br>\n",
    "   https://optimization-online.org/2022/12/globally-linearly-convergent-nonlinear-conjugate-gradients-without-wolfe-line-search/\n",
    "9. A nonlinear conjugate gradient method with complexity guarantees and its application to nonconvex regression.<br>\n",
    "   Rémi Chan--Renous-Legoubin, Clément W. Royer. 2022.<br>\n",
    "   https://arxiv.org/abs/2201.08568\n",
    "10. Iteration Complexity of Fixed-Step Methods by Nesterov and Polyak for Convex Quadratic Functions.<br>\n",
    "    Melinda Hagedorn, Florian Jarre. 2022. <br>\n",
    "    https://arxiv.org/abs/2211.10234\n",
    "11. Subsampled cubic regularization method for finite-sum minimization.<br> \n",
    "    Max L. N. Gonçalves. 2022.<br>\n",
    "    https://files.cercomp.ufg.br/weby/up/922/o/Inexact_CNMSubsampled16November2022.pdf\n",
    "12. Optimized convergence of stochastic gradient descent by weighted averaging.<br>\n",
    "    Melinda Hagedorn, Florian Jarre. 2022. <br>\n",
    "    https://arxiv.org/abs/2209.14092\n",
    "13. Two efficient gradient methods with approximately optimal stepsizes based on regularization models for unconstrained optimization. <br>\n",
    "    Zexian Liu, Wangli Chu, Hongwei Liu. <br>\n",
    "    https://arxiv.org/abs/1907.01794\n",
    "14. A Trust Region Method for the Optimization of Noisy Functions.<br>\n",
    "    Shigeng Sun, Jorge Nocedal. 2022.<br>\n",
    "    https://arxiv.org/abs/2201.00973\n",
    "15. A modified quasi-Newton method for nonlinear equations. <br>\n",
    "    Xiaowei Fang, Qin Ni, Meilan Zeng. 2018. <br>\n",
    "    https://drive.google.com/file/d/13s8ey-LVioDjx3BFksWTUzccRXV9sEbG/view?usp=sharing\n",
    "16. A minibatch stochastic Quasi-Newton method adapted for nonconvex deep learning problems. <br>\n",
    "    Joshua D. GriffinMajid JahaniMartin TakacSeyedalireza YektamaramWenwen Zhou. 2022. <br>\n",
    "    https://optimization-online.org/2022/01/8760/\n",
    "17. Nonlinear conjugate gradient for smooth convex functions. <br>\n",
    "    Sahar Karimi, Stephen Vavasis. 2024.\n",
    "    https://arxiv.org/abs/2111.11613\n",
    "18. Quadratic Regularization Methods with Finite-Difference Gradient Approximations. <br>\n",
    "    Geovani Nunes Grapiglia. 2021. <br>\n",
    "    https://optimization-online.org/wp-content/uploads/2021/11/8665.pdf\n",
    "19. Adaptive Finite-Difference Interval Estimation for Noisy Derivative-Free Optimization. <br>\n",
    "    Hao-Jun Michael Shi, Yuchen Xie, Melody Qiming Xuan, Jorge Nocedal. 2021.<br>\n",
    "    https://arxiv.org/abs/2110.06380\n",
    "20. Full-low evaluation methods for derivative-free optimization. <br>\n",
    "    Albert S. Berahas, Luis Nunes, Vicente Oumaima Sohab. 2021.\n",
    "    https://arxiv.org/abs/2107.11908\n",
    "21. A stochastic first-order trust-region method with inexact restoration for finite-sum minimization.<br>\n",
    "    Stefania Bellavia, Natasa Krejic, Benedetta Morini, Simone Rebegoldi. 2022.<br>\n",
    "    https://arxiv.org/abs/2107.03129\n",
    "22. Robust Conjugate Gradient Methods for Non-smooth Convex Optimization and Image Processing Problems.<br>\n",
    "    Salar Farahmand-Tabar, Fahimeh Abdollahi, and Masoud Fatemi. <br>\n",
    "    https://drive.google.com/file/d/1ItSZ_7N0QKJLvqI8jhyAKU532oxWMmwg/view?usp=sharing\n",
    "23. A family of hybrid conjugate gradient method with restart procedure for\n",
    "    unconstrained optimizations and image restorations.<br>\n",
    "    Xianzhen Jiang, Xiaomin Ye, Zefeng Huang, Meixing Liu. 2023<br>\n",
    "    https://drive.google.com/file/d/1h1TQpydDxkYBF0G_jd7O7-9J69X-V-dI/view?usp=sharing\n",
    "24. A modified Broyden-like quasi-Newton method for nonlinear equations. <br>\n",
    "    Weijun Zhou, Li Zhang. 2020. <br>\n",
    "    https://drive.google.com/file/d/152StQd3SVdNUXRSSboABFpjHbqEVxUP9/view?usp=sharing\n",
    "\n",
    "### Respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
